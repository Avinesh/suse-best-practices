:docinfo:

= SAP Data Intelligence 3 on CaaS Platform 4.2

== Introduction

This guide describes the on-premises installation of SAP Data Intelligence 3 on
SUSE CaaS Platform 4.2.

== Pre-requisites

=== Hardware

* Sizing

For sizing information, see the SAP documentation:
link:https://help.sap.com/viewer/835f1e8d0dde4954ba0f451a9d4b5f10/3.0.latest/en-US[Sizing Guide for SAP Data Intelligence]

At least 8 nodes are needed for a production grade Kubernetes cluster.

* Minimal requirements:
** 3 master nodes
** 4 worker nodes
** 1 or 2 Load-balancers; these can be virtual machines.

For the installation of CaaSP 4.2, additional hosts are needed:

* Management host

* Registry for storing container images

These can be virtual machines.


=== Software

* SUSE Linux Enterprise 15 SP1

* SUSE CaaSP 4.2



== Installation of SUSE CaaSP 4.2

=== Documentation

SUSE CaaS Platform 4.2 is documented here.

* link:https://documentation.suse.com/suse-caasp/4.2/[SUSE product documentation]


// CAVE!
// This needs to be removed when the fix is shipped as maintenance update.
// Due to a bug in cri-o version shipped with SUSE CaaSP 4.2 at the time of this writing, it is necessary to ask SUSE for a PTF for SUSE bug bsc# 117400.
// Download the PTF and install it on all your CaaSP cluster nodes.

// Link to the SUSE TID How to install a PTF.

=== Preparations

On all the nodes, install SUSE Linux Enterprise 15 SP1 or higher, as per the
documentation for CaaS Platform 4.2.

On each respective node, the following Modules or Products are required.

* Management host:

** SLE 15 SP1
** SLE 15 SP1 Containers Modules
** SLE 15 SP1 Public Cloud
** SUSE CaaSP 4

* Kubernetes master nodes:

** SLE 15 SP1
** SLE 15 SP1 Public Cloud
** SUSE CaaSP 4

* Kubernetes worker nodes:

** SLE 15 SP1
** SLE 15 SP1 Public Cloud
** SUSE CaaSP 4

* Load-balancer host:

** SLES 15 SP1 for SAP applications
+
or
+
** SLE 15 SP1 plus High Availability Extension

=== Base installation of the CaaSP 4 cluster nodes

* Install SLE 15 SP1
+
NOTE: Use the "Expert Partitioner" to disable and remove any automatically-configured
swap partitions on the Kubernetes nodes.

See the relevant product documentation:

* link:https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#book-sle-deployment[SLE 15 SP1 Deployment Guide]
* link:https://documentation.suse.com/suse-caasp/4.2/single-html/caasp-deployment/[SUSE CaaSP Deployment Guide]



=== Installing the load-balancer for the Kubernetes cluster

* Install SLE 15 SP1
* Install `ha-proxy` or `nginx`
* Configure the load-balancer

See the link:https://documentation.suse.com/suse-caasp/4.2/single-html/caasp-deployment/[SUSE CaaSP Deployment Guide]

=== Installation of the management workstation

* Install SLE 15 SP1

* Add the necessary SLE 15 SP1 modules: value
+
----
$ sudo SUSEConnect -r CAASP_REGISTRATION_CODE
$ sudo SUSEConnect -p sle-module-containers/15.1/x86_64
$ sudo SUSEConnect -p caasp/4.0/x86_64 -r CAASP_REGISTRATION_CODE
$ sudo SUSEConnect -p sle-module-python2/15.1/x86_64
----

=== Bootstrap the SUSE CaaSP 4 Cluster

* Run the `skuba` tool for initialization of the cluster.

* Make sure that `ssh` is working between all nodes without using passwords, and
configure `ssh-agent`.
+
----
$ eval `ssh-agent`
$ ssh-add <path to key>
----
+
----
$ skuba cluster init --control-plane <LB IP/FQDN> my-cluster
----

* Bootstrap the cluster:
+
----
$ cd my-cluster
$ skuba node bootstrap --target <IP/FQDN> <NODE NAME>
----

* Add additional master nodes:
+
----
$ cd my-cluster
$ skuba node join --role master  --target <IP/FQDN> <NODE NAME>
----

* Repeat for all the master nodes

* Add the worker nodes:
+
----
$ cd my-cluster
$ skuba node join --role worker --target <IP/FQDN> <NODE NAME>
----

* Repeat for all worker nodes.

* Finally, check the cluster status.
+
----
$ cd my-cluster
$ skuba cluster status
$ cp -av  ~/my-cluster/admin.conf  ~/.kube/config
$ kubectl get nodes -o wide
----

++++
<?pdfpagebreak?>
++++
== Secure private Docker Registry for container images

TIP: This step is optional if you already have a private secure Docker registry.
If you do skip this chapter, follow the instructions at <<SUSE Enterprise Storage>>

To satisfy the requirements for SAP Data Intelligence 3, you also need a Docker
Registry. The easiest way to build and manage one is using the link:https://goharbor.io/[Harbor project].

First, you need to create a dedicated server for your Docker registry and the
Harbor stack.

WARNING: As Docker only allows characters within the range [a-z],[A-Z],[0-9] and
'-' for domain names, make sure that your FQDN does not contain any other
characters.

In our example the server will be connected to a local bridge which provides
common services (DNS, SMT, Docker-registry) for the SAP Data Intelligence stack.
The FQDN of this server will be `harbor-registry.example.com.

[#Prerequisites]
=== Prerequistes

Find the prerequisites for Harbor here: link:https://goharbor.io/docs/2.1.0/install-config/installation-prereqs/[Harbor Installation Prerequisites]

According to this, you will need to install Docker and Docker Compose before you
can set up Harbor.

* To install Docker, simply run:
+
----
# zypper in -y docker
----

* To install Docker Compose, you must download the executable from its
link:https://github.com/docker/compose[GitHub repository] and put it into a
directory within your $PATH.
+
For example, run:
+
----
# curl -L "https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose

# chmod +x /usr/local/bin/docker-compose

# ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
----

The next steps will generate the certificates used to make Harbor secure.
These can also be found at link:https://goharbor.io/docs/2.1.0/install-config/configure-https/[Configure HTTPS Access to Harbor].

. The first step is to generate a CA certificate private key:
+
----
# openssl genrsa -out ca.key 4096
----

. Then generate a certificate with the given key for your domain.
+
NOTE: For all further steps, replace <FQDN> with your fully qualified domain name. In our example this would be `harbor-registry.example.com`
+
----
# openssl req -x509 -new -nodes -sha512 -days 3650 \
  -subj "/C=DE/ST=BW/O=SUSE/CN=<FQDN>" \
  -key ca.key \
  -out ca.crt
----
+
Your CA certificate is now ready for use.

. Next, you must generate a server certificate, as follows:
+
----
# openssl genrsa -out <FQDN>.key 4096
----

. Generate a certificate signing request (CSR):
+
----
# openssl req -sha512 -new \
  -subj "/C=DE/ST=BW/O=SUSE/CN=<FQDN>" \
  -key <FQDN>.key \
  -out <FQDN>.csr
----

. Create an x509 v3 extension file with the following content:
+
----
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
extendedKeyUsage = serverAuth
subjectAltName = @alt_names

[alt_names]
DNS.1=<FQDN>
DNS.2=<hostname>
----

. Now, use the extension file to generate a certificate:
+
----
# openssl x509 -req -sha512 -days 3650 \
  -extfile v3.ext \
  -CA ca.crt -CAkey ca.key -CAcreateserial \
  -in $fqdn.csr \
  -out $fqdn.crt
----

. Copy the `.crt` and `.key` files to the system's certificate directory:
+
----
# cp <FQDN>.crt /etc/pki/trust/anchors/
# cp <FQDN>.key /etc/pki/trust/anchors/
----

. As Docker interprets `.crt` files as CA certificates and `.cert` files as
clients, you must convert your `.crt` file like this:
+
----
# openssl x509 -inform PEM -in <FQDN>.crt -out $<FQDN>.cert
----

. You can now copy the newly-created certificates to your Docker certificate
directory. If the directory `/etc/docker/certs.d` does not exist, create it.
+
----
# mkdir /etc/docker/certs.d/<FQDN>
# cp <FQDN>.cert /etc/docker/certs.d/<FQDN>/
# cp <FQDN>.key /etc/docker/certs.d/<FQDN>/
# cp ca.crt /etc/docker/certs.d/<FQDN>/
----
+
NOTE: If you want to expose your registry on a port other than 443, you must
create a directory `/etc/docker/certs.d/<FQDN>:<Port>` and copy the certificates to
this directory instead.

. To introduce the certificates to Docker, you must restart the Docker
daemon:
+
----
# systemctl restart docker
----

++++
<?pdfpagebreak?>
++++

=== Setting up Harbor

. Fetch the Harbor installer and extract its contents:
+
----
# wget https://github.com/goharbor/harbor/releases/download/v2.1.1/harbor-online-installer-v2.1.1.tgz
# tar xvf harbor-online-installer-v2.1.1.tgz
----

. Enter the extracted directory:
+
----
# cd harbor
----

. Within this directory, you should find a file called `harbor.yml.tmpl`. It holds
the configuration for the Harbor Registry and must be adjusted.

.. First, you must edit the `hostname` field and enter your FQDN.

.. Next is the *HTTPS* configuration. The subentries `certificate` and
`private_key` must be adjusted so they point to the `.crt` and `.key` files you
created in
<<Prerequisites>>.
+
This may then look like:
+
image::harbor_yml.png[Harbor Configuration File.png,480,640]
+
If you want to expose registry on a port other than 443, you can change the `port`
subentry to match your desired port.

.. You should also change the `administrator` password for Harbor, which by default
is defined in the field `harbor_admin_password: Harbor12345`.

.. The last field to mention is `data_volume: /data`, which defines where all
Harbor data will be stored. If you want Harbor to store the data somewhere else,
enter the path to the desired directory here.

. When done, save your changes, and rename the file by removing the `.tmpl`
suffix:
+
----
# mv harbor.yml.tmpl harbor.yml
----

. As Harbor uses `nginx` as a reverse proxy for all services, you must run the
`prepare` script to configure it correctly:
+
----
# ./prepare
----

. You can now start up the needed containers using Docker Compose:
+
----
# docker-compose up -d
----
+
Harbor should now be up and running.

. Distribute the CA certificate to all Kubernetes nodes, so they can access the
registry. Run following commands on all the nodes:
+
----
# scp <FQDN>:/etc/docker/certs.d/<FQDN>/ca.crt /etc/docker/certs.d/<FQDN>/<FQDN>.crt
# systemctl restart docker
----
+
For the example <FQDN> of `harbor-registry.example.com`, this looks like:
+
----
# scp harbor-registry.example.com:/etc/docker/certs.d/harbor-registry.example.com/ca.crt /etc/docker/certs.d/harbor-registry.example.com/harbor-registry.example.com.crt
# systemctl restart docker
----


=== Verifying configuration and setting up Harbor projects

To verify Harbor is running, you may access its Web front-end by visiting
\https://<FQDN> from your browser.

You may get a warning about an insecure certificate, which looks something like
this:

image::harbor_security_warning.png[Harbor Certificate Warning,480,640]

You can toggle the lower box by clicking `Advanced...`
Then click `Accept the Risk and Continue`

You will then be redirected to the login page of your Harbor registry.

image::harbor_login.png[Harbor Login,480,640]

Enter `admin` as the user name and enter the password specified in the `harbor.yml`
file. By default, this is "Harbor12345".

By default, you will be redirected to the project page which holds the
"libraries" project.

image::harbor_projects.png[Harbor Projects,480,640]

// TODO projects aufsetzen

You should also check if the Docker clients on your Kubernetes nodes can
access the registry. To do so, run:

----
# docker login <FQDN>
----

You will be prompted to enter a user name and password. Use `admin` for the
user name, and the password you set in `harbor.yml` (default: "Harbor12345").

If Docker can access the registry, you will see a line that says
"Login Succeeded" or similar.

If your machines cannot resolve the FQDN of your registry, edit your
`/etc/hosts` file and add a line of the form:

----
<IP>  <FQDN>  <Hostname>
----

In our example, this will look like this:

----
192.168.180.100  harbor-registry.example.com  harbor-registry
----



//TODO remove everything below for this chapter

Create the namespaces on your registry that are needed for SAP Data Intelligence
3:

* `com.sap.hana.container`
* `com.sap.datahub.linuxx86_64`
* `com.sap.datahub.linuxx86_64.gcc6`
* `consul`
* `elasticsearch`
* `fabric8`
* `google_containers`
* `grafana`
* `kibana`
* `prom`
* `vora`
* `kaniko-project`
* `com.sap.bds.docker`

++++
<?pdfpagebreak?>
++++
== SUSE Enterprise Storage

TIP: This step is optional if you already have a storage that provides RBD
volumes and/or S3 buckets. If you do skip this chapter, follow the instructions at <<Installation of SAP Data Intelligence 3 on top of SUSE CaaSP 4.2>>

On-premises installation of SAP Data Intelligence 3requires SUSE Enterprise
Storage 5.5 or higher.

If you plan to use SUSE Enterprise Storage not only for your Kubernetes dynamic
storage class but also for your Kubernetes control plane, virtualized or
otherwise, you should reserve enough resources to address the `etcd`
requirements specified in the link:https://etcd.io/docs/current/op-guide/hardware/[`etcd` Hardware recommendations]

The following steps will deploy a minimalistic, virtualized, test-oriented
instance of SUSE Enterprise Storage 6.

In the following example, we will build a four-nodes (1 admin + 3 OSD) Ceph
Cluster.

=== Before starting

* Obtain registration codes for SUSE Linux Enterprise Server 15 SP1 and SUSE
Enterprise Storage from https://scc.suse.com, or have SMT/RMT properly set up
and already mirroring theses products.

** link:https://scc.suse.com[SCC]
+
image::scc-sle.png[scc-sle,480,640]

** link:https://documentation.suse.com/smt/11.3/html/SLE-smt/index.html[SMT]
+
image::scc-ses.png[scc-ses,480,640]


=== Setting up the machines

. Download the installation media for SLES 15 SP1, for example from:
+
* SLES:
** https://www.suse.com/download/sles/
* SES:
** https://www.suse.com/download/ses/

. To install, boot the destination machine and insert the installation media.
After boot, you should see a screen like this:
+
image::SLES_Setup_Init.png[SLES Setup Initial Page,480,640]

. Select your prefered language and keyboard layout. Then select
the option "SUSE Linux Enterprise Server 15 SP1" and click the "Next" button.

. You then need to agree to the license agreement, and click the "Next" button
again.

. The registration screen should now be displayed.
+
image::SLES_Setup_Registration.png[SLES Setup Registration Page,480,640]

. Now, either enter your e-mail and the registration code you collected from the
SCC as described in the previous chapter, or select "Register System via local
SMT Server" and enter the URL of your SMT server.

. If required, you can use the *Network Configuration* button at the top right, to configure your network settings.
+
image::SLES_Setup_AddOn.png[SLES Setup Add On Product Page, 480,640]

. When registered the system, you will be shown the "Add On Product" page. You
can skip by clicking the "Next" button again.

. The "Suggested Partitioning" page will be shown. You may now edit the proposed
partitioning if needed. For this guide, we will accept the proposal and continue
by clicking the "Next" button.
+
image::SLES_Setup_Partitioning.png[SLES Setup Partitioning Page, 480,640]

. On the "Clock and Time Zone" page, you should select your preferred Region and
Time Zone.
+
image::SLES_Setup_TimeZone.png[SLES Setup Clock and Time Zone Page, 480,640]
+
Then click "Other Settings" to open the "Change Date and Time" page. Select the
"Synchronize with NTP-Server" option. You must then select an NTP server and
click the "Accept" button.
+
image::SLES_Setup_NTP.png[SLES Setup Change Date and Time Page, 480,640]

IMPORTANT: All machines in the cluster must synchronize with the same NTP Server!

. Next, you can create a user with name and password of your choice. When done confirm and again click the "Next" button.
+
image::SLES_Setup_User.png[SLES Setup Create User Page, 480,640]

. You now will see the "Installation Settings" page, where you must disable the
firewall. You can achieve this by clicking the "disable" button after "Firewall
will be enabled".
+
image::SLES_Setup_Firewall.png[SLES Setup Installation Settings Page, 480,640]

. The final installation step is "Extension and Module Selection", where you
should pick the option for "SUSE Enterprise Storage 6 x86_64".
+
image::SLES_Setup_Extensions.png[SLES Setup Extension Page, 480,640]

Set up at least four machines this way.

=== Setting up SUSE Enterprise Storage

Check that your machines have the correct host names:

----
# hostname -f
----

This should output the FQDN of the machine. If this is not the case, you can use
the following command to set the name:

----
# hostnamectl --set-hostname <FQDN>
----

In this guide, for the machine's names we will use:

- `admin.example.com`
- `mon1.example.com`
- `mon2.example.com`
- `mon3.example.com`

To ensure that DNS resolution for these names works properly, you should edit
the `/etc/hosts` file on the nodes as follows:

----
127.0.0.1   localhost
<Admin-IP>  admin.example.com admin
<Mon1-IP>   mon1.example.com  mon1
<Mon2-IP>   mon2.example.com  mon2
<Mon3-IP>   mon3.example.com  mon3
----

If you do not know the IP address of a machine, run:

----
$ ip a
----

This should output something like:

image::fetch_ip.png[Fetch IP, 480,640]

==== Preparing the nodes

. On the OSD nodes, install `salt-minion` and `deepsea`:
+
----
# zypper in -y salt-minion deepsea
----

. On the admin node install salt-minion, salt-master and deepsea:
+
----
# zypper in -y salt-minion salt-master deepsea
----

. Set the Salt master on all machines by editing the associated line:
+
----
# echo "master: admin.example.com" > /etc/salt/minion
----

. On the Admin node, enable the `salt-master` service:
+
----
# systemctl enable salt-master --now
----

. Enable the `salt-minion` service on all nodes:
+
----
# systemctl enable salt-minion --now
----

. Now accept the Salt keys on the admin node:
+
----
# salt-key --accept-all -y
----
+
TIP: To check the keys, use the command `salt-key -L`. This should output something like:
+
image::salt-key.png[salt-key.png,240,319]

. Make all nodes Deepsea minions:
+
----
# echo "deepsea_minions: '*'" > /srv/pillar/ceph/deepsea_minions.sls
----

. Synchronize your salt-minions:
+
----
# salt '*' saltutil.sync_all
----

. Make sure your desired disks are all cleared. For example, you can use:
+
----
# wipefs -a /dev/vdb
----
+
NOTE: Be aware that you might need to change the device (`vdb`) here. Ensure
you clear all disks on all nodes.

. Next, apply the cleared disks by using:
+
----
# salt '*' state.apply ceph.subvolume
----

==== Deploying the cluster

You now can run the first stage of deploying the Ceph cluster.

. First, prepare the cluster:
+
----
# salt-run state.orch ceph.stage.prep
----
+
TIP: If you want, you can watch the progress of the Ceph stages in a separate
terminal with the command `deepsea monitor`.

. The result for the preparation stage should look similar to this:
+
image::ceph-stage-0.png[ceph-stage-0.png,480,500]

. The next stage is collecting information about the nodes:
+
----
# salt-run state.orch ceph.stage.discovery
----
+
image::ceph-stage-1.png[ceph-stage-1.png,480,640]

. Before you can run the last three stages, you must provide a role
configuration for the nodes. This will be stored in  `/srv/pillar/ceph/proposals/policy.cfg`.
This example uses the following configuration:
+
----
#General config
config/stack/default/global.yml
config/stack/default/ceph/cluster.yml

#CEPH Cluster members
cluster-ceph/cluster/admin.example.com.sls
cluster-ceph/cluster/mon1.example.com.sls
cluster-ceph/cluster/mon2.example.com.sls
cluster-ceph/cluster/mon3.example.com.sls

#CEPH Admin nodes
role-admin/cluster/mon1.example.com.sls
role-admin/cluster/mon2.example.com.sls
role-admin/cluster/mon3.example.com.sls

#CEPH Master node
role-master/cluster/admin.example.com.sls

#CEPH Manager nodes
role-mgr/cluster/mon1.example.com.sls
role-mgr/cluster/mon2.example.com.sls
role-mgr/cluster/mon3.example.com.sls

#CEPH Monitor nodes
role-mon/cluster/mon1.example.com.sls
role-mon/cluster/mon2.example.com.sls
role-mon/cluster/mon3.example.com.sls

#CEPH RGW nodes
role-rgw/cluster/admin.example.com.sls
role-rgw/cluster/mon1.example.com.sls
role-rgw/cluster/mon2.example.com.sls
role-rgw/cluster/mon3.example.com.sls

#CEPH Storage nodes
role-storage/cluster/admin.example.com.sls
role-storage/cluster/mon1.example.com.sls
role-storage/cluster/mon2.example.com.sls
role-storage/cluster/mon3.example.com.sls
----

. You can now safely deploy your configuration:
+
----
# salt-run state.orch ceph.stage.configure
----
+
image::ceph-stage-2.png[ceph-stage-2.png,480,500]

. Deploy your configuration to the cluster:
+
----
# salt-run state.orch ceph.stage.deploy
----
+
image::ceph-stage-3.png[ceph-stage-3.png,480,500]

. When the deployment stage has been successfully passed, check the cluster
health to insure that all is running properly.
+
----
# ceph -s
----
+
image::ceph-health.png[ceph-health.png,480,640]

. The last stage to run is deploying the service roles, which were specified
in the `policy.cfg` file:
+
----
# salt-run state.orch ceph.stage.services
----
+
image::ceph-stage-4.png[ceph-stage-4.png,480,500]

=== Access the Dashboard and create a new pool

After the Ceph cluster is up and running,you must create a pool for SAP Data
Intelligence 3.

In this example, we will use the dashboard for this purpose.

. The dashboard is published by any of the monitor nodes. To access it, use a Web
browser to visit the dashboard:
+
image::ceph-dashboard-login.png[ceph-dashboard-login.png,480,640]
+
You can fetch the credentials to log in by running the following command on the
Admin node:
+
----
# salt-call grains.get dashboard_creds
----
+
image::ceph-dashboard-creds.png[ceph-dashboard-creds.png,480,640]
+
After logging in, the landing page should appear.

. Select the "Pools" tab at the top of the page:
+
image::ceph-dashboard-landing.png[ceph-dashboard-creds.png,480,640]
+
The Pools page gives an overview of the currently-defined pools.

. Click the "Create" button at the top of the table to create a new pool:
+
image::ceph-dashboard-pools.png[ceph-dashboard-pools.png,480,640]

. Enter the name of the pool and for the "Pool type" select `replicated`.

. On the left side of "Applications", click the pencil and select `rbd`.

. Confirm the creation of the pool by clicking the "CreatePool" button at the
bottom.
+
image::ceph-dashboard-create-pool.png[ceph-dashboard-create-pool.png,480,640]
+
IMPORTANT: Remember the name of the pool; it will be needed in the chapter
<<Create Storage Class>>

. After this, the pools page is shown again, and the newly created pool is shown
in the table of pools.
+
image::ceph-dashboard-pools2.png[ceph-dashboard-pools2.png,480,640]

. Now we must provide access to this pool through an RBD device. So, go to the
RDB overview page by selecting "Block->Images"
+
image::ceph-dashboard-access-rbd.png[ceph-dashboard-access-rbd.png,480,640]
+
An overview of the configured RDBs is now displayed.

. Click the *Create* button.
+
image::ceph-dashboard-rbd.png[ceph-dashboard-rbd.png,480,640]

. Enter the name, and if it is not already selected, select the
previously-created pool. Select the size of the RBD and confirm creation by
clicking the "CreateRBD" button to create the RBD.
+
image::ceph-dashboard-create-rbd.png[ceph-dashboard-create-rbd.png,480,640]
+
The overview page of the RBD's is shown again containg your newly created RBD.
+
image::ceph-dashboard-rbd2.png[ceph-dashboard-rbd2.png,480,640]

The SUSE Enterprise Storage cluster is now ready for the usage with SAP Data Intelligence.

== Installation of SAP Data Intelligence 3 on top of SUSE CaaSP 4.2

=== Documentation

// FIXME add links

* SAP Notes:

** link:https://launchpad.support.sap.com/#/notes/2871970[Release Notes for SAP Data Intelligence 3]

** link:https://help.sap.com/viewer/a8d90a56d61a49718ebcb5f65014bbe7/3.0.latest/en-US[Installation Guide for SAP Data Intelligence 3]

//** Installation Notes ← moved to end & commented out as no link & I can't find one — LGHP

=== Planning the Installation with the SAP Maintenance Planner

For the installation of SAP Data Intelligence, you should start here:
link:https://support.sap.com/en/alm/solution-manager/processes-72/maintenance-planner.html[SAP Maintenance Planner]

NOTE: You will need to have your SAP `S-User` available.

The landing page of the SAP Maintenance Planner looks like this:

image::SAP-DI-30-CaaSP42-0001.png[title="SAP Maintenance Planner Start page",480,640,scaledwidth=80%,align=center]

Click "Plan a New System" on the right hand side.

The next page shown looks like:

image::SAP-DI-30-CaaSP42-0002.png[title="SAP Maintenance Planner: Select Plan",480,640,scaledwidth=80%,align=center]

You will see a circle in which all options are greyed out except for "Plan".
Click the "Plan" option.

The next page shows the "Define Change" step of your planning.

image::SAP-DI-30-CaaSP42-0003.png[title="SAP Maintenance Planner: Select Container Based Product",480,640,scaledwidth=80%,align=center]

On the left hand side, there is a window with three toggle buttons. Select
"CONTAINER BASED" and click "OK".

image::SAP-DI-30-CaaSP42-0004.png[title="SAP Maintenance Planner: Select Container Based Product #2",480,640,scaledwidth=80%,align=center]

To the right, the option "SAP DATA INTELLIGENCE" should appear. When you select
it, a sub-selection should open with choices for "SAP DATA INTELLIGENCE 3" and
"SAP DATA HUB 2". Select "SAP DATA INTELLIGENCE 3".

image::SAP-DI-30-CaaSP42-0005.png[title="SAP Maintenance Planner: Select Data Intelligence 3",480,640,scaledwidth=80%,align=center]

A pop-up window will appear to inform you about the related SAP note. Click
"Continue" to go ahead.

image::SAP-DI-30-CaaSP42-0006.png[title="SAP Maintenance Planner: Select Continue",480,640,scaledwidth=80%,align=center]

On the right hand side, a drop-down box is shown with "Select Support Package
Stack". Click this, and select from the available patch levels as you need.

image::SAP-DI-30-CaaSP42-0007.png[title="SAP Maintenance Planner: Select from available patch levels as you need. ",480,640,scaledwidth=80%,align=center]

To the left, the sub-selection for "SAP DATA INTELLIGENCE 3" will changes.
Select what you need and click the "Confirm Selection" at the very right.

image::SAP-DI-30-CaaSP42-0008.png[title="SAP Maintenance Planner: Select according your needs and confirm.",480,640,scaledwidth=80%,align=center]

An overview of your selection is shown. If this fits your needs, click the
"Next" button at the upper right corner.

image::SAP-DI-30-CaaSP42-0009.png[title="SAP Maintenance Planner: Select next if satisfied",480,640,scaledwidth=80%,align=center]

Next, select the operating system upon which SAP Data Intelligence will
be installed. Select "Linux on x86_64 64bit" and click "Confirm Selection".

image::SAP-DI-30-CaaSP42-0010.png[title="SAP Maintenance Planner: Select Linux and confirm",480,640,scaledwidth=80%,align=center]

The next page shows the preselected files to use and download. Again, click
the "Next" button on the upper right.

image::SAP-DI-30-CaaSP42-0011.png[title="SAP Maintenance Planner: Confirm",480,640,scaledwidth=80%,align=center]

You should now arrive at the "Download Files" page. The required *SLC bridge*
is already preselected. Click "Next" again to go ahead.

image::SAP-DI-30-CaaSP42-0012.png[title="SAP Maintenance Planner: Execute Plan",480,640,scaledwidth=80%,align=center]

Your maintenance plan is shown as a PDF. Confirm everything by clicking the
"Execute Plan" button on the upper right side.

image::SAP-DI-30-CaaSP42-0013.png[title="SAP Maintenance Planner: Download Stack.xml and SLC Bridge Installer",480,640,scaledwidth=80%,align=center]

Download the *SLC Bridge Installer* and copy it to your management workstation.
You will need this file in the next chapter: <<Installing the SAP SLC Bridge>>

You will be prompted to enter the FQDN and the port of the machine your SLC
bridge will run on.

image::SAP-DI-30-CaaSP42-0014.png[title="SAP Maintenance Planner: Enter FQDN of host where the SLC Bridge will run",480,640,scaledwidth=80%,align=center]

Fill in the values. An example looks like:

image::SAP-DI-30-CaaSP42-0015.png[title="SAP Maintenance Planner: Example for host and port",480,640,scaledwidth=80%,align=center]


=== Installing the SAP SLC Bridge

* Download the file containing the SLC Bridge Installer

* If you have not already done so, copy this file to the management workstation.

* Run the *SLC Bridge Installer* on the management workstation.

// Prepare environment for kubectl, helm etc.

// FIXME check parameters for SLCB01_50
----
$ ./SLCB01_<YOUR DOWLOADED VERSION>.EXE init
----

image::SAP-DI-30-CaaSP42-0017.png[title="SAP SLC Bridge",480,640,scaledwidth=80%,align=center]

This interactive script gathers all the necessary information to run the SAP SLC
Bridge, and at the end, deploys it into the CaaSP cluster.

image::SAP-DI-30-CaaSP42-0025.png[title="SAP SLC Bridge",480,640,scaledwidth=80%,align=center]

Identify the service port for the SLC Bridge:

----
# kubectl -n sap-slcbridge get pods
NAME                              READY   STATUS    RESTARTS   AGE
di-platform-full-product-bridge   2/2     Running   0          3d23h
slcbridgebase-858f895bd6-74gps    2/2     Running   1          10d
# kubectl -n sap-slcbridge get svc
NAME                                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
di-platform-full-product-bridge-service   ClusterIP   10.101.2.224   <none>        9000/TCP         3d23h
slcbridgebase-service                     NodePort    10.97.79.26    <none>        9000:30778/TCP   10d
----

In this example, the port number on which the SLC Bridge listens is `30778`.

Make a note of this information. It is needed for the installation process via
SAP Maintenance Planner.

image::SAP-DI-30-CaaSP42-0026.png[title="SAP SLC Bridge",480,640,scaledwidth=80%,align=center]


=== Installation of SAP Data Intelligence 3

This section describes the installation of SAP Data Intelligence 3 on top of
SUSE CaaSP 4.2.

==== Preparations

Before the installation of SAP DI 3 can start, some preparation work must be
done:

* Create a namespace on the Kubernetes cluster

* Define a (default) storage class on the Kubernetes cluster

* Adapt `PodSecurityPolicies`

* create the necessary `ClusterRoleBindings`

* If you use self-signed certificates for the private registry, a special secret
must be created.

NOTE: Unless otherwise specified, all these tasks are run from the management
workstation.

===== Create namespace for SAP DI 3 on Kubernetes

Define the namespace into which Data Intelligence will be installed:

----
$ kubectl create namespace <YOUR NAMESPACE>
----


===== Create Storage Class

* Create the storage class to provide volumes for SAP Data Intelligence 3 on
SUSE Enterprise Storage.

* Make sure that:

** You have the connection data for your SUSE Enterprise Storage at hand.

** The IP addresses and port number (default: 6789) of the monitor nodes of your
SES cluster

** That you have created a data pool on your SES cluster for use with SAP Data
Intelligence 3

** That you have the name of this pool (`datahub` in this example)

* Edit the example below to fit your environment.
+
----
$ cat > storageClass.yaml <<EOF
apiVersion: storage.kubernetes.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  name: datahub
  namespace: default
parameters:
  adminId: admin
  adminSecretName: ceph-admin-secret
  adminSecretNamespace:  default
  imageFeatures: layering
  imageFormat: "2"
  monitors: <IP ADDRESS OF MONITOR 1>:6789, <IP ADDRESS OF MONITOR 2>:6789, <IP ADDRESS OF MONITOR 3 >:6789
  pool: datahub
  userId: admin
  userSecretName: ceph-user-secret
provisioner: kubernetes.io/rbd
reclaimPolicy: Delete
volumeBindingMode: Immediate
EOF

$ kubectl create -f storageClass.yaml
----

* Create secrets for the StorageClass

** Create the secrets needed to access the storage

** Obtain the keys from your SES cluster. These are located in
`ceph.admin.keyring` and `ceph.user.keyring`.
+
You must encode the keys with `base64`.
+
----
$ echo <YOUR KEY HERE> | base64
----
+
----
$ cat > ceph-admin-secret.yaml <<EOF
apiVersion: v1
kind: Secret
metadata:
    name: ceph-admin-secret
type: "kubernetes.io/rbd"
data:
   key: <YOUR BASE64 ENCODED KEY HERE>
EOF
----
+
----
$ cat > ceph-user-secret.yaml <<EOF
apiVersion: v1
kind: Secret
metadata:
    name: ceph-user-secret
type: "kubernetes.io/rbd"
data:
   key: <YOUR BASE64 ENCODED KEY HERE>
EOF

$ kubectl apply -f ceph-admin-secret.yaml
$ kubectl apply -f ceph-user-secret.yaml
----

** Create the credentials for accessing the storageClass from the namespace where DI 3 will be installed into.
+
//does this work ?
----
$ kubectl -n <YOUR NAMESPACE FOR DI 3> create -f ceph-admin-secret.yaml
$ kubectl -n <YOUR NAMESPACE FOR DI 3> create -f ceph-user-secret.yaml
----


===== Create `PodSecurityPolicies` and `ClusterRoleBindings`

* PodSecurityPolicies
+
----
$ kubectl edit psp suse.caasp.psp.privileged
----
+
Change the `pathPrefix` in `allowedHostPaths` to `/`

* ClusterRoleBindings
+
----
$ cat > clusterrolebinding.yaml << EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: suse:caasp:psp:priviliged:default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: suse:caasp:psp:privileged
subjects:
- kind: ServiceAccount
  name: default
  namespace: XXX
- kind: ServiceAccount
  name: vora-vsystem-XXX
  namespace: XXX
- kind: ServiceAccount
  name: vora-vsystem-XXX-vrep
  namespace: XXX
- kind: ServiceAccount
  name: XXX-elasticsearch
  namespace: XXX
- kind: ServiceAccount
  name: XXX-fluentd
  namespace: XXX
- kind: ServiceAccount
  name: XXX-nodeexporter
  namespace: XXX
- kind: ServiceAccount
  name: vora-vflow-server
  namespace: XXX
- kind: ServiceAccount
  name: mlf-deployment-api
  namespace: XXX
EOF
$ sed -i s/XXX/<your-di-namespace>/g clusterrolebinding.yaml
$ kubectl apply -f clusterrolebinding.yaml
----

* Additional changes
+
----
$ kubectl edit clusterrolebinding system:node
----
+
Insert the following at the end of the file:
+
----
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:nodes
----

* If you use self-signed SSL certificates for the secure private registry,
create a secret for accessing this registry.
+
NOTE: The certificate chain should be saved in `pem` format into a single file
called `cert`.
+
----
export NAMESPACE=<your namespace>
mv cert cert_with_carriage_return
tr -d '\r' < cert_with_carriage_return > cert
kubectl create secret generic cmcertificates --from-file=cert -n $NAMESPACE
----

==== Installation of SAP DI 3

After you successfully finish the preparation stages, proceed with the
installation of SAP DI 3. To do so, several steps must be taken.

===== Connect to the SLC-Bridge

* Point your browser to the SLC-Bridge service

** https://<yournode>:<yourport>/docs/index.html

* Fill in your credentials you created during the deployment of the SLC Bridge.

// The installation workflow starts by connecting the service port of the slcb bridge.

//FIXME screen shot???


===== Installation work-flow

// FIXME lots of images


* Connect and authenticate to the SLCB service created above. Use the credentials created during the setup of the SLC Bridge:
+
image::SAP-DI-30-CaaSP42-0030.png[title="Connect and authenticate to the SLCB serive",480,640,scaledwidth=80%,align=center]

* Select "Planned software Changes"
+
image::SAP-DI-30-CaaSP42-0031.png[title="Installation select planned software changes",480,640,scaledwidth=80%,align=center]

* Select the SAP Data Intelligence deployment required by your needs. Select
"Next".
+
image::SAP-DI-30-CaaSP42-0032.png[title="Installation Select the SAP DI deployment you want to install, for example SAP DI Platform Full",480,640,scaledwidth=80%,align=center]

* Enter the Kubernetes namespace created beforehand, for example di310. Click "Next":
+
image::SAP-DI-30-CaaSP42-0033.png[title="Installation Pre-requisites Check",480,640,scaledwidth=80%,align=center]
+
image::SAP-DI-30-CaaSP42-0034.png[title="Installation Enter kubernetes namespace ",480,640,scaledwidth=80%,align=center]

* Select Advanced installation:
+
image::SAP-DI-30-CaaSP42-0035.png[title="Installation select advanced installation",480,640,scaledwidth=80%,align=center]

* Enter the URI of your Private secure registry. Click "Next":
+
image::SAP-DI-30-CaaSP42-0036.png[title="Installation Private Container registry",480,640,scaledwidth=80%,align=center]

* Enter a password for the system tenant in SAP DI 3.0:
+
image::SAP-DI-30-CaaSP42-0037.png[title="Installation System tenant password",480,640,scaledwidth=80%,align=center]

* Assign a name to the initially-created tenant, for example "default":
+
image::SAP-DI-30-CaaSP42-0038.png[title="Installation create default tenant",480,640,scaledwidth=80%,align=center]

* Create the administrator user for the default tenant in SAP DI 3.0:
+
image::SAP-DI-30-CaaSP42-0039.png[title="Installation Admin user name of default tenant",480,640,scaledwidth=80%,align=center]

* Set the password for the administrator user of the default tenant:
+
image::SAP-DI-30-CaaSP42-0040.png[title="Installation set admin user password",480,640,scaledwidth=80%,align=center]

* If you need a proxy to connect to the Internet, set the proxy settings accordingly.
+
image::SAP-DI-30-CaaSP42-0041.png[title="Installation proxy settings",480,640,scaledwidth=80%,align=center]

* Select if you want to use a checkpoint storage.
+
image::SAP-DI-30-CaaSP42-0042.png[title="Installation configure checkpoint storage",480,640,scaledwidth=80%,align=center]

* Define the storage class that should be used by SAP DI 3.0. Enter the name of
the storage class you created previously.
+
image::SAP-DI-30-CaaSP42-0043.png[title="Installation Define storage class to be used",480,640,scaledwidth=80%,align=center]

* For the SAP DI 3 installation on SUSE CaaSP 4.2 the docker log path needs to be adapted. Check the check box and click "Next".
+
image::SAP-DI-30-CaaSP42-0044.png[title="Installation Docker log path",480,640,scaledwidth=80%,align=center]

* Enter the docker log path: "/var/log/containers"
+
image::SAP-DI-30-CaaSP42-0045.png[title="Installation Docker log path",480,640,scaledwidth=80%,align=center]

* Enable Kaniko.
+
image::SAP-DI-30-CaaSP42-0046.png[title="Installation Enable usage of  kaniko",480,640,scaledwidth=80%,align=center]

* Here a different private registry can be configured if needed.
To proceed, click "Next".
+
image::SAP-DI-30-CaaSP42-0047.png[title="Installation Docker registry for SAP DI Modeler images",480,640,scaledwidth=80%,align=center]

* Enable the loading of NFS kernel modules. This ensures that the NFS kernel
modules are loaded on all Kubernetes nodes. Check the check box and click "Next".
+
image::SAP-DI-30-CaaSP42-0048.png[title="Installation Load NFS kernel modules",480,640,scaledwidth=80%,align=center]

* If needed enable Network policies. Click "Next".
+
image::SAP-DI-30-CaaSP42-0049.png[title="Installation Enable Network policies",480,640,scaledwidth=80%,align=center]

* Configure timeout during installation, leave the default and click "Next".
+
image::SAP-DI-30-CaaSP42-0050.png[title="Installation Configure timeout during installation of SAP DI 3",480,640,scaledwidth=80%,align=center]

* Into "Additional Installation Parameters", enter:
+
`-e diagnostic.fluentd.logDriverFormat=regexp -e vsystem.vRep.exportsMask=true`
+
image::SAP-DI-30-CaaSP42-0051.png[title="Installation Additional Installation Parameters",480,640,scaledwidth=80%,align=center]
+
image::SAP-DI-30-CaaSP42-0053.png[title="Installation #24",480,640,scaledwidth=80%,align=center]

// does not exist image::SAP-DI-30-CaaSP42-0052.png[title="Installation #23",480,640,scaledwidth=80%,align=center]

* Check the Summary page. If you are happy with the settings therein:
+
image::SAP-DI-30-CaaSP42-0054.png[title="Summary of Installation parameters",480,640,scaledwidth=80%,align=center]

* Press "Next" to start the deployment of SAP DI 3.0.

//image::SAP-DI-30-CaaSP42-0055.png[title="Installation #26",480,640,scaledwidth=80%,align=center]
//image::SAP-DI-30-CaaSP42-0056.png[title="Installation #27",480,640,scaledwidth=80%,align=center]
//image::SAP-DI-30-CaaSP42-0057.png[title="Installation #28",480,640,scaledwidth=80%,align=center]
//image::SAP-DI-30-CaaSP42-0058.png[title="Installation #29",480,640,scaledwidth=80%,align=center]


// FIXME Parameters used in the installation workflow
// from SAP Note:
// -e Choose "Configure container log path" for "Docker Container Log Path Configuration"
// Set "Container Log Path" as "/var/log/containers"
// Set "Enable Kaniko Usage" as yes
// Enter "-e diagnostic.fluentd.logDriverFormat=regexp -e vsystem.vRep.exportsMask=true" into "Additional Installation Parameters"
// container log path
// enable kaniko
// load kernel module nfsd and nfsv4 !! CAVE !! 5.x kernels might crash when container tries to load these modules. This for future releases on SLE 15 SP2.

//    If the registry certificate is untrusted or self-signed, follow the steps below to create necessary Kubernetes secret with the certificates so that SAP Data Intelligence installation procedure can import it into SAP Data Intelligence Connection Manager.
//        Certificate chain should be saved into file named "cert" in PEM format.
//        export DI installation namespace into NAMESPACE environment variable.
//        Ensure that your file doesn't contain DOS-type line endings. To convert from DOS-type line endings, you may use the following commmands:
//
//                mv cert cert_with_carriage_return
//
//                tr -d '\r' < cert_with_carriage_return > cert
//
//        run "kubectl create secret generic cmcertificates --from-file=cert -n $NAMESPACE" command.





== Appendix



// Standard SUSE Best Practices includes
== Legal Notice
include::common_sbp_legal_notice.adoc[]

// Standard SUSE Best Practices includes
include::common_gfdl1.2_i.adoc[]
