:docinfo:

= Building a Sandbox SAP HANA SUSE Linux Enterprise Server for SAP Applications Cluster on AWS: Using the SUSE High Availability Automation Project

== About the Guide

This guide will walk you through the deployment of a two-node SAP HANA Cluster using the SUSE Automation Project into a sandbox environment.  
This project uses Terraform and Salt to deploy and configure the operating system, SAP software and High Availability (HA) Cluster.  
If extensive configuration and customization are required, refer to the project documentation at https://github.com/SUSE/ha-sap-terraform-deployments

For the purposes of simplicity, this guide uses a SUSE Linux Enterprise Server 15 SP2 on-demand base deployment in AWS as a workstation 
to manage the deployment as it provides easy access to all the required tooling. It is possible to easily use a local Linux or OSX computer, 
but some of the commands may need modification or omission.  

The architecture for the deployment is similar to the below.

image::aws_suse_high_availability_2020.jpg[width=470]

The project will deploy the following components:

* Infrastructure 
* Instances
* Operating system configuration
* SAP HANA Install
* SR configuration
* HAE configuration


== Configuring the SUSE Linux Enterprise Server 15 SP2 Workstation

From a terminal on your SUSE Linux Enterprise Server SP2 instance, refresh the reponsitories by using the following command:

----
sudo zypper ref
----

Next, install Terraform. The packages are provided in the public cloud module published by SUSE.

----
sudo zypper --non-interactive in terraform
sudo zypper --non-interactive in terraform-provider-aws
----

Now (install and) configure the AWS CLI.
On-demand SUSE Linux Enterprise Server 15 instances have the AWS CLI installed by default. Otherwise, run the following command:

----
sudo zypper --non-interactive in aws-cli
----

The next step is to configure the CLI and provide API access keys to allow the creation of AWS resources and infrastructure.  
The API Keys are created from the AWS console. For more details, refer to 
https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html

The quickest way to configure the AWS CLI is by running the command ‘aws configure’ as described in the documentation linked above.

This generates a file in ‘$HOME/.aws/credentials` which is referenced later.

Test the credentials with the following command:

----
aws s3 ls
----

No errors should be returned.

IMPORTANT: The user specified in this step needs certain AWS permissions to ensure the deployement is successful. (See Step 4)


== Preparing the SAP Media

The SAP Media needs to be made available in an S3 Bucket so it can be accessed during the deployment.  
The SUSE Automation project allows for three methods for presenting the SAP media:

a. SAR file and SAPCAR executable (HANA Database only)
b. Multipart exe/RAR files
c. Extracted media 

This guide recommends the simplest method which is the multipart exe/RAR files.

With the correct entitlement, SAP media can be downloaded from the SAP website at
https://support.sap.com/en/my-support/software-downloads.html

The versions listed below are for illustration. When downloading the multipart files for SAP HANA, use your required version.

----
51052481_part1.exe
51052481_part2.rar
51052481_part3.rar
51052481_part4.rar
----

Using the AWS Console, perform the following actions:

* Create an S3 bucket. (The example shows a bucket called mysapmedia, but a unique name should be used.)
* Create a folder within the bucket.
* Upload the SAP media to the folder in the S3 bucket.

image::s3_bucket.png[width=470]


== Downloading and Configuring the Automation Code

The SUSE Automation code is published in GitHub. This means the Git packages need to be installed to clone the project to the workstation.
For SUSE Linux Enterprise Server 15 SP2, Git packages are provided in the SUSE Linux Enterprise Developer tools module.

----
sudo zypper --non-interactive in git
----

After having installed the Git packages, clone the SUSE project from GitHub.

----
git clone https://github.com/SUSE/ha-sap-terraform-deployments
----

////
PAYG: Which version of SLES for SAP?

----
aws ec2 describe-images --owners aws-marketplace  --region eu-central-1 --filters "Name=tag:Name,Values=*"
----
////

Next, generate SSH keys to allow for inter-cluster communication between the provisioned nodes. 
If SSH keys already exist, the next step can be skipped.

----
#optional
ssh-keygen -t rsa -N '' 
----


////
----
mkdir -p ~/ha-sap-terraform-deployments/salt/hana_node/files/sshkeys
ssh-keygen
cp ~/.ssh/id_rsa.pub  ../salt/hana_node/files/sshkeys/cluster.id_rsa.pub
cp ~/.ssh/id_rsa ../salt/hana_node/files/sshkeys/cluster.id_rsa
----

Salt Pillars are used to configure the SAP Database and Cluster. A default version is provided in the 'automatic' package:

----

cp pillar_examples/automatic/hana/*.sls pillar/hana
cp pillar_examples/automatic/drbd/*.sls pillar/drbd
cp pillar_examples/automatic/netweaver/*.sls pillar/netweaver
----

////

=== Configuring the Deployment Options and Modify the Terraform Variables  

The files that need to be configured are contained in an AWS subdirectory of the project. Use that as the working directory.

----
cd ~/ha-sap-terraform-deployments/aws 
----

A template is provided which for a sandbox environment only needs around 10 parameters modifying.  
Copy the terraform example file to _terraform.tfvars_.  

----
cp terraform.tfvars.example terraform.tfvars
----

Edit _terraform.tfvars_ and modify as explained below. If you are duplicating the lines before modification, 
ensure the original is commented out, or the deployment will fail.

Firstly, choose the region and instance types required for the deployment.

----
# Region where to deploy the configuration
aws_region = "eu-central-1"

# Instance type to use for the hana cluster nodes
hana_instancetype = "r3.8xlarge"
----

Next, enter the path for the public and private SSH keys that were generated earlier.

----
# SSH Public key location to configure access to the remote instances
public_key_location = "/path/to/your/public/ssh/key"

# Private SSH Key location
private_key_location = "/path/to/your/private/ssh/key"
----

The following parameters select the version of SUSE Linux Enterprise Server for SAP Applications to deploy and where to deploy from. 
For simplicity, the 'hana_os_owner' is set to the AWS Marketplace.  
If an existing SUSE Subscription can be used, this can be changed to use BYOS images. Refer to the project documentation.

----
hana_os_image = "suse-sles-sap-15-sp2"
hana_os_owner = "679593333241"
----

This setting provides Terraform the credentials to deploy the infrastructure on the AWS cloud.

----
aws_credentials = "~/.aws/credentials"
----

Modify the following to point to SAP Media that was uploaded to the S3 Bucket earlier.

----
hana_inst_master = "s3://mysapmedia/SAPHANA"

hana_archive_file = "5102481_part1.exe"
----

To keep the cluster architecture simple and to provide additional packages needed to deploy, set the following parameters:

----
hana_cluster_sbd_enabled = false

# Repository url used to install HA/SAP deployment packages"
ha_sap_deployment_repo = https://download.opensuse.org/repositories/network:ha-clustering:sap-deployments:v6
pre_deployment = true
----

Finally, ensure the following lines are *commented out* using a the hashtag sign #:

----
#hana_disk_device = "/dev/xvdd"
#aws_access_key_id = my-access-key-id
#aws_secret_access_key = my-secret-access-key
----


=== Subscribing to the AWS Marketplace Offer

To automatically deploy instances from the AWS Marketplace, ensure to 'Subscribe' to the offering.  

A link for SUSE Linux Enterprise Server for SAP Applications 15 SP2 can be found here:
https://aws.amazon.com/marketplace/server/procurement?productId=e9701ac9-43ee-4dda-b944-17c6c231c8db

If a different version of SUSE Linux Enterprise Server for SAP Applications is required, subscribe to the relevant version on the marketplace.


== Configuring IAM Policies

If the deployment is being run from the root user of the AWS account, or if the user specified when configuring the AWS CLI has 
Admin priviledges in your AWS account, you can skip this step.

If using an IAM user with limited permissions, additional IAM rights may be required as IAM policies are created and attached 
during deployment.  e.g. access and manage EC2 instances, S3 buckets, IAM (to create roles and policies) and EFS storage.

There are 2 options available to achieve this:

a. Attach the IAMFullAccess Policy to the user executing the project. However, this is not recommended.
b. The recommended method is to create an new IAM policy and attach it to the desired user.  

TIP: Depending on your own IAM rights, you may need to reach out to an AWS administrator for your account to set this up for you.

Create the folling policy and attach it to the IAM user running the deployment:
////
Create a file ./my-terraform-policy.json
////


----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "iam:CreateServiceLinkedRole",
                "iam:PassRole",
                "iam:CreateRole",
                "iam:TagRole",
                "iam:GetRole",
                "iam:DeleteRole",
                "iam:GetRolePolicy",
                "iam:PutRolePolicy",
                "iam:DeleteRolePolicy",
                "iam:ListInstanceProfilesForRole",
                "iam:CreateInstanceProfile",
                "iam:GetInstanceProfile",
                "iam:RemoveRoleFromInstanceProfile",
                "iam:DeleteInstanceProfile",
                "iam:AddRoleToInstanceProfile"
            ],
            "Resource": "*"
        }
    ]
}
----

////
----
aws iam create-policy --policy-name my-terraform-policy --policy-document file://my-terraform-policy.json
----

The output will show the ARN of the policy, note for later as it will be needed later when attaching to a user.

To list policies (to find ARN), run the following command:

----
aws iam list-policies --scope Local
----

Now search for your IAM Policy and attach it to a user:

----
aws iam attach-user-policy --policy-arn arn:aws:iam::YOUR_ACCOUNT_NUMBER:policy/my-terraform-policy --user-name USERNAME
----
////


== Deploying the Project

When running the deployment, Terraform will create and name resources on AWS based on the 'workspace' in use.  
It is recommended to create a unique workspace from which to run the dpeloyment.

----
terraform init
terraform workspace new sandbox1
terraform workspace select sandbox1
terraform plan
terraform apply
----

If successful, the output will be the public IP addresses for the nodes.

image::complete.png[width=470]


== Teardown

When finshed with the deployment, or even if the deployment has failed, ensure that Terraform is used to tear down the environment.  

----
terraform destroy
----

This method will ensure all AWS resource, Instances, Volumes, VPCs, IAM Roles etc. are cleaned up.

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
== Legal Notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
// :leveloffset: 0
include::common_gfdl1.2_i.adoc[]
