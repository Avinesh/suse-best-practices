
:docinfo:

= SAP Data Hub 2 on SUSE CaaS Platform 3: Installation Guide

++++
<?pdfpagebreak?>
++++

== Introduction

Today more and more data is created in business and industries. In the same extend as data grows the need grows to manage these data and get best out of these data.
SAP Data Hub 2 is a tool that makes dealing with these amounts of data more easy and SUSE delivers with SUSE CaaS Platform 3 the foundation to run SAP Data Hub 2 on top of it.

== Requirements

In order to install SAP Data Hub 2 on SUSE CaaS Platform 3 we need to meet certain requirements:

Relevant documentation:

* SUSE

** link:https://www.suse.com/documentation/suse-caasp-3/index.html[SUSE CaaS Platform 3]
** link:https://www.suse.com/documentation/suse-enterprise-storage-5[SUSE Enterprise Storage]

* SAP

** link:https://help.sap.com/viewer/product/SAP_DATA_HUB/2.5.latest/en-US[SAP Data Hub]
** link:https://launchpad.support.sap.com/#/notes/2764652[SAP Data Hub 2.5 release note]
** link:https://launchpad.support.sap.com/#/notes/2686169[Pre-requisites for installing SAP Data Hub]
** link:https://launchpad.support.sap.com/#/notes/2776522[SAP Data Hub 2: Specific Configurations for Installation on SUSE CaaS Platform]

=== Hardware and Software Requirements

==== SUSE CaaS Platform 3 Cluster

Hardware requirements (see https://help.sap.com/viewer/product/SAP_DATA_HUB/[SAP Data Hub Install Guide])
See SAP's sizing recommendations:

* link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.3.latest/en-US/79724de552db4b2b81c4a893f2c7ed18.html[SAP Data Hub 2.3 ]

* link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.4.latest/en-US/7e2a9bf62ec94e9694648e2b5d2ce882.html[SAP Data Hub 2.4]

* link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.5.latest/en-US[SAP Data Hub 2.5]

* link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.6.latest/en-US[SAP Data Hub 2.6]

The minimum hardware requirements for installing SAP Data Hub 2 on premise are:

* 4 Kubernetes cluster nodes (1 master node and 3 worker nodes)

   ** The master node should be a 4 core machine with > 32 GiB RAM

   ** The three kubernetes worker nodes should be a machine with 4 cores and with > 64 GiB RAM
// TODO check network requirements
// TODO disk requirements

* Access to a SUSE Enterprise Storage 5 system (see SAP Note link:https://launchpad.support.sap.com/#/notes/2686169[Pre-requisites for installing SAP Data Hub])

==== Management ("Jump") Host

It is recommended to do the installation of SAP Data Hub Foundation from an external Jump host and not from within the SUSE CaaS Platform Cluster.

The hardware and operating system specifications for the jump host can be e.g.

- SUSE Linux Enterprise Server 12 SP4 or SUSE Linux Enterprise Server 15 (or even openSUSE Leap 15.X)
- 2 Cores
- 8 GiB RAM
- Diskspace: 50 GiB for /, including the space for the SAP Data Hub 2 software and at least 20 GiB for /var/lib/docker (needed for the SAP Data Hub 2 installation)
- Network connectivity to the SUSE CaaS Platform cluster ( 1 GBit/s)

=== Software Requirements

Following software is needed

* SUSE CaaS Platform 3

* SAP Data Hub 2

* optional SAP Maintenance Planner

* optional SAP Hostagent

* optional Hadoop/Spark see Vora's Spark extensions


== Installation of SUSE CaaS Platform 3

=== Get the Install Media

All install media can be found at:
link:++https://download.suse.com++[SUSE CaaS Platform ISO images]

=== Get a Subscription for SUSE CaaS Platform 3

In order to be able to get all maintenance updates for your SUSE products you will need a valid subscription for the particular product.
https://www.suse.com/support/?id=SUSE_CaaS_Platform[Order subscription for SUSE CaaS Platform]


=== Read the Deployment Guide for SUSE CaaS Platform 3

SUSE CaaS Platform is designed to make the installation of Kubernetes easy, to get a deeper understanding you should read the Deployment Guide for SUSE CaaS Platform 3, available at https://www.suse.com/documentation/
For further reference there also exists a Quickstart Guide and an Administrator's Guide as well.

=== Installation of SUSE CaaS Platform 3

In this guide we describe the installation of SUSE CaaS Platform 3 from ISO images.
Make sure that the hostnames you will use for the installation are resolvable via DNS.
It is preferred to have a static network setup.

Have the FQDN  or ip address of your timeserver available. A reliable system time is required.
Connect the media to your hardware and boot from the media.
// Screenshot?

Choose *Installation* from the GRUB menu.

==== Installation of the Admin Node
// Add Pictures

After booting the hardware the YaST installer shows up the network configuration dialog.


image::002-SCT-CaaSP.png[Network,480,640,align=center]

Installation Settings Dialog

image::007-SCT-CaaSP.png[Install Settings,480,640,align=center]

On the installer screen select keyboard layout and language according to your needs.
Enter your subscription credentials or the URL of your SMT server to register this installation.
Set the password for the root account.

Assign role administration host to this installation.


// Screen Shots????


==== Installation of remaining Cluster Nodes

Boot the machine from the ISO image and select *Installation* from grub2 bootmenu.

The installer starts and the network configuration dialogue is shown.

image::002-SCT-CaaSP.png[Network,480,640,align=center]


Configure network according your needs.





Installation Settings Dialog

image::021-SCT-CaaSP.png[Install Cluster Node,480,640, align=center]

//TODO Screenshots

==== Bootstrap of the Kubernetes Cluster

After the installation of the admin node, open your browser and visit
https://name.domain.tld

image::015-SCT-CaaSP.png[Velum,480,640, align=center]

Now create the admin account, set admin username and password.
Login with the newly created admin credentials.

image::014-SCT-CaaSP.png[Velum login,480,640, align=center]

Configure SUSE CaaS Platform 3

image::016-SCT-CaaSP.png[Velum Cluster Config,480,640,align=center]

Select nodes to be included in the cluster

image::024-SCT-CaaSP.png[nodes,480,640,align=center]

Assign roles (master/worker) according to your needs

image::027-SCT-CaaSP.png[assign roles,480,640, align=center]

image::028-SCT-CaaSP.png[assign roles 2,480,640, align=center]

Bootstrap kubernetes cluster.

image::030-SCT-CaaSP.png[Bootstrap,480,640,align=center]

Finally after successfully bootstrap this screen is shown

image::032-SCT-CaaSP.png[finish,480,640,align=center]



==== Installation of the jump host

For further configuration and deployments on SUSE CaaS Platform it is highly recommended to install a jump or management host.
This chapter describes the installation of the necessary tools to be able to deploy SAP Data Hub 2 on the SUSE CaaS Platform installation.

Install SLES 12 SP4 or SLES 15 as OS on the jump host.
Register your installation against SCC or local SMT or local RMT.
Register the SLE-Container module (included in the SLES subscription).

----
# SUSEConnect -p /x86_64/SLE-Container-Module
----

Install docker from container module:

----
# zypper in docker
----

Download kubectl matching the kubernetes version of your SUSE CaaS Platform installation.
Download kubectl 1.10.11 or higher, e.g.

----
$ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.10.11/bin/linux/amd64/kubectl
$ sudo mv kubectl /usr/bin/kubectl
$ sudo chmod a+x /usr/bin/kubectl
----

Download kubectl-configuration from Velum-Dashboard

image::032a-SCT-CaaSP.png[kubeconfig,480,640]

image::033-SCT-CaaSP.png[kubeconfig,480,640]

image::034-SCT-CaaSP.png[kubeconfig,480,640]

----
$ export KUBECONFIG=<PATH/TO/YOUR/kubeconfig-file>/kubeconfig
----

or

----
$ mv <PATH/TO/YOUR/kubeconfig-file>/kubeconfig ~/.kube/config
----

Connect to your kubernetes cluster using kubectl:

----
$ kubectl get cluster-info
$ kubectl get nodes
----

The output of the commands above should look similar to this:

----
OUTPUT //TODO
----

Download helm from helm.sh, install and configure helm client.
The version should match the tiller version deployed on your SUSE CaaS Platform installation.

----
$ curl https://raw.githubusercontent.com/helm/helm/master/scripts/get > get_helm.sh
$ chmod 700 get_helm.sh
$ ./get_helm.sh --version 2.8.2
$ mv helm ~/bin/helm
$ helm init --client-only
----


==== Optional components deployed in the SUSE CaaS Platform 3 Cluster

You can also deploy some useful applications in your SUSE CaaS Platform 3 cluster:

* heapster (creates simple graphs)

* kubernetes dashboard (allows administration and basic monitoring of kubernetes cluster)



See also https://www.suse.com/documentation/suse-caasp-3/book_caasp_admin/data/book_caasp_admin.html

Installation of  heapster:

----
$ helm install --name heapster-default --namespace=kube-system stable/heapster \
--version=0.2.7 --set rbac.create=true
----

Installation of the kubernetes dashboard:

----
$ helm install --namespace=kube-system \
--name=kubernetes-dashboard stable/kubernetes-dashboard \
--version=0.6.1
----

To authenticate against the kubernetes cluster extract the id-token from kubeconfig file.
----
$ grep id-token $KUBECONFIG | awk '{ print $2 }'
----

----
$ kubectl proxy
----

Point your browser to
----
http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/
----

Use the id grep'ed from kubeconfig to login into kubernetes dashboard.



== Installation of SAP Data Hub 2

Description of the preparation and installation of SAP Data Hub 2 on SUSE CaaS Platform 3 Cluster


=== Prepare the SAP Data Hub 2 installation

Here are the steps that are needed to install SAP Data Hub on SUSE CaaS Platform successfully:


==== Download the SAP Data Hub 2 software archive:

Go to SAP Software Download Center, login with your SAP account and search for SAP DATA HUB 2 or access this link.
Download the SAP Data Hub Foundation file, for example: DHFOUNDATION03_3-80004015.ZIP (SAP DATA HUB - FOUNDATION 2.3) or DHFOUNDATION04_0-80004015.ZIP (SAP DATA HUB - FOUNDATION 2.4


Unzip the software archive on to your jump host.

There are three ways to install the SAP Data Hub 2

* use the SL Plugin, there are two variants of this:

** SL Plugin with Maintenance Planner (mpsl)
** SL Plugin only (mpfree)

* use the command line install.sh script. This document will focus on this installation method.



==== Prerequisites on the SUSE CaaS Platform 3 Cluster

The following steps are done on the jump host if not stated otherwise:

Create the namespace in the Kubernetes cluster to install SAP Data Hub 2 into:

----
$ kubectl create namespace datahub
----

Create the storage class to provide volumes for SAP Data Hub 2 on SUSE Enterprise Storage:

Make sure you have the connection data for your SUSE Enterprise Storage at hand.

* IP addresses and port number (defaults to 6789) of the monitor nodes of your SUSE Storage

* created a data pool (datahub in this example) on your SUSE Enterprise Storage for the use with SAP Data Hub 2

Edit the example below to fit your environment.

----
$ cat > storageClass.yaml <<EOF
apiVersion: storage.kubernetes.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  name: datahub
  namespace: default
parameters:
  adminId: admin
  adminSecretName: ceph-admin-secret
  adminSecretNamespace:  default
  imageFeatures: layering
  imageFormat: "2"
  monitors: <IP ADDRESS OF MONITOR 1>:6789, <IP ADDRESS OF MONITOR 2>:6789, <IP ADDRESS OF MONITOR 3 >:6789
  pool: datahub
  userId: admin
  userSecretName: ceph-user-secret
provisioner: kubernetes.io/rbd
reclaimPolicy: Delete
volumeBindingMode: Immediate
EOF

$ kubectl create -f storageClass.yaml
----

Create the secrets needed to access the storage:
Obtain the keys from your SUSE Storage located in
ceph.admin.keyring and ceph.user.keyring

You have to do a base64 encoding of the keys

----
echo <YOUR KEY HERE> | base64
----

----
$ cat > ceph-admin-secret.yaml <<EOF
apiVersion: v1
kind: Secret
metadata:
    name: ceph-admin-secret
type: "kubernetes.io/rbd"
data:
   key: <YOUR BASE64 ENCODED KEY HERE>
EOF
image::002-SCT-CaaSP.png
$ cat > ceph-user-secret.yaml <<EOF
apiVersion: v1
kind: Secret
metadata:
    name: ceph-user-secret
type: "kubernetes.io/rbd"
data:
   key: <YOUR BASE64 ENCODED KEY HERE>
EOF

$ kubectl create -f ceph-admin-secret.yaml
$ kubectl create -f ceph-user-secret.yaml
----



=== Installation of SAP Data Hub 2 using the Maintenance Planner with SL Plugin

Installation using the Maintenance Planner with SL Plugin (mpsl method)
Now follows the installation method via SL plugin.
Is a web-based installation method recommended by SAP offering you an option to send analytics data and feedback to SAP. All the necessary prerequisites have been satisfied by applying all the steps described above.

Note: You have to install the latest SAP Host Agent on the jump host, you can use the rpm package downloadable from SAP software download center.

=== Installation of SAP Data Hub 2 using the SL Plugin (mpfree method)

Is an alternative command-line-based installation method. Please refer to the SAP Data Hub documentation (2.3)) / (2.4) / (2.5) / (2.6) for more information and the exact procedure.
//TODO insert URLs


=== Installation of SAP Data Hub 2 using the command line (manual installation)

Unpack the SAP Data Hub software archive on the jump host, e.g.:

----
$ unzip DHFOUNDATION04_1-80004015.ZIP
----

Run the install command as described in SAP Data Hub 2 install guide, https://help.sap,com/datahub
// TODO URL help.sap.com/
https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.6.latest/en-US
----
$ cd SAP-Datahub-2.4.63-Foundation
$ export DOCKER_REGISTRY=<URI of your registry>
$ export NAMESPACE=datahub
$ ./install.sh
----

This interactive script configures the install of SAP Data Hub.
You should have the following information at a hand.
//TODO namespace, login on registry, S-User
The name and credentials of your SAP S-User , the login credentials to your secure registry.

//TODO explain cmdline options


=== Post-install Actions

After successful installation you can connect to the SAP Data Hub web UI.
You need to identify the service IP and port of the SAP Data Hub UI.

----
kubectl get services
kubectl describe service
----

Point your browser to the IP and port you gained from the steps above.
// TODO Example.

Use the login data you defined during the installation.

==== Post-Installation work

Follow the documentation provided by SAP (link) to the post installation work.

* Create the vflow-secret for the modeler app as pointed out in the SAP documentation

* Import necessary CAs, e.g. the CA that signed the certificate of the secure registry.


== Upgrade of SAP Data Hub 2

This section describes the update of an existing SAP Data Hub 2 installation to a higher version (e.g. 2.3 to 2.4)

Execute the SAP Data Hub upgrade as described in the official instructions. One can choose between different upgrade methods:

* Maintenance Planner: Upgrade SAP Data Hub 2 using the Maintenance Planner / SL Plugin and SAP Host Agent (https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.6.latest/en-US/31079833a65f4f379d5a76957ff8073c.html)
* SL Plugin method: Upgrade SAP Data Hub 2 using the SL Plugin and SAP Host Agent (https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.6.latest/en-US/ff37f3ccf6504bb38d7db53936fe8017.html)
* Command line method: Upgrade SAP Data Hub 2 using the install.sh script (https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.6.latest/en-US/aec679bc0209443ba4ae03a9018d4bd8.html)

== Appendix


=== Secure Private Registry

To statisfy DataHub requirements you also need a Docker Registry.
The easiest way to build and manage it comes with the project Portus.

** link:http://port.us.org/[Portus]

First you need to create a dedicated server for your Docekr registry and Portus stack.

----
# sudo virt-install --name portus-dr --ram 8192 --disk path=/var/lib/libvirt/VMS/portus-dr.qcow2,size=40 --vcpus 4 --os-type linux --os-variant generic --network bridge=common --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/isos/SLE-12-SP4-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial ifcfg=eth0=10.10.10.11/24,10.10.10.1,10.10.10.11,suse-sap.net hostname=portus-dr domain=suse-sap.net Textmode=1'
----

In our example this server will be connected to another local bridge which provides common services (DNS, SMT, Docker-registry) for the Datahub stack.

Our Portus deployment will be based on Container, and orchestrated locally with docker-compose.

Portus docker-compose deployment requires an up-to-date release of docker-compose.

----
sudo curl -L "https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
----

Now you could simply clone the Portus repository, adapt the .env and the nginx configuration to your naming convention.

----
# git clone https://github.com/SUSE/Portus.git /tmp/Portus-DR
# mv /tmp/Portus-DR/examples/compose ./portus
# cd portus
----

Now you could edit both .env and nginx/nginx.conf.
This is how our configuration looks like :

----
# cat .env
MACHINE_FQDN=portus-dr.suse-sap.net
SECRET_KEY_BASE=b494a25faa8d22e430e843e220e424e10ac84d2ce0e64231f5b636d21251eb6d267adb042ad5884cbff0f3891bcf911bdf8abb3ce719849ccda9a4889249e5c2
PORTUS_PASSWORD=XXXXXXXX
DATABASE_PASSWORD=YYYYYYYY
----

In the nginx/nginx.conf file, you should now adapt the following section :

----
server {
    listen 443 ssl http2;
    server_name portus-dr.suse-sap.net;
    root /srv/Portus/public;
----

Now, you could pull the latest docker-compose.yml.

----
# rm docker-compose.*
# wget https://gist.githubusercontent.com/Patazerty/d05652294d5874eddf192c9b633751ee/raw/6bf4ac6ba14192a1fe5c337494ab213200dd076e/docker-compose.yml
----

To avoid dealing with Docker insecure registry configuration we'll add SSL to our setup.

----
echo "subjectAltName = DNS:portus-dr.suse-sap.net" > extfile.cnf
openssl genrsa -out secrets/rootca.key 2048
openssl req -x509 -new -nodes -key secrets/rootca.key -subj "/C=FR/ST=FR/O=SUSE"  -sha256 -days 1024 -out secrets/rootca.crt
openssl genrsa -out secrets/portus.key 2048
openssl req -new -key secrets/portus.key -out secrets/portus.csr -subj "/C=FR/ST=FR/O=SUSE/CN
openssl req -new -key secrets/portus.key -out secrets/portus.csr -subj "/C=FR/ST=FR/O=SUSE/CN=portus-dr.suse-sap.net"
openssl x509 -req -in secrets/portus.csr -CA secrets/rootca.crt -extfile extfile.cnf -CAkey secrets/rootca.key -CAcreateserial  -out secrets/portus.crt -days 500 -sha256
----

Now all we have to do is to make the servers aware of this certificate

----
cp -p secrets/rootca.crt /etc/pki/trust/anchors/.net-ca.crt
scp secrets/rootca.crt root@jumpbox.suse-sap.net:/etc/pki/trust/anchors/portus-dr.suse-sap.net-ca.crt
----

Then on all servers that will needs to interact with the docker-registry :

----
sudo update-ca-certificates
sudo systemctl restart docker
----

It's time now to start your Portus setup.

----
docker-compose up -d
----

You could now log on Portus and set the registry.

image::portus-registry.png[portus-registry.png,480,640]

Installation and configuration of a secure private registry using SLES, SLE-Container-Module
The needed componentes are docker, registry and portus.
Create SSL certificates as needed.
Distribute the CA certificate to all your kubernetes nodes.
Run

----
# update-ca-certificates
# systemctl restart docker
----


Create the namespaces on your registry that are needed for SAP Data Hub 2:

* com.sap.hana.container

* com.sap.datahub.linuxx86_64

* com.sap.datahub.linuxx86_64.gcc6

* consul

* elasticsearch

* fabric8

* google_containers

* grafana

* kibana

* prom

* vora

* kaniko-project

* com.sap.bds.docker


//TODO Liste der Namespaces





=== SUSE Enterprise Storage

//SAP Data Hub 2 installation on premise require SUSE Enterprise Storage 5 or higher.
//Follow the installation documentation to create SUSE Enterprise Storage.
//Create a storage pool on your SES for the use with SAP Data Hub 2.
//You can also leverage the S3 API to create buckets on SES that can be used from applications from within SAP Data Hub 2.


SAP Data Hub 2 installation on premise require SUSE Enterprise Storage 5 or higher.
If you plan to use SUSE Enterprise Storage not only for your Kubernetes dynamic storageclass but also for your Kubernetes Control plan, virtualised or not, you should reserve enough resources to address etcd requirements regarding
** link:https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md.[etcd Hardware]

Follow the next steps to deploy a minimalistic, virtualised, test oriented, SUSE Enterprise Storage 5.5.

In the following exemple, we’re going to build a 4 nodes (1 admin + 3 OSD) Ceph Cluster.

Before starting :

* Collect your SUSE Linux Enterprise Server 12 SP3 and SUSE Enterprise Storage registration code from scc.suse.com or have a SMT/RMT properly set up and already mirroring theses products.

** link:https://scc.suse.com[SCC]
** link:https://documentation.suse.com/sles/12-SP4/html/SLES-all/book-smt.html[SMT]

image::scc-sle.png[scc-sle,480,640]
image::scc-ses.png[scc-ses,480,640]

Your DNS zone should be already set, in our exemple where all Datahub components are in the same dns zone and same subnet, it should looks like :

image::dns.png[dns,480,640]

Also, in order to be as efficient as possible when using interactive shell scripted infrastructure deployment, we advice you to use an advanced terminal client or multiplexer which permits to address multiple shells at once.

image::multi-s-virtinstall.png[multi-s-virtinstall,480,640]

----
# sudo virt-install --name ses55-admin --ram 16384 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin.qcow2,size=40 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin-osd0.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin-osd1.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin-osd2.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin-osd3.qcow2,size=20 --vcpus 4 --os-type linux --os-variant generic --network bridge=caasp3 --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/ISOS/SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial autoyast-ses5=http://10.10.10.101/autoyast-ses5 ifcfg=eth0=10.18.10.200/24,10.18.10.1,10.10.10.11,suse-sap.net domain=suse-sap.net Textmode=1'

# sudo virt-install --name ses55-osd0 --ram 16384 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0.qcow2,size=40 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0-osd0.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0-osd1.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0-osd2.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0-osd3.qcow2,size=20 --vcpus 4 --os-type linux --os-variant generic --network bridge=caasp3 --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/ISOS/SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial autoyast-ses5=http://10.10.10.101/autoyast-ses5 ifcfg=eth0=10.18.10.230/24,10.18.10.1,10.10.10.11,suse-sap.net domain=suse-sap.net Textmode=1'

# sudo virt-install --name ses55-osd1 --ram 16384 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1.qcow2,size=40 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1-osd0.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1-osd1.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1-osd2.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1-osd3.qcow2,size=20 --vcpus 4 --os-type linux --os-variant generic --network bridge=caasp3 --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/ISOS/SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial autoyast-ses5=http://10.10.10.101/autoyast-ses5 ifcfg=eth0=10.18.10.231/24,10.18.10.1,10.10.10.11,suse-sap.net domain=suse-sap.net Textmode=1'

# sudo virt-install --name ses55-osd2 --ram 16384 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2.qcow2,size=40 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2-osd0.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2-osd1.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2-osd2.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2-osd3.qcow2,size=20 --vcpus 4 --os-type linux --os-variant generic --network bridge=caasp3 --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/ISOS/SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial autoyast-ses5=http://10.10.10.101/autoyast-ses5 ifcfg=eth0=10.18.10.232/24,10.18.10.1,10.10.10.11,suse-sap.net domain=suse-sap.net Textmode=1'
----

image::multi-s-smt.png[multi-s-smt.png,480,640]

Select the SUSE Entreprise Storage 5 Extension

image::multi-s-addon.png[multi-s-addon.png,480,640]

On the hypervisor you should also be able to route or bridge ( either a traditional bridge using brctl or a virtual-bridge) your upcoming SUSE Enterprise Storage 5.5 network segment. In our example, for simplicity, we’re using the same bridge and network address than our CaaSP cluster "--network bridge=caasp3”

In the following example, each node is powered by 16GB of RAM, 4 VCPU, 40 GB for the root disk, 4 * 20GB OSDB disk.

image::multi-s-default.png[multi-s-default.png,480,640]

NTP must be configured on each nodes

image::multi-s-ntp.png[multi-s-ntp.png,480,640]

Unselect AppArmor and unnecessary X and Gnome Patterns, but select the SUSE Enterprise Storage pattern

image::multi-s-patterns.png[multi-s-patterns.png,480,640]

Desactivate the Firewall on the nodes.
Start the Installation on all nodes.

image::multi-s-install.png[multi-s-install.png,480,640]

Once nodes have reboot, log in and finish network/hostname and NTP configurations so that

----
# hostname -f
----

returns the FQDN of the nodes and

----
#ntpq -p
----

returns a stratum less than 16

image::multi-s-hostname-ntp.png[multi-s-hostname-ntp.png,480,640]

Using ssh-keygen then ssh-copy-id, spread your SUSE Entreprise Storage Admin node ssh pub key to all other nodes.

Verify that the drive we’re going to allocate for SUSE Entreprise Storage OSDs are clean by wiping them (refer to 4.3.12 of the deployment guide)

** link:https://documentation.suse.com/ses/5.5/html/ses-all/ceph-install-saltstack.html#ceph-install-stack[Wipe disk]

Install salt-minion on all nodes (admin also) and salt-master only on admin (in our example ses55-admin.suse-sap.net). Install also deepsea on admin node.

Then restart salt-minion on all nodes and restart salt-master on admin

image::multi-s-salt-install-restart.png[multi-s-salt-install-restart.png,480,640]

Accept related pending Salt key.

image::salt-key.png[salt-key.png,480,640]

Verify that /srv/pillar/ceph/master_minion.sls point to your admin node, in our example it contains our salt-master FQDN : master_minion: ses55-admin.suse-sap.net

Prepare the cluster

----
# salt-run state.orch ceph.stage.0
----

image::ceph-stage-0.png[ceph-stage-0.png,480,640]

Collect informations about the nodes :

----
# salt-run state.orch ceph.stage.1
----

image::ceph-stage-1.png[ceph-stage-1.png,480,640]

Adapt the file  /srv/pillar/ceph/proposals/policy.cfg to your needs.

In our example where the only deployed service is OpenAttic it contains the following :

----
cluster-ceph/cluster/ses55-osd2.suse-sap.net.sls
config/stack/default/ceph/cluster.yml
config/stack/default/global.yml
profile-default/cluster/ses55-admin.suse-sap.net.sls
profile-default/cluster/ses55-osd0.suse-sap.net.sls
profile-default/cluster/ses55-osd1.suse-sap.net.sls
profile-default/cluster/ses55-osd2.suse-sap.net.sls
profile-default/stack/default/ceph/minions/ses55-admin.suse-sap.net.yml
profile-default/stack/default/ceph/minions/ses55-osd0.suse-sap.net.yml
profile-default/stack/default/ceph/minions/ses55-osd1.suse-sap.net.yml
profile-default/stack/default/ceph/minions/ses55-osd2.suse-sap.net.yml
role-admin/cluster/ses55-admin.suse-sap.net.sls
role-admin/cluster/ses55-osd0.suse-sap.net.sls
role-admin/cluster/ses55-osd1.suse-sap.net.sls
role-admin/cluster/ses55-osd2.suse-sap.net.sls
role-master/cluster/ses55-admin.suse-sap.net.sls
role-mgr/cluster/ses55-osd0.suse-sap.net.sls
role-mgr/cluster/ses55-osd1.suse-sap.net.sls
role-mgr/cluster/ses55-osd2.suse-sap.net.sls
role-mon/cluster/ses55-osd0.suse-sap.net.sls
role-mon/cluster/ses55-osd1.suse-sap.net.sls
role-mon/cluster/ses55-osd2.suse-sap.net.sls
role-openattic/cluster/ses55-admin.suse-sap.net.sls
----

Prepare the final state of configuration files set.

----
# salt-run state.orch ceph.stage.2
----

image::ceph-stage-2.png[ceph-stage-2.png,480,640]

You can now safely deploy your configuration

----
# salt-run state.orch ceph.stage.3
----

image::ceph-stage-3.png[ceph-stage-3.png,480,640]

Once the stage 3 has been successfully passed, check the cluster health to insure that all is running properly

----
# ceph -s
----

image::ceph-health.png[ceph-health.png,480,640]

In order to take benefits of the OpenAttic WebUI you now have to initiate the ceph.stage.4 which will install OpenAttic service.

----
# salt-run state.orch ceph.stage.4
----

image::ceph-stage-4.png[ceph-stage-4.png,480,640]

You can now manage your cluster through the WebUI

image::openattic-dash.png[openattic-dash.png,480,640]

To provide a Datahub RBD device we need first to create a related pool.

image::openattic-pool.png[openattic-pool.png,480,640]

And provide access to this pool through a RBD device.

image::openattic-rbd.png[openattic-rbd.png,480,640]

You could now go to Section 4.1.2 of this documentation and follow the Prerequisites on the SUSE CaaS Platform 3 Cluster

=== Troubleshooting

// Standard SUSE Best Practices includes
include::common_gfdl1.2_i.adoc[]
