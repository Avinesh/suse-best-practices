:docinfo:

// = {title}
= MongoDB with SUSE Rancher: A Technical Quick Look

// SUSE Linux Enterprise Server 12 SP3 - SP5, SQL Server 2019
// :author: Samip Prikh
:revnumber: 0.0.1
:toc2:
:toc-title: MongoDB with SUSE Rancher A Technical Quick Look

:toclevels: 4

:sles: SUSE Linux Enterprise Server

== Motivation
Agility is the name of the game in modern application development.  This is driving the transformation of traditional development toward more agile methodologies, like DevOps, in which developers and operators work hand-in-hand to streamline the development-to-production workflow.  Underpinning this transformation is the move to microservices architectures, implemented with modern orchestration tools, like Kubernetes, that empower organizations to simplify their development and operations pipelines and accelerate their business goals.

The value of Kubernetes in a data solution lies in resilience and scalability.  Applications can be built with smaller, autonomous services focused on discrete, business-specific objectives or functions.  This makes it possible to scale data-aware software landscapes like never before and to create resilient structures to eliminate downtime and data loss.  Moreover, the modern, Kubernetes-powered, microservices architecture enables teams to focus on their own, discrete requirements while still integrating into the whole application stack.

SUSE Rancher gives organizations a robust, scalable, and efficient management platform to tame their entire Kubernetes landscape.  Deploying MongoDB with Rancher delivers highly available data services along with improved monitoring and management, resulting in optimizations across the organization.  Developers can focus on building data-rich applications that deliver innovative features to end-users and increase business value.  Operations can experience simplified management of robust, dynamic IT landscapes, which can span multiple sites and multiple geographic regions.  Business stakeholders can leverage these capabilities and efficiencies to accelerate business goals and gain market edge.

MongoDB offers a variety of compelling features additive to the SUSE Rancher Kubernetes management platform.  These include:

*Flexible Data Model:* MongoDB’s dynamic schema is ideal for handling changing requirements and continuous delivery.  You can seamlessly roll out new features without having to update existing records — a process that can take weeks for traditional, relational databases.  DevOps teams can quickly model data against an ever-changing environment and roll these changes into production, resulting in faster time to market and faster time to value.

*Resilience:* MongoDB’s replica sets have built-in redundancy, providing greater resilience and enhancing disaster recovery capabilities.  Administrators can even isolate operational workloads from analytical reporting in single database cluster to ensure sufficient resources are allocated to handle demand.

*Monitoring and Automation:*  Heterogenous services increase the level of complexity and can stall productivity. Technology that handles monitoring and automation is critical to keeping DevOps teams productive as their environments evolve. MongoDB Ops Manager features visualization, custom dashboards, and automated alerting.  It tracks, reports, processes, and visualizes 100+ key database and systems-health metrics, including operations counters, CPU utilization, replication status, and node status.

*Scalability:* MongoDB’s auto-sharding automatically partitions and distributes the database across nodes, serving IT infrastructures that require dynamic, high-performance capabilities.  Distribution can even span different geographic regions.  MongoDB is ideally suited to scale-out architectures.

All of this with SUSE, trusted by over two-thirds of Global Fortune 100 companies to deliver the open source, enterprise solutions that power their mission-critical operations.  With outstanding products and services from SUSE and partners, like MongoDB, our customers are empowered with the tools and support they need for success.


=== Technical Overview
SUSE Rancher is a lightweight Kubernetes installer that supports installation on bare-metal and virtualized servers.  Rancher solves a common issue in the Kubernetes community: installation complexity.  With Rancher, Kubernetes installation is simplified, regardless of what operating systems and platforms you are running.

This document reviews considerations for deploying and managing a highly available, MongoDB NoSQL database on a SUSE Rancher Kubernetes cluster.

In practice, the process is as follows:

* 1.Install a Kubernetes cluster through Rancher Kubernetes Engine
* 2.Install a cloud native storage solution on Kubernetes
* 3.Deploy https://docs.mongodb.com/kubernetes-operator/master/[MongoDB Enterprise Kubernetes Operator]
* 4.Configure a storage class and define storage requirements via Operator
* 5.Test failover by killing or cordoning nodes in your cluster

=== Value of HA in data terms
One of the primary benefits of running a Kubernetes environment is flexibility, the ability to easily adapt to varying circumstances.  Traditional database deployments exist in fairly static configurations and environments.  The beauty of running a data-oriented service on Kubernetes lies in maintaining stability while enabling adaptability to meet real-world situations.

Imagine a scenario where your e-commerce site is consistently taking 100 orders per day.  Suddenly a viral marketing event occurs, and your site is pushed to 5000 orders per day for a day.  This in-crease could easily lead to data overload – or, worse, corruption or downtime, which could result to considerable loss of revenue.  Having a way to design for such failure scenarios and maintain resilient operations is a tangible market advantage.

MongoDB can run in a single node configuration and in a clustered configuration using replica sets (not to be confused with Kubernetes Stateful Sets). A replica set is a group of MongoDB instances that maintain the same data. A replica set contains several data-bearing nodes and optionally one arbiter node. Of the data-bearing nodes, one and only one member is deemed the primary node, while the other nodes are deemed secondary nodes.  Resiliency of the data is achieved, as illustrated below.
IMAGE1

In this configuration the failover process generally completes within a minute.  It may take about 30 seconds for the members of a replica set to declare a primary inaccessible. One of the remaining secondaries will then be enabled as the “new primary.”  The election itself may take another 10 to 30 seconds.  During this time, the data will be preserved in a virtually seamless way for dependent services.

=== Setting up a cluster with SUSE Rancher

SUSE Rancher is a tool to install and configure Kubernetes in a choice of environments including bare metal, virtual machines, and IaaS. Rancher is a complete container management platform built on upstream Kubernetes.
It consists of three major components:
* A certified Kubernetes Distribution – Rancher Kubernetes Engine (RKE)
* A Kubernetes Management platform (Rancher)
* Application Catalog and management (Third-party)
Rancher has the capabilities to manage any Kubernetes cluster from a central location, via the
Rancher server.  As illustrated below, Rancher can manage any Kubernetes flavor and is not restricted to RKE.

INSERT IMAGES

For reference, see Rancher deployment guides for specific details on installation.    By the end of this step, you should have a cluster with one master and three worker nodes.

INSERT IMAGES

== Storage Considerations

When deploying an application that needs to retain data, you’ll need to create persistent storage. Persistent storage allows you to store application data external from the pod running your application. This storage practice allows you to maintain application data, even if the application’s pod fails.

A variety of storage options exist and can be used to create an HA data solution with Rancher.  Some considerations you may need to consider for your storage solution include:

* Volumes as persistent storage for the distributed stateful applications

* Partitioned block storage for Kubernetes volumes with or without a cloud provider

* Replicated block storage across multiple nodes and data centers to increase availability

* Secondary data backup storage (e.g., NFS or S3)

•	Cross-cluster disaster recovery volumes

•	Recurring volume snapshots

•	Recurring backups to secondary storage

•	Non-disruptive upgrades

Some common storage solutions to consider:

https://longhorn.io[Longhorn] - Distributed block storage system for Kubernetes. originally developed by Rancher Labs.  Currently sandbox project of the Cloud Native Computing Foundation

https://openebs.io[OpenEBS] – open source, CNCF Sandbox storage with flexible storage engine options - requires third-party integration

https://ceph.io[Ceph] – powerful, open source, general purpose storage in the CNCF Sandbox – requires third-party integration

https://portworx.com[Portworx] – proprietary solution with Rancher certified integration - installation steps can be found https://docs.portworx.com/install-with-other/rancher/rancher-2.x[here]

INSERT IMAGE HERE

=== Setting up your storage

Before proceeding, be sure that you understand the Kubernetes concepts of persistent volumes, persistent volume claims, and storage classes.  For more information, refer to https://rancher.com/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/how-storage-works[How Persistent Storage Works] in the Rancher documentation.

The workflow for setting up existing storage is as follows:
*1.	Ensure you have access to Set up your persistent storage. This may be storage in an infrastructure provider, or it could be your own storage.
*2.	Add a persistent volume (PV) that refers to the persistent storage.
*3.	Add a persistent volume claim (PVC) that refers to the PV.
*4.	Mount the PVC as a volume in your workload.

Visit the Rancher documentation section on https://rancher.com/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/attaching-existing-storage[Setting Up Existing Storage] for further details and prerequisites.

The overall workflow for provisioning new storage is as follows:
*1.	Add a StorageClass and configure it to use your storage provider. The StorageClass could refer to storage in an infrastructure provider, or it could refer to your own storage.
*2.	Add a persistent volume claim (PVC) that refers to the storage class.
*3.	Mount the PVC as a volume for your workload.

See https://rancher.com/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/provisioning-new-storage[Dynamically Provisioning New Storage in Rancher] for details and prerequisites.

=== Creating a storage class for MongoDB

Once the Kubernetes cluster is running and storage is configured, it is time to deploy a highly available MongoDB database.

MongoDB resources are created in Kubernetes as custom resources. After you create or update a MongoDB Kubernetes resource specification, you direct MongoDB Kubernetes Operator to apply this specification to your Kubernetes environment. Kubernetes Operator creates the defined StatefulSets, services and other Kubernetes resources. After the Operator finishes creating those objects, it updates the Ops Manager deployment configuration to reflect changes.

The following example shows a resource specification for a https://docs.mongodb.com/manual/reference/glossary/#term-replica-set[replica set] configuration

[source,bash]
----
apiVersion: mongodb.com/v1
kind: MongoDB
metadata:
  name: my-replica-set
spec:
  members: 3
  version: "4.2.2-ent"
  service: my-service
  opsManager: # Alias of cloudManager
    configMapRef:
      name: my-project
  credentials: my-credentials
  persistent: true
  type: ReplicaSet
  podSpec:
    cpu: "0.25"
    memory: "512M"
    persistence:
      multiple:
        data:
          storage: "10Gi"
        journal:
          storage: "1Gi"
          labelSelector:
            matchLabels:
              app: "my-app"
        logs:
          storage: "500M"
          storageClass: standard
    podAntiAffinityTopologyKey: nodeId
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: failure-domain.beta.kubernetes.io/zone
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
    podTemplate:
      metadata:
        labels:
          label1: mycustomlabel
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  topologyKey: "mykey"
                weight: 50
  security:
    tls:
      enabled: true
    authentication:
      enabled: true
      modes: ["X509"]
      internalCluster: "X509"
  additionalMongodConfig:
    net:
      ssl:
        mode: preferSSL
----

Full details can be found https://docs.mongodb.com/kubernetes-operator/master/reference/k8s-operator-specification[here].

=== Creating a Persistent Volume
We can now create a Persistent Volume Claim (PVC) based on the Storage Class. Dynamic provisioning will be created without explicitly provisioning a persistent volume (PV). As part of deployment, the Kubernetes Operator creates https://kubernetes.io/docs/concepts/storage/persistent-volumes[Persistent Volumes] for the Ops Manager StatefulSets. The Kubernetes container uses Persistent Volumes to maintain the cluster state between restarts.


=== Deploying MongoDB Operator on Kubernetes
Kubernetes needs help creating and managing stateful applications like databases.  The typical lifecycle events of a MongoDB cluster may include provisioning storage and computing power, configuring network connections, setting up users, and more.  This is where the MongoDB Enterprise Kubernetes Operator comes in.  It translates the human knowledge of how to create a MongoDB instance into a scalable, repeatable, and standardized methodology.  And it does this by using the built-in Kubernetes API and tools.
To use the operator, you simply need to provide it with the specifications for your MongoDB cluster.  The operator uses this information to direct Kubernetes into performing all the required steps to achieve the end state.

The general commands for deploying the MongoDB Enterprise Operator are:

[source,bash]
----
kubectl describe deployments mongodb-enterprise-operator -n <namespace>


helm install <chart-name> helm_chart \
     --values helm_chart/values.yaml \
----


The next step after deploying the operator is to create the database using a yaml file, such as:

[source,bash]
----
apiVersion: mongodb.com/v1
kind: MongoDB
metadata:

  name: <my-standalone>

spec:

  version: "4.2.2-ent"

  opsManager:
    configMapRef:

      name: <configMap.metadata.name>

            # Must match metadata.name in ConfigMap file

  credentials: <mycredentials>

  type: Standalone
  persistent: true
----

Now, we can deploy the database and check the status of the deployment with:
[source,bash]
----
kubectl apply -f <standalone-conf>.yaml

kubectl get mdb <resource-name> -o yaml
----

At this point, MongoDB has been deployed via the operator.  Additional settings can be applied to create https://docs.mongodb.com/kubernetes-operator/stable/tutorial/deploy-replica-set/[replica] sets to further enhance data availability.  Also, sharded clusters can be created to ena-ble greater throughput across a distributed system.

Additionally, see specific https://github.com/mongodb/mongodb-enterprise-kubernetes[documentation] and steps for full installation and configuration options.

=== Deploying Ops Manager Resource
Kubernetes needs help creating and managing stateful applications like databases.  The typical lifecycle events of a MongoDB cluster may include provisioning, storage and compute. Ops Manager can be deployed via the Operator to manage MongoDB resources in a cluster.  The Operator manages the lifecycle of each of these deployments differently.
The Operator manages Ops Manager deployments using the Ops Manager custom resource. The Operator watches the custom resource’s specification for changes. When the specification changes, the Operator validates the changes and makes the appropriate updates to the resources in the cluster.
Ops Manager custom resources specification defines the following Ops Manager components: Application Database, Ops Manager application, and Backup Daemon. Summarized instructions to create Ops Manager are below, but full details can be found https://docs.mongodb.com/kubernetes-operator/master/tutorial/deploy-om-container[here].

To begin, run the following command to execute all kubectl commands in the namespace you created:
[source,bash]
----
kubectl config set-context $(kubectl config current-context) --namespace=<namespace>
----
Create Ops Manager object as below
[source,bash]
----
apiVersion: mongodb.com/v1
kind: MongoDBOpsManager
metadata:
  name: <myopsmanager>
spec:
  replicas: 1
  version: <opsmanagerversion>
  adminCredentials: <adminusercredentials> # Should match metadata.name
  externalConnectivity:
    type: LoadBalancer
  applicationDatabase:
    members: 3
    version: <mongodbversion>
    persistent: true
----


=== Loading and querying the database

Once the database has been created, we can populate it with some sample data.

We first find the pod that is running MongoDB:

[source,bash]
----
POD=`kubectl get pods -l app=mongo | grep Running | grep 1/1 | awk '{print $1}'`
----

Then, access the MongoDB shell on that POD instance:

[source,bash]
----
$ kubectl exec -it $POD mongo
MongoDB shell version v4.0.0
connecting to: mongodb://127.0.0.1:27017
MongoDB server version: 4.0.0
Welcome to the MongoDB shell.
----

Now, using the MongoDB shell, we can populate a collection:
[source,bash]
----
db.ships.insert({name:'USS Enterprise-D',operator:'Starfleet',type:'Explorer',class:'Galaxy',crew:750,codes:[10,11,12]})
db.ships.insert({name:'USS Prometheus',operator:'Starfleet',class:'Prometheus',crew:4,codes:[1,14,17]})
db.ships.insert({name:'USS Defiant',operator:'Starfleet',class:'Defiant',crew:50,codes:[10,17,19]})
db.ships.insert({name:'IKS Buruk',operator:' Klingon Empire',class:'Warship',crew:40,codes:[100,110,120]})
db.ships.insert({name:'IKS Somraw',operator:' Klingon Empire',class:'Raptor',crew:50,codes:[101,111,120]})
db.ships.insert({name:'Scimitar',operator:'Romulan Star Empire',type:'Warbird',class:'Warbird',crew:25,codes:[201,211,220]})
db.ships.insert({name:'Narada',operator:'Romulan Star Empire',type:'Warbird',class:'Warbird',crew:65,codes:[251,251,220]})
----

And we can run some operations on the MongoDB collection.
For example, find one arbitrary document:

[source,bash]
----
db.ships.findOne()
{
	"_id" : ObjectId("5b5c16221108c314d4c000cd"),
	"name" : "USS Enterprise-D",
	"operator" : "Starfleet",
	"type" : "Explorer",
	"class" : "Galaxy",
	"crew" : 750,
	"codes" : [
		10,
		11,
		12
	]
}
----

 Or find ALL documents and apply some basic formatting:

[source,bash]
----
db.ships.find().pretty()
 {
 	"_id" : ObjectId("5b5c16221108c314d4c000d1"),
 	"name" : "IKS Somraw",
 	"operator" : " Klingon Empire",
 	"class" : "Raptor",
 	"crew" : 50,
 	"codes" : [
 		101,
 		111,
 		120
 	]
 }
 {
 	"_id" : ObjectId("5b5c16221108c314d4c000d2"),
 	"name" : "Scimitar",
 	"operator" : "Romulan Star Empire",
 	"type" : "Warbird",
 	"class" : "Warbird",
 	"crew" : 25,
 	"codes" : [
 		201,
 		211,
 		220
 	]
 }
----

 And return a list of the names of the ships:

[source,bash]
----
 db.ships.find({}, {name:true, _id:false})
 { "name" : "USS Enterprise-D" }
 { "name" : "USS Prometheus" }
 { "name" : "USS Defiant" }
 { "name" : "IKS Buruk" }
 { "name" : "IKS Somraw" }
 { "name" : "Scimitar" }
 { "name" : "Narada" }
----


=== Simulating node failure & restoration
Now, let’s simulate a node failure by cordoning off the node on which MongoDB is running.
[source,bash]
----
$ NODE=`kubectl get pods -l app=mongo -o wide | grep -v NAME | awk '{print $7}'`

$ kubectl cordon ${NODE} node/ip-172-31-29-132.compute.internal cordoned
----

The above command disables scheduling on one of the nodes.

We can check this with:
[source,bash]
----
$ kubectl get nodes
NAME                                           STATUS                     ROLES               AGE   VERSION
ip-172-31-24-121.compute.internal   Ready                      worker              47h   v1.13.4
ip-172-31-26-49.compute.internal    Ready                      controlplane,etcd   47h   v1.13.4
ip-172-31-28-65.compute.internal    Ready                      worker              47h   v1.13.4
ip-172-31-29-132.compute.internal   Ready,SchedulingDisabled   worker              47h   v1.13.4
----

Now, let’s go ahead and delete the MongoDB pod.

[source,bash]
----
$ POD=`kubectl get pods -l app=mongo -o wide | grep -v NAME | awk '{print $1}'`
$ kubectl delete pod ${POD}
pod "mongo-68cc69bc95-7q96h" deleted
----

As soon as the pod is deleted, it is relocated to the node with the replicated data, even when that node is in a different zone.  Each pod is rescheduled on the exact node where the data is stored.

Let’s verify this.

[source,bash]
----
$ kubectl get pods -l app=mongo -o wide
NAME                     READY     STATUS    RESTARTS   AGE       IP               NODE
mongo-68cc69bc95-thqbm   1/1       Running   0          19s       192.168.82.119   ip-172-31-24-121.compute.internal
----

Notice that a new pod has been created and scheduled in a different node.

Next, let’s uncordon the node to bring it back to action.
[source,bash]
----
$ kubectl uncordon ${NODE}
node/ip-172-31-29-132.compute.internal uncordoned
----


Finally, let’s verify that the data is still available.

First, find the pod name and run the ‘exec’ command.  Then, access the Mongo shell.
[source,bash]
----
POD=`kubectl get pods -l app=mongo | grep Running | grep 1/1 | awk '{print $1}'`
kubectl exec -it $POD mongo
MongoDB shell version v4.0.0
connecting to: mongodb://127.0.0.1:27017
MongoDB server version: 4.0.0
Welcome to the MongoDB shell.
----


Now, let's query the collection to verify that the data is intact.

Find one arbitrary document:

[source,bash]
----
db.ships.findOne()
{
	"_id" : ObjectId("5b5c16221108c314d4c000cd"),
	"name" : "USS Enterprise-D",
	"operator" : "Starfleet",
	"type" : "Explorer",
	"class" : "Galaxy",
	"crew" : 750,
	"codes" : [
		10,
		11,
		12
	]
}
----


And find all documents and apply formatting:
[source,bash]
----
db.ships.find().pretty()
…..
{
	"_id" : ObjectId("5b5c16221108c314d4c000d1"),
	"name" : "IKS Somraw",
	"operator" : " Klingon Empire",
	"class" : "Raptor",
	"crew" : 50,
	"codes" : [
		101,
		111,
		120
	]
}
{
	"_id" : ObjectId("5b5c16221108c314d4c000d2"),
	"name" : "Scimitar",
	"operator" : "Romulan Star Empire",
	"type" : "Warbird",
	"class" : "Warbird",
	"crew" : 25,
	"codes" : [
		201,
		211,
		220
	]
}
----

For further validation, feel free to run all the same initial steps to compare that data is still available with originally queried values.

=== Summary

MongoDB with SUSE Rancher makes it possible to easily build, deploy, and manage highly available data services.

=== Additional resources
*	https://rancher.com/docs/rancher/v2.x/en/best-practices/[Rancher best practices guide]
*	https://rancher.com/docs/rancher/v2.x/en/troubleshooting/[Rancher troubleshooting tips]
*	https://github.com/mongodb/mongodb-kubernetes-operator[MongoDB best practices]


:leveloffset: 0
// Standard SUSE Best Practices includes
== Legal notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
// include::common_gfdl1.2_i.adoc[]

:leveloffset: 0
include::common_gfdl1.2_i.adoc[]
