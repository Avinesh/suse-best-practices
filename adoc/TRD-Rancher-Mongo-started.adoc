:docinfo:

// = {title}
= MongoDB with SUSE Rancher: A Technical Quick Look

// SUSE Linux Enterprise Server 12 SP3 - SP5, SQL Server 2019
// :author: Samip Prikh
:revnumber: 0.0.1
:toc2:
:toc-title: MongoDB with SUSE Rancher A Technical Quick Look

:toclevels: 4

:sles: SUSE Linux Enterprise Server

== Motivation
Agility is the name of the game in modern application development.  This is driving the transformation of traditional development toward more agile methodologies, like DevOps, in which developers and operators work hand-in-hand to streamline the development-to-production workflow.  Underpinning this transformation is the move to microservices architectures, implemented with modern orchestration tools, like Kubernetes, that empower organizations to simplify their development and operations pipelines and accelerate their business goals.

The value of Kubernetes in a data solution lies in resilience and scalability.  Applications can be built with smaller, autonomous services focused on discrete, business-specific objectives or functions.  This makes it possible to scale data-aware software landscapes like never before and to create resilient structures to eliminate downtime and data loss.  Moreover, the modern, Kubernetes-powered, microservices architecture enables teams to focus on their own, discrete requirements while still integrating into the whole application stack.

SUSE Rancher gives organizations a robust, scalable, and efficient management platform to tame their entire Kubernetes landscape.  Deploying MongoDB with Rancher delivers highly available data services along with improved monitoring and management, resulting in optimizations across the organization.  Developers can focus on building data-rich applications that deliver innovative features to end-users and increase business value.  Operations can experience simplified management of robust, dynamic IT landscapes, which can span multiple sites and multiple geographic regions.  Business stakeholders can leverage these capabilities and efficiencies to accelerate business goals and gain market edge.

MongoDB offers a variety of compelling features additive to the SUSE Rancher Kubernetes management platform.  These include:

*Flexible Data Model:* MongoDB’s dynamic schema is ideal for handling changing requirements and continuous delivery.  You can seamlessly roll out new features without having to update existing records — a process that can take weeks for traditional, relational databases.  DevOps teams can quickly model data against an ever-changing environment and roll these changes into production, resulting in faster time to market and faster time to value.

*Resilience:* MongoDB’s replica sets have built-in redundancy, providing greater resilience and enhancing disaster recovery capabilities.  Administrators can even isolate operational workloads from analytical reporting in single database cluster to ensure sufficient resources are allocated to handle demand.

*Monitoring and Automation:*  Heterogenous services increase the level of complexity and can stall productivity. Technology that handles monitoring and automation is critical to keeping DevOps teams productive as their environments evolve. MongoDB Ops Manager features visualization, custom dashboards, and automated alerting.  It tracks, reports, processes, and visualizes 100+ key database and systems-health metrics, including operations counters, CPU utilization, replication status, and node status.

*Scalability:* MongoDB’s auto-sharding automatically partitions and distributes the database across nodes, serving IT infrastructures that require dynamic, high-performance capabilities.  Distribution can even span different geographic regions.  MongoDB is ideally suited to scale-out architectures.

All of this with SUSE, trusted by over two-thirds of Global Fortune 100 companies to deliver the open source, enterprise solutions that power their mission-critical operations.  With outstanding products and services from SUSE and partners, like MongoDB, our customers are empowered with the tools and support they need for success.


=== Technical Overview
SUSE Rancher is a lightweight Kubernetes installer that supports installation on bare-metal and virtualized servers.  Rancher solves a common issue in the Kubernetes community: installation complexity.  With Rancher, Kubernetes installation is simplified, regardless of what operating systems and platforms you are running.

This document reviews considerations for deploying and managing a highly available, MongoDB NoSQL database on a SUSE Rancher Kubernetes cluster.

In practice, the process is as follows:

* 1.Install a Kubernetes cluster through Rancher Kubernetes Engine
* 2.Install a cloud native storage solution on Kubernetes
* 3.Deploy https://docs.mongodb.com/kubernetes-operator/master/[MongoDB Enterprise Kubernetes Operator]
* 4.Configure a storage class and define storage requirements via Operator
* 5.Test failover by killing or cordoning nodes in your cluster


=== Setting up a cluster with SUSE Rancher

SUSE Rancher is a tool to install and configure Kubernetes in a choice of environments including bare metal, virtual machines, and IaaS. Rancher is a complete container management platform built on upstream Kubernetes.
It consists of three major components:
* A certified Kubernetes Distribution – Rancher Kubernetes Engine (RKE)
* A Kubernetes Management platform (Rancher)
* Application Catalog and management (Third-party)
Rancher has the capabilities to manage any Kubernetes cluster from a central location, via the
Rancher server.  As illustrated below, Rancher can manage any Kubernetes flavor and is not restricted to RKE.

INSERT IMAGES

For reference, see Rancher deployment guides for specific details on installation.    By the end of this step, you should have a cluster with one master and three worker nodes.

INSERT IMAGES

== Storage Considerations

When deploying an application that needs to retain data, you’ll need to create persistent storage. Persistent storage allows you to store application data external from the pod running your application. This storage practice allows you to maintain application data, even if the application’s pod fails.

A variety of storage options exist and can be used to create an HA data solution with Rancher.  Some considerations you may need to consider for your storage solution include:

* Volumes as persistent storage for the distributed stateful applications

* Partitioned block storage for Kubernetes volumes with or without a cloud provider

* Replicated block storage across multiple nodes and data centers to increase availability

* Secondary data backup storage (e.g., NFS or S3)

•	Cross-cluster disaster recovery volumes

•	Recurring volume snapshots

•	Recurring backups to secondary storage

•	Non-disruptive upgrades

Some common storage solutions to consider:

https://longhorn.io[Longhorn] - Distributed block storage system for Kubernetes. originally developed by Rancher Labs.  Currently sandbox project of the Cloud Native Computing Foundation

https://openebs.io[OpenEBS] – open source, CNCF Sandbox storage with flexible storage engine options - requires third-party integration

https://ceph.io[Ceph] – powerful, open source, general purpose storage in the CNCF Sandbox – requires third-party integration

https://portworx.com[Portworx] – proprietary solution with Rancher certified integration - installation steps can be found https://docs.portworx.com/install-with-other/rancher/rancher-2.x[here]

INSERT IMAGE HERE

=== Setting up your storage

Before proceeding, be sure that you understand the Kubernetes concepts of persistent volumes, persistent volume claims, and storage classes.  For more information, refer to https://rancher.com/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/how-storage-works[How Persistent Storage Works] in the Rancher documentation.

The workflow for setting up existing storage is as follows:
*1.	Ensure you have access to Set up your persistent storage. This may be storage in an infrastructure provider, or it could be your own storage.
*2.	Add a persistent volume (PV) that refers to the persistent storage.
*3.	Add a persistent volume claim (PVC) that refers to the PV.
*4.	Mount the PVC as a volume in your workload.

Visit the Rancher documentation section on https://rancher.com/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/attaching-existing-storage[Setting Up Existing Storage] for further details and prerequisites.

The overall workflow for provisioning new storage is as follows:
*1.	Add a StorageClass and configure it to use your storage provider. The StorageClass could refer to storage in an infrastructure provider, or it could refer to your own storage.
*2.	Add a persistent volume claim (PVC) that refers to the storage class.
*3.	Mount the PVC as a volume for your workload.

See https://rancher.com/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/provisioning-new-storage[Dynamically Provisioning New Storage in Rancher] for details and prerequisites.

=== Creating a storage class for MongoDB

Once the Kubernetes cluster is running and storage is configured, it is time to deploy a highly available MongoDB database.

MongoDB resources are created in Kubernetes as custom resources. After you create or update a MongoDB Kubernetes resource specification, you direct MongoDB Kubernetes Operator to apply this specification to your Kubernetes environment. Kubernetes Operator creates the defined StatefulSets, services and other Kubernetes resources. After the Operator finishes creating those objects, it updates the Ops Manager deployment configuration to reflect changes.

The following example shows a resource specification for a https://docs.mongodb.com/manual/reference/glossary/#term-replica-set[replica set] configuration

[source,bash]
----
apiVersion: mongodb.com/v1
kind: MongoDB
metadata:
  name: my-replica-set
spec:
  members: 3
  version: "4.2.2-ent"
  service: my-service
  opsManager: # Alias of cloudManager
    configMapRef:
      name: my-project
  credentials: my-credentials
  persistent: true
  type: ReplicaSet
  podSpec:
    cpu: "0.25"
    memory: "512M"
    persistence:
      multiple:
        data:
          storage: "10Gi"
        journal:
          storage: "1Gi"
          labelSelector:
            matchLabels:
              app: "my-app"
        logs:
          storage: "500M"
          storageClass: standard
    podAntiAffinityTopologyKey: nodeId
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: failure-domain.beta.kubernetes.io/zone
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
    podTemplate:
      metadata:
        labels:
          label1: mycustomlabel
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  topologyKey: "mykey"
                weight: 50
  security:
    tls:
      enabled: true
    authentication:
      enabled: true
      modes: ["X509"]
      internalCluster: "X509"
  additionalMongodConfig:
    net:
      ssl:
        mode: preferSSL
----

Full details can be found https://docs.mongodb.com/kubernetes-operator/master/reference/k8s-operator-specification[here].

=== Creating a Persistent Volume
We can now create a Persistent Volume Claim (PVC) based on the Storage Class. Dynamic provisioning will be created without explicitly provisioning a persistent volume (PV). As part of deployment, the Kubernetes Operator creates https://kubernetes.io/docs/concepts/storage/persistent-volumes[Persistent Volumes] for the Ops Manager StatefulSets. The Kubernetes container uses Persistent Volumes to maintain the cluster state between restarts.


=== Deploying MongoDB Operator on Kubernetes
Kubernetes needs help creating and managing stateful applications like databases.  The typical lifecycle events of a MongoDB cluster may include provisioning storage and computing power, configuring network connections, setting up users, and more.  This is where the MongoDB Enterprise Kubernetes Operator comes in.  It translates the human knowledge of how to create a MongoDB instance into a scalable, repeatable, and standardized methodology.  And it does this by using the built-in Kubernetes API and tools.
To use the operator, you simply need to provide it with the specifications for your MongoDB cluster.  The operator uses this information to direct Kubernetes into performing all the required steps to achieve the end state.

The general commands for deploying the MongoDB Enterprise Operator are:

[source,bash]
----
kubectl describe deployments mongodb-enterprise-operator -n <namespace>


helm install <chart-name> helm_chart \
     --values helm_chart/values.yaml \
----


The next step after deploying the operator is to create the database using a yaml file, such as:

[source,bash]
----
apiVersion: mongodb.com/v1
kind: MongoDB
metadata:

  name: <my-standalone>

spec:

  version: "4.2.2-ent"

  opsManager:
    configMapRef:

      name: <configMap.metadata.name>

            # Must match metadata.name in ConfigMap file

  credentials: <mycredentials>

  type: Standalone
  persistent: true
----

Now, we can deploy the database and check the status of the deployment with:
[source,bash]
----
kubectl apply -f <standalone-conf>.yaml

kubectl get mdb <resource-name> -o yaml
----

At this point, MongoDB has been deployed via the operator.  Additional settings can be applied to create https://docs.mongodb.com/kubernetes-operator/stable/tutorial/deploy-replica-set/[replica] sets to further enhance data availability.  Also, sharded clusters can be created to ena-ble greater throughput across a distributed system.

Additionally, see specific https://github.com/mongodb/mongodb-enterprise-kubernetes[documentation] and steps for full installation and configuration options.

=== Deploying Ops Manager Resource
Kubernetes needs help creating and managing stateful applications like databases.  The typical lifecycle events of a MongoDB cluster may include provisioning, storage and compute. Ops Manager can be deployed via the Operator to manage MongoDB resources in a cluster.  The Operator manages the lifecycle of each of these deployments differently.
The Operator manages Ops Manager deployments using the Ops Manager custom resource. The Operator watches the custom resource’s specification for changes. When the specification changes, the Operator validates the changes and makes the appropriate updates to the resources in the cluster.
Ops Manager custom resources specification defines the following Ops Manager components: Application Database, Ops Manager application, and Backup Daemon. Summarized instructions to create Ops Manager are below, but full details can be found https://docs.mongodb.com/kubernetes-operator/master/tutorial/deploy-om-container[here].

To begin, run the following command to execute all kubectl commands in the namespace you created:
[source,bash]
----
kubectl config set-context $(kubectl config current-context) --namespace=<namespace>
----
Create Ops Manager object as below
[source,bash]
----
apiVersion: mongodb.com/v1
kind: MongoDBOpsManager
metadata:
  name: <myopsmanager>
spec:
  replicas: 1
  version: <opsmanagerversion>
  adminCredentials: <adminusercredentials> # Should match metadata.name
  externalConnectivity:
    type: LoadBalancer
  applicationDatabase:
    members: 3
    version: <mongodbversion>
    persistent: true
----


=== Loading and querying the database

Once the database has been created, we can populate it with some sample data.

We first find the pod that is running MongoDB:

[source,bash]
----
POD=`kubectl get pods -l app=mongo | grep Running | grep 1/1 | awk '{print $1}'`
----

Then, access the MongoDB shell on that POD instance:

[source,bash]
----
$ kubectl exec -it $POD mongo
MongoDB shell version v4.0.0
connecting to: mongodb://127.0.0.1:27017
MongoDB server version: 4.0.0
Welcome to the MongoDB shell.
----

Now, using the MongoDB shell, we can populate a collection:
[source,bash]
----
db.ships.insert({name:'USS Enterprise-D',operator:'Starfleet',type:'Explorer',class:'Galaxy',crew:750,codes:[10,11,12]})
db.ships.insert({name:'USS Prometheus',operator:'Starfleet',class:'Prometheus',crew:4,codes:[1,14,17]})
db.ships.insert({name:'USS Defiant',operator:'Starfleet',class:'Defiant',crew:50,codes:[10,17,19]})
db.ships.insert({name:'IKS Buruk',operator:' Klingon Empire',class:'Warship',crew:40,codes:[100,110,120]})
db.ships.insert({name:'IKS Somraw',operator:' Klingon Empire',class:'Raptor',crew:50,codes:[101,111,120]})
db.ships.insert({name:'Scimitar',operator:'Romulan Star Empire',type:'Warbird',class:'Warbird',crew:25,codes:[201,211,220]})
db.ships.insert({name:'Narada',operator:'Romulan Star Empire',type:'Warbird',class:'Warbird',crew:65,codes:[251,251,220]})
----

And we can run some operations on the MongoDB collection.
For example, find one arbitrary document:

[source,bash]
----
db.ships.findOne()
{
	"_id" : ObjectId("5b5c16221108c314d4c000cd"),
	"name" : "USS Enterprise-D",
	"operator" : "Starfleet",
	"type" : "Explorer",
	"class" : "Galaxy",
	"crew" : 750,
	"codes" : [
		10,
		11,
		12
	]
}
----

 Or find ALL documents and apply some basic formatting:

[source,bash]
----
db.ships.find().pretty()
 {
 	"_id" : ObjectId("5b5c16221108c314d4c000d1"),
 	"name" : "IKS Somraw",
 	"operator" : " Klingon Empire",
 	"class" : "Raptor",
 	"crew" : 50,
 	"codes" : [
 		101,
 		111,
 		120
 	]
 }
 {
 	"_id" : ObjectId("5b5c16221108c314d4c000d2"),
 	"name" : "Scimitar",
 	"operator" : "Romulan Star Empire",
 	"type" : "Warbird",
 	"class" : "Warbird",
 	"crew" : 25,
 	"codes" : [
 		201,
 		211,
 		220
 	]
 }
----

 And return a list of the names of the ships:

[source,bash]
----
 db.ships.find({}, {name:true, _id:false})
 { "name" : "USS Enterprise-D" }
 { "name" : "USS Prometheus" }
 { "name" : "USS Defiant" }
 { "name" : "IKS Buruk" }
 { "name" : "IKS Somraw" }
 { "name" : "Scimitar" }
 { "name" : "Narada" }
----



=== References

* https://documentation.suse.com/sles/12-SP5/[SUSE Linux Enterprise Server 12 SP5]
* https://documentation.suse.com/external-tree/en-us/suma/4.1/suse-manager/index.html[SUSE Manager 4.1 Documentation]
* https://documentation.suse.com/sles/12-SP5/single-html/SLES-smt/#book-smt[Subscription Management Tool Guide]
* https://documentation.suse.com/sles/15-SP2/single-html/SLES-rmt/#book-rmt[Repository Mirroring Tool Guide]
* https://www.suse.com/support/kb/doc/?id=000019401[How to register a SLE system against a SMT server]
* https://www.suse.com/campaign/sql-server-on-linux/[The Power of SQL Server on Linux]
* https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup?view=sql-server-ver15[Installation guidance for SQL Server on Linux]
* https://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-suse?view=sql-server-ver15[Quickstart: Install SQL Server and create a database on SUSE Linux Enterprise Server]
* https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-configure-mssql-conf?view=sql-server-ver15[Configure SQL Server on Linux with the mssql-conf tool]
* https://docs.microsoft.com/en-us/sql/tools/sqlcmd-utility?view=sql-server-ver15[sqlcmd Utility]
* https://www.suse.com/lp/geek-guide-sql-server-on-suse[Modernizing with SQL Server on Linux for a Cloud Native World]
* https://github.com/microsoft/sql-server-samples[sql-server-samples]


:leveloffset: 0
// Standard SUSE Best Practices includes
== Legal notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
// include::common_gfdl1.2_i.adoc[]

:leveloffset: 0
include::common_gfdl1.2_i.adoc[]
