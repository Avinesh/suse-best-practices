:docinfo:

= SAP Data Intelligence 3.1 on Rancher Kubernetes Engine 1  

++++
<?pdfpagebreak?>
++++
== Introduction

This guide describes the installation of SAP Data Intelligence 3.1 on premise on top of Rancher Kubernetes Engine (RKE) 1. In a nutshell the installation of SAP DI 3.1 consists of the following steps.

* Install SUSE Linux Enterprise Server 15 SP2

* Install RKE 1 Kubernetes cluster on the dedicated nodes

* Deploy SAP DI 3.1 on RKE 1 Kubernetes cluster

* Post-Installation steps for SAP DI 3.1

* Test the installation of SAP DI 3.1
 

++++
<?pdfpagebreak?>
++++
== Pre-requisites

=== Hardware Requirements

This chapter describes the hardware requirements for installing SAP DI 3.1 on RKE 1 on top of SUSE Linux Enterprise Server 15 SP2.
Only x86_64 architecture is applicable for this use case.

====  Hardware Sizing

* Minimal hardware requirements for a SAP DI 3 deployment 
** At least 7 nodes are needed for a minimal Kubernetes cluster

[cols="40,.^10,15,.^10,25",options="header"]
|===
|Server Role |Count|RAM|CPU|Disk space
|Management Workstation|1|16 GiB|4|>100 GiB
|Master Node|3|16 GiB|4|>120 GiB
|Worker Node|4|32 GiB|8|>120 GiB
|===


* Minimal hardware requirements for production
** At least 7 nodes are needed for a production grade Kubernetes cluster

[cols="40,.^10,15,.^10,25",options="header"]
|===
|Server Role|Count|RAM|CPU|Disk space
|Management Workstation|1|16 GiB|4|>100 GiB
|Master Node|3|16 GiB|4|>120 GiB
|Worker Node|4|64 GiB|16|>120 GiB
|===

* see Rancher RKE documentation https://rancher.com/docs/rke/latest/en/os/ 

* For more sizing information see the SAP documentation that can be found here:
** https://help.sap.com/viewer/835f1e8d0dde4954ba0f451a9d4b5f10/3.1.latest/en-US

=== Software Requirements

Here we list the software components needed to install SAP DI 3.1 on RKE 1:

* SUSE Linux Enterprise Server 15 SP2

* Rancher RKE 1

* SAP Software Lifecycle Bridge

* SAP Data Intelligence 3.1

* Secure private registry for container images

* Access to a storage solution providing dynamically physical volumes

* If it is planned to use Vora's streaming tables checkpoint store, a S3 bucket like object store is needed 

* If it is planned to enable backup of SAP DI 3.1 during installation access to a S3 compatible object store is needed

++++
<?pdfpagebreak?>
++++
== Preparations

* Get SUSE SLES subscription

* download installer for SLES 15 SP2

* download rke binary

* check storage requirements

* create or get access to a private container registry

* get an SAP S-user to access software and documentation by SAP

* read relevant SAP documentation
** Release Note for SAP DI 3 https://launchpad.support.sap.com/#/notes/2871970
** SAP SLCBridge
** Installation Guide at help.sap.com https://help.sap.com/viewer/a8d90a56d61a49718ebcb5f65014bbe7/3.1.latest/en-US


++++
<?pdfpagebreak?>
++++
== Installation of Rancher RKE 1 Cluster

The installation of Rancher Kubernetes Engine 1 cluster is straight forward. After the installation and basic configuration of the operating system the Kubernetes cluster configuration is created on the management host. Finally the Kubernetes cluster will be deployed. The following sections will describe this in more detail.

===  Preparation of Management Host and Kubernetes Cluster Nodes

All servers in this landscape will use SUSE Enterprise Linux 15 SP2 (SLES 15 SP2) for x86_64 architecture.

* See https://documentation.suse.com/sles/15-SP2/

==== Installation of SUSE Linux Enterprise Server 15 SP2

On each server in your environment for SAP Data Intelligence 3.1 install SLES 15 SP2 as operating system.
This chapter will show all recommended steps when during the installation.

TIP: If you already have all machines and the OS setup, you may skip this chapter and follow the instructions at <<Configuration of Kubernetes nodes>>.

++++
<?pdfpagebreak?>
++++
It's recommended to use a static network configuration. During installation setup the first time to adjust this is when the Registration page is displayed. In the upper right corner is a button "Network Configuration ...".

image::SLES15_SP2_Setup_Registration.png[SLES Setup Registration Page, 480, 640]

++++
<?pdfpagebreak?>
++++
When clicked, the Network Settings page is shown. By default the network adapter is configured to use DHCP.
To change this, click the Button "Edit".

image::SLES15_SP2_Setup_Network_Settings.png[SLES Setup Network Settings, 480, 640]

++++
<?pdfpagebreak?>
++++
On the Network Card Setup page, select "Statically Assigned IP Address" and fill out the fields "IP Address", "Subnet Mask" and "Hostname".

image::SLES15_SP2_Setup_Network_Card_Setup.png[SLES Setup Network Card, 480, 640]

++++
<?pdfpagebreak?>
++++
Next thing to adjust during the installation are the extensions to be installed.
The Container Module is needed to operate RKE and Docker.

image::SLES15_SP2_Setup_Extensions.png[SLES Setup Extensions, 480, 640]

++++
<?pdfpagebreak?>
++++
Further, as there's no graphical interface needed, it's recommended to install just a text based server.

image::SLES15_SP2_Setup_SystemRole.png[SLES Setup System Role, 480, 640]

++++
<?pdfpagebreak?>
++++
To run Kubernetes the swap partition needs to be disabled.
To achieve this the partition proposal during installation can be adjusted.

image::SLES15_SP2_Setup_Partitioning_Expanded.png[SLES Setup Partitioning, 480, 640]

++++
<?pdfpagebreak?>
++++
When opening the Expert Partitioner, the Swap partition needs to be selected to delete it.

image::SLES15_SP2_Setup_Expert_Partitioner.png[SLES Setup Expert Partitioner Swap, 480, 640]

++++
<?pdfpagebreak?>
++++
After deleting the swap partition, there will be some space left that can be used to enlarge the main partition.
To do so, the resize page can be called.

image::SLES15_SP2_Setup_Expert_Partitioner3.png[SLES Setup Expert Partitioner Resize, 480, 640]

++++
<?pdfpagebreak?>
++++
Easiest way to use all the unused space is to select the "Maximum Size" option there.

image::SLES15_SP2_Setup_Resize_Disk.png[SLES Setup Resize Disk, 480, 640]

++++
<?pdfpagebreak?>
++++
Next thing to do is to enable the NTP time syncronization.
This can be done when facing the "Clock and Time Zone" page during installation.
To enable NTP, the "Other Settings ..." button needs to be clicked.

image::SLES15_SP2_Setup_Clock_and_Time.png[SLES Setup Timezone, 480, 640]

++++
<?pdfpagebreak?>
++++
Then the "Synchronize with NTP Server" option needs to be selected.
A custom NTP server adress can be added if desired.
Important is to check in the boxes for "Run NTP as daemon" and "Save NTP Configuration" 

image::SLES15_SP2_Setup_NTP.png[SLES Setup NTP, 480, 640]

++++
<?pdfpagebreak?>
++++
When facing the "Installation Settings" page, it's recommended to make sure that:
* The firewall will be disabled
* The SSH service will be enabled
* Kdump status is disabled

image::SLES15_SP2_Setup_Summary.png[SLES Setup Summary, 480, 640]

++++
<?pdfpagebreak?>
++++
To disable Kdump, its label can be clicked which opens the "Kdump Start-Up" page.
On that page, make sure "Disable Kdump" is selected.

image::SLES15_SP2_Setup_KDump.png[SLES Setup Kdump, 480, 640]

Finish installation and go to the next chapter.

++++
<?pdfpagebreak?>
++++
=== Configuration of the Kubernetes nodes

In this guide the Workstation will be used to orchestrate all other machines via Salt.

==== Installation and configuration of Salt-Minions

First step is to register all systems to the SUSE Customer Center or a SMT/RMT server to obtain updates during installation and afterwards.

When using a SMT/RMT server the address must be specified:
----
$ sudo SUSEConnect --url "https://<SMT/RMT-address>"
----

When registering via SUSE Customer Center, use your subscription and email address:
----
$ sudo SUSEConnect -r <SubscriptionCode> -e <EmailAddress>
----

The basesystem is required by all other modules. For installation run:
----
$ sudo SUSEConnect -p sle-module-basesystem/15.2/x86_64
----


Before the Workstation can be used for orchestration, Salt needs to be installed and configured on all Kubernetes nodes:

----
$ sudo zypper in -y salt-minion
$ sudo echo "master: <WorkstationIP>" > /etc/salt/minion
$ sudo systemctl enable salt-minion --now
----

++++
<?pdfpagebreak?>
++++
=== Configuration of the Management Workstation

The management workstation is used to deploy and maintain the Kubernetes cluster and workloads running on it.

==== Installation and configuration of Salt-Masters

It's recommended to use Salt to orchestrate all Kubernetes nodes.
This can be skipped but means every node must be configured manually afterwards.

To install Salt run:
----
$ sudo zypper in -y salt-master
$ sudo systemctl enable salt-master --now
----

Make sure all Kubernetes nodes show up when running:
----
$ salt-key -L
----

Accept and verify all minion keys:

----
$ salt-key -A -y
$ salt-key -L
----

Since RKE deployment needs ssh, a ssh key is needed.
To generate a new one run:
----
$ ssh-keygen -t rsa -b 4096
----

The generated key needs to be distributed to all other nodes:
----
$ ssh-copy-id -i <path to your sshkey> root@<nodeIP>
----

++++
<?pdfpagebreak?>
++++
==== Configuration of Kubernetes nodes

Check the status of the firewall and disable it if it isn't allready:
----
$ sudo salt '*' cmd.run 'systemctl status firewalld'
$ sudo salt '*' cmd.run 'systemctl disable firewalld --now'
----

Check the status of Kdump and disable it if it isn't allready:
----
$ sudo salt '*' cmd.run 'systemctl status kdump'
$ sudo salt '*' cmd.run 'systemctl disable kdump --now'
----

Check the NTP time synchronization and enable it if it isn't:
----
$ sudo salt '*' cmd.run 'systemctl status chronyd'
$ sudo salt '*' cmd.run 'systemctl enable chronyd --now'
$ sudo salt '*' cmd.run 'chronyc sources'
----

Make sure the SSH server is running:
----
$ sudo salt '*' cmd.run 'systemctl status sshd'
$ sudo salt '*' cmd.run 'systemctl enable sshd --now'
----

Activate needed SUSE modules:
----
$ sudo salt '*' cmd.run 'SUSEConnect -p sle-module-containers/15.2/x86_64'
----

Install packages required to run SAP Data Intelligence and enable the docker service:
----
$ sudo salt '*' cmd.run 'zypper in -y nfs-client nfs-kernel-server xfsprogs ceph-common docker'
$ sudo salt '*' cmd.run 'systemctl enable docker --now'
----

++++
<?pdfpagebreak?>
++++
=== Install RKE

In order to install Rancher RKE 1 on the cluster nodes download the RKE 1 binary to your management workstation, create the configuration for the Kubernetes cluster and deploy cluster.
The single steps are described in the following.
For reference see the documentation provided by Rancher.

* https://rancher.com/docs/rke/latest/en/installation/


==== Download RKE

To download the RKE binary go to the RKE product page and choose "download RKE":

* https://rancher.com/products/rke/

Follow the link to the latest stable release, get the amd64-binary as shown in the example below:

----
$ mkdir rke
$ cd rke
$ curl -LO https://github.com/rancher/rke/releases/download/v1.0.16/rke_linux-amd64
$ mv rke_linux-amd64 rke
$ chmod a+x rke
----

==== Create the configuration for the RKE cluster

Running the RKE configure option creates the configuration file for the Kubernetes cluster as a .yaml file in an interactive process.
Make sure to have IP addresses of the dedicated cluster nodes at hand.

----
$ cd rke
$ ./rke config --name <name of your config file>
----

==== Deploy RKE

Now deploy the Kubernetes cluster:

----
$ cd rke
$ ./rke up --config <name of your config file>
----

This will create kubeconfig for accessing the Kubernetes cluster in the current directory.
Please create a backup of the files contained in this directory (here: rke/).


==== Check the installation

Download a matching kubectl version to the management workstation:

* Example for kubectl version 1.17.17:

----
$ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.17.17/bin/linux/amd64/kubectl
$ chmod a+x kubectl
$ sudo cp -av kubectl /usr/bin/kubectl
----


Verify by running:

----
$ export KUBECONFIG=<PATH to your kubeconfig>
$ kubectl version
$ kubectl get nodes
----

++++
<?pdfpagebreak?>
++++
== Installation of SAP DI 3.1

This section describes the installation of SAP DI 3.1 on RKE 1 powered Kubernetes cluster.

=== Preparations

These are the steps to fulfill before the deployment of SAP DI 3.1 can start:

* create a namespace for SAP DI 3.1
* create access to secure private registry
* create a default storage class
* download and install SAP SLCBridge
* download the stack.xml file for provisioning the DI 3.1 install
* check if nfsd nfsv4 kernel modules are loaded and/or loadable on the Kubernetes nodes


==== Create namespace for SAP DI 3.1 in the Kubernetes cluster

Log on your management workstation and create the namespace in the Kubernetes cluster where DI 3.1 will be deployed.

----
$ kubectl create ns <NAMESPACE for DI 31>
$ kubectl get ns
----

==== Create cert file for accessing the secure private regsitry

Create a file named cert that contains the SSL certificate chain for the secure private registry.
This imports the certificates into SAP DI 3.1. 

----
$ cat CA.pem > cert
$ kubectl -n <NAMESPACE for DI 31> create secret generic cmcertificates --from-file=cert
----


=== Create default storage class

In order to install SAP DI 3.1 a default storage class is needed to provision the installation with physical volumes (PV).

Here is an example for a ceph/rbd based storage class that uses the CSI.

Create the yaml files for the storage class, get in contact with your storage admin to get the information needed:

Create config-map:

----
$ cat << EOF > csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "<ID of your ceph cluster>",
        "monitors": [
          "<IP of Monitor 1>:6789",
          "<IP of Monitor 2>:6789",
          "<IP of Monitor 3>:6789"
        ]
      }
    ]
metadata:
  name: ceph-csi-config
EOF
----

Create a secret to access the storage:

----
$ cat << EOF > csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: default
stringData:
  userID: admin
  userKey: AQCR7htglvJzBxAAtPN0YUeSiDzyTeQe0lveDQ==
EOF
----

Download

----
$ curl -LO https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
----

Download

----
$ curl -LO https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml
----

Create pool on ceph storage where the PVs will be created, insert the poolname and the Ceph cluster id:

----
$ cat << EOF > csi-rbd-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: <your ceph cluster id>
   pool: <your pool>
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
mountOptions:
   - discard
EOF
----

Create config for encryption, this is needed else the deploment of the CSI driver for ceph/rbd will fail.

----
$ cat << EOF > kms-config.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    {
      },
      "vault-tokens-test": {
          "encryptionKMSType": "vaulttokens",
          "vaultAddress": "http://vault.default.svc.cluster.local:8200",
          "vaultBackendPath": "secret/",
          "vaultTLSServerName": "vault.default.svc.cluster.local",
          "vaultCAVerify": "false",
          "tenantConfigName": "ceph-csi-kms-config",
          "tenantTokenName": "ceph-csi-kms-token",
          "tenants": {
              "my-app": {
                  "vaultAddress": "https://vault.example.com",
                  "vaultCAVerify": "true"
              },
              "an-other-app": {
                  "tenantTokenName": "storage-encryption-token"
              }
          }
       }
    }
metadata:
  name: ceph-csi-encryption-kms-config
EOF
----

Deploy the ceph/rbd CSI and storage class: 

----
$ kubectl apply -f csi-config-map.yaml
$ kubectl apply -f csi-rbd-secret.yaml
$ kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
$ kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml
$ kubectl apply -f csi-rbdplugin-provisioner.yaml 
$ kubectl apply -f csi-rbdplugin.yaml 
$ kubectl apply -f csi-rbd-sc.yaml 
$ kubectl apply -f kms-config.yaml
$ kubectl patch storageclass csi-rbd-sc -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
----

Check your storage class:

----
$ kubectl get sc
NAME                   PROVISIONER        RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
csi-rbd-sc (default)   rbd.csi.ceph.com   Delete          Immediate           false                  103m
----

=== Longhorn for Physical Volumes 

A possible valid alternative is to deploy Longhorn storage for the PVs.

==== Pre-requisites

==== Installation of Longhorn

----
$ kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.1.0/deploy/longhorn.yaml
----

==== Create a Storage Class on top of Longhorn

----
$ kubectl create -f https://raw.githubusercontent.com/longhorn/longhorn/v1.1.0/examples/storageclass.yaml
----

==== Longhorn Documentation

For more details see the Longhorn documentation:
https://longhorn.io/docs/1.1.0/


=== Download SLCBridge

The SLCBridge can be obtained via the following ways

* download from SAP software center https://support.sap.com/en/tools/software-logistics-tools.html#section_622087154 choose download SLCBridge

* see release note of SLCBridge https://launchpad.support.sap.com/#/notes/2589449

* see https://help.sap.com/viewer/a8d90a56d61a49718ebcb5f65014bbe7/3.1.latest/en-US/8ae38791d71046fab1f25ee0f682dc4c.html

* download the SLCBridge software to the management workstation.


=== Install the SLCBridge

Re-name the SLCBridge binary to slcb and make it executable. Deploy the SLCBridge to the Kubernetes cluster.

----
$ mv SLCB01_XX-70003322.EXE slcb
$ chmod 0700 slcb
$ export KUBECONFIG=<KUBE_CONFIG>
$ ./slcb init
----
During the interactive install the following information is needed:

* URL of secure private registry
* choose expert mode
* choose NodePort for the service

Take a note of the service port of the slcbridge, it is needed for the install of SAP DI 3.1 or re-configuring DI 3.1, e.g. enabling backup.

----
$ kubectl -n sap-slcbridge get svc
----

=== Create and Download Stack XML for SAP DI installation

Follow the steps in SAP DI 3.1 installation guide:
Install SAP Data Intelligence with SLC Bridge in a Cluster with Internet Access:
https://help.sap.com/viewer/a8d90a56d61a49718ebcb5f65014bbe7/3.1.latest/en-US/7e4847e241c340b3a3c50a5db11b46e2.html

==== Create a stack xml


The stack.xml can be created via the SAP Maintenance Planner, this tool can be accessed via https://support.sap.com/en/alm/solution-manager/processes-72/maintenance-planner.html
Go to the Maintenance Planner at https://apps.support.sap.com/sap/support/mp published on SAP site and generate a Stack XML file with the container image definitions of the SAP Data Intelligence release that you want to install. Download the Stack XML file to a local directory. Copy the stack.xml to the management workstation.


=== Run the Installation of SAP DI

The installation of SAP DI 3.1 is invoked by:

----
$ export KUBECONFIG=<path to kubeconfig>
$ ./slcb execute --useStackXML MP_Stack_XXXXXXXXXX_XXXXXXXX_.xml --url https://<node>:<service port>/docs/index.html
----

This starts an interactive process for configuring and deploying SAP DI 3.1.

This table lists some of the parameters possible for SAP DI 3.1 installation:

[cols="3",options="header"]
|===
| Parameter| Condition | Recommendation
| Kubernetes Namespace | Always | set to namespace created beforehand
| Installation Type | installation or update| either
| Container Registry| Always | add the uri for the secure private registry
| Checkpoint Store Configuration| installation | wether to enable Checkpoint Store
| Checkpoint Store Type |if Checkpoint Store is enabled | use S3 object store from SES
| Checkpoint Store Validation |if Checkpoint is enabled | Object store access will be verified
| Container Registry Settings for Pipeline Modeler |optional| used if a second container registry is used
| StorageClass Configuration ||
| Default StorageClass ||
| Enable Kaniko Usage |optional if running on Docker| enable
| Container Image Repository Settings for SAP Data Intelligence Modeler||
| Container Registry for Pipeline Modeler |optional|
| Loading NFS Modules |optional| Make sure that nfsd and nfsv4 kernel modules are loaded on worker nodes
| Additional Installer Parameters |optional|
|===
See SAP documenation here for details on input parameters for SAP DI 3.1 installation.
https://help.sap.com/viewer/a8d90a56d61a49718ebcb5f65014bbe7/3.1.latest/en-US/abfa9c73f7704de2907ea7ff65e7a20a.html


=== Post-Installation Tasks

After the successful finish of the installation workflow there are some tasks to be done:

* create ingress to access the SAP DI installation

----
$ cat <<EOF > ingress.yaml
EOF
$ kubectl apply -f ingress.yaml
----

* download vctl

=== Test of Data Intelligence Installation

Finally the SAP DI installation should be verified with some very basic tests:

* logon to SAP DI's launchpad

* create example pipeline

* create ML Scenario

* test machine learning

++++
<?pdfpagebreak?>
++++
== Troubleshooting

Here some are listed some errors and their respective solution.

=== error acessing registry

get sa

----
 kubectl -n $NAMESPACE get  -o jsonpath=$'{.spec.serviceAccountName}\n' pod/default-4wtmgwe-backup-hana-zbdlc
----

create secret

----
kubectl -n $NAMESPACE create secret docker-registry pull-secret --docker-server="<URI of registry>" --docker-username=<username> --docker-password=<password>
----

patch sa

----
kubectl -n $NAMESPACE patch serviceaccount <service account> -p '{"imagePullSecrets": [{"name": "pull-secret"}]}'
----

restart pod or parent

----
kubectl -n $NAMESPACE delete pod 
----

++++
<?pdfpagebreak?>
++++
== Day 2 Operation considerations

* Monitoring
** built-in monitoring in SAP DI

* security

* availability


== Maintenance Tasks

This section gives some hints what should and could be done to maintain the Kubernetes cluster, operating system and SAP DI.

=== Backup

It is good practice to keep backups of all relevant data to be able to restore the environment in case of failure.

* Regular backups

** RKE see https://rancher.com/docs/rke/latest/en/etcd-snapshots/



=== Upgrade/Update

Keep the installation up to date.

==== Updating the Operating System

* In order to be eligible and to obtain updates for SLES 15 SP2, the installations must be registered either to SUSE Customer Center or a SMT/RMT-server or SUSE Manager with a valid subscription.

* The OS can be updated using the zypper commandline tool

----
$ zypper ref -s
$ zypper lu
$ zypper patch
----

* other methods for updating SLES 15 SP2 are described in the product documentation

* if an update requires a reboot of the server, make sure that this can be done safely, i.e. shutdown SAP DI, drain and cordon the Kubernetes node before rebooting.



==== Updating RKE

* Download the version of RKE that fits your needs and uses a Kubernetes version that is compatible with SAP DI 3.1

* Create a backup of everything. 

* Shutdown the SAP DI

* Run the update with the new RKE binary with your cluster.yaml file.

* See Rancher RKE documentation https://rancher.com/docs/rke/latest/en/upgrades/

==== Updating SAP Data Intelligence

Follow SAP's update guide and notes.


