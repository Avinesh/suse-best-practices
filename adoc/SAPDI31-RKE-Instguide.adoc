:docinfo:

= SAP Data Intelligence 3.1 on Rancher Kubernetes Engine 1  

== Introduction

This guide describes the installation of SAP Data Intelligence 3.1 on premise on top of Rancher Kubernetes Engine (RKE) 1. In a nutshell the installation of SAP DI 3.1 consists of the following steps.

* Install SUSE Linux Enterprise Server 15 SP2

* Install RKE 1 Kubernetes cluster on the dedicated nodes

* Deploy SAP DI 3.1 on RKE 1 Kubernetes cluster

* Post-Installation steps for SAP DI 3.1

* Test the installation of SAP DI 3.1
 

== Pre-requisites

=== Hardware Requirements

This chapter describes the hardware requirements for installing SAP DI 3.1 on RKE 1 on top of SUSE Linux Enterprise Server 15 SP2.
Only x86_64 architecture is applicable for this use case.

====  Hardware Sizing


* Minimal hardware requirements for a SAP DI 3 deployment 
** At least 7 nodes are needed for a minimal Kubernetes cluster

[cols="40,.^10,15,.^10,25",options="header"]
|===
|Server Role |Count|RAM|CPU|Disk space
|Management Workstation|1|16 GiB|4|>100 GiB
|Master Node|3|16 GiB|4|>120 GiB
|Worker Node|4|32 GiB|8|>120 GiB
|===


* Minimal hardware requirements for production
** At least 7 nodes are needed for a production grade Kubernetes cluster

[cols="40,.^10,15,.^10,25",options="header"]
|===
|Server Role|Count|RAM|CPU|Disk space
|Management Workstation|1|16 GiB|4|>100 GiB
|Master Node|3|16 GiB|4|>120 GiB
|Worker Node|4|64 GiB|16|>120 GiB
|===

* see Rancher RKE documentation https://rancher.com/docs/rke/latest/en/os/ 

* For more sizing information see the SAP documentation that can be found here:
** https://help.sap.com/viewer/835f1e8d0dde4954ba0f451a9d4b5f10/3.1.latest/en-US

=== Software Requirements

Here we list the software components needed to install SAP DI 3.1 on RKE 1:

* SUSE Linux Enterprise Server 15 SP2

* Rancher RKE 1

* SAP Software Lifecycle Bridge

* SAP Data Intelligence 3.1

* Secure private registry for container images

* Access to a storage solution providing dynamically physical volumes

* If it is planned to use Vora's streaming tables checkpoint store, a S3 bucket like object store is needed 

* If it is planned to enable backup of SAP DI 3.1 during installation access to a S3 compatible object store is needed


== Preparations

* Get SUSE SLES subscription

* download installer for SLES 15 SP2

* download rke binary

* check storage requirements

* create or get access to a private container registry

* get an SAP S-user to access software and documentation by SAP

* read relevant SAP documentation
** Release Note for SAP DI 3 https://launchpad.support.sap.com/#/notes/2871970
** SAP SLCBridge
** Installation Guide at help.sap.com https://help.sap.com/viewer/a8d90a56d61a49718ebcb5f65014bbe7/3.1.latest/en-US



== Installation of Rancher RKE 1 Cluster

The installation of Rancher Kubernetes Engine 1 cluster is straight forward. After the installation and basic configuration of the operating system the Kubernetes cluster configuration is created on the managemant host. Finally the Kuberntes cluster iwill be deployed. The following sections will describe this in more detail.

===  Preparation of Management Host and Kubernetes Cluster Nodes

All servers in this landscape will use SUSE Enterprise Linux 15 SP2 (SLES 15 SP2) for x86_64 architecture.

* See https://documentation.suse.com/sles/15-SP2/

==== Installation of SUSE Linux Enterprise Server 15 SP2

Install on each server in your environment for SAP Data Intelligence 3.1 SLES 15 SP2 as operating system.

* Make sure to not configure swap for the nodes dedicated for Kubernetes. 
* It is highly recommended to use a static network setup for all nodes in this environment.
* Use the following setup for installing SLES:
** SLES 15 SP2
** add basesystem and application server module at least 
** add container module 


==== Configuration of SUSE Linux Enterprise 15 SP2 (SLES 15 SP2)

* static IP configuration is highly recommended
* deploy a text based install
* configure ntp client
* disable firewall
* enable ssh access
* disable kdump
* register installation to SUSE Customer Center or a SMT/RMT server to obtain updates during installation and afterwards
* install nfs-client, nfs-kernelserver, xfsprogs, ceph-common on any node.


=== Configure Management Workstation

The management workstation is used to deploy and administer the Kubernetes cluster and the workloads running on it.

* download and install kubectl in a version matching the Kubernetes version of the cluster (see below).
* access to the cluster nodes via ssh



=== Install RKE

In order to install Rancher RKE 1 on the cluster nodes download the RKE 1 binary to your management workstation, create the configuration for the Kubernetes cluster and deploy cluster.
The single steps are described in the following.
For reference see the documentation provided by Rancher.

* https://rancher.com/docs/rke/latest/en/installation/


==== Download RKE

To download the RKE binary go to the RKE product page and choose "download RKE":

* https://rancher.com/products/rke/

Follow the link to the latest stable release, get the amd64-binary as shown in the example below:

----
$ mkdir rke
$ cd rke
$ curl -LO https://github.com/rancher/rke/releases/download/v1.0.16/rke_linux-amd64
$ mv rke_linux-amd64 rke
$ chmod a+x rke
----

==== Create the  configuration for the RKE cluster

Running the RKE configure option creates the configuration file for the Kubernetes cluster as a yaml-file in an interactive process.
Make sure to have IP addresses of the dedicated cluster nodes at hand.

----
$ cd rke
$ ./rke configure --name <name of your config file>
----

==== Deploy RKE

Now deploy the Kubernetes cluster:

----
$ cd rke
$ ./rke up --config <name of your config file>
----

This will create kubeconfig for accessing the Kubernetes cluster in the current directory.
Please create a backup of the files contained in this directory (here: rke/).


==== Check the installation

Download a matching kubectl version to the management workstation:

* Example for kubectl version 1.17.17:

----
$ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.17.17/bin/linux/amd64/kubectl
$ chmod a+x kubectl
# cp -av kubectl /usr/bin/kubectl
----


Verify by running:

----
$ export KUBECONFIG=<PATH to your kubeconfig>
$ kubectl version
$ kubectl get nodes
----


== Installation of SAP DI 3.1

This section describes the installation of SAP DI 3.1 on RKE 1 powered Kubernetes cluster.

=== Preparations

These are the steps to fulfill before the deployment of SAP DI 3.1 can start:

* create a namespace for SAP DI 3.1
* create access to secure private registry
* create a default storage class
* download and install SAP SLCBridge
* download the stack.xml file for provisioning the DI 3.1 install
* check if nfsd nfsv4 kernel modules are loaded and/or loadable on the Kubernetes nodes


==== Create namespace for SAP DI 3.1 in the Kubernetes cluster

Log on your management workstation and create the namespace in the Kubernetes cluster where DI 3.1 will be deployed.

----
$ kubectl create ns <NAMESPACE for DI 31>
$ kubectl get ns
----

==== Create cert file for accessing the secure private regsitry

Create a file named cert that contains the SSL certificate chain for the secure private registry.
This imports the certificates into SAP DI 3.1. 

----
$ cat CA.pem > cert
$ kubectl -n <NAMESPACE for DI 31> create secret generic cmcertificates --from-file=cert
----


=== Create default storage class

In order to install SAP DI 3.1 a default storage class is needed to provision the installation with physical volumes (PV).

Here is an example for a ceph/rbd based storage class that uses the CSI.

Create the yaml files for the storage class, get in contact with your storage admin to get the information needed:

Create config-map:

----
$ cat << EOF > csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "<ID of your ceph cluster>",
        "monitors": [
          "<IP of Monitor 1>:6789",
          "<IP of Monitor 2>:6789",
          "<IP of Monitor 3>:6789"
        ]
      }
    ]
metadata:
  name: ceph-csi-config
EOF
----

Create a secret to access the storage:

----
$ cat << EOF > csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: default
stringData:
  userID: admin
  userKey: AQCR7htglvJzBxAAtPN0YUeSiDzyTeQe0lveDQ==
EOF
----

Download

----
$ curl -LO https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
----

Download

----
$ curl -LO https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml
----

Create pool on ceph storage where the PVs will be created, insert the poolname and the Ceph cluster id:

----
$ cat << EOF > csi-rbd-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: <your ceph cluster id>
   pool: <your pool>
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
mountOptions:
   - discard
EOF
----

Create config for encryption, this is needed else the deploment of the CSI driver for ceph/rbd will fail.

----
$ cat << EOF > kms-config.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    {
      },
      "vault-tokens-test": {
          "encryptionKMSType": "vaulttokens",
          "vaultAddress": "http://vault.default.svc.cluster.local:8200",
          "vaultBackendPath": "secret/",
          "vaultTLSServerName": "vault.default.svc.cluster.local",
          "vaultCAVerify": "false",
          "tenantConfigName": "ceph-csi-kms-config",
          "tenantTokenName": "ceph-csi-kms-token",
          "tenants": {
              "my-app": {
                  "vaultAddress": "https://vault.example.com",
                  "vaultCAVerify": "true"
              },
              "an-other-app": {
                  "tenantTokenName": "storage-encryption-token"
              }
          }
       }
    }
metadata:
  name: ceph-csi-encryption-kms-config
EOF
----

Deploy the ceph/rbd CSI and storage class: 

----
$ kubectl apply -f csi-config-map.yaml
$ kubectl apply -f csi-rbd-secret.yaml
$ kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
$ kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml
$ kubectl apply -f csi-rbdplugin-provisioner.yaml 
$ kubectl apply -f csi-rbdplugin.yaml 
$ kubectl apply -f csi-rbd-sc.yaml 
$ kubectl apply -f kms-config.yaml
$ kubectl patch storageclass csi-rbd-sc -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
----

Check your storage class:

----
$ kubectl get sc
NAME                   PROVISIONER        RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
csi-rbd-sc (default)   rbd.csi.ceph.com   Delete          Immediate           false                  103m
----

=== Longhorn for Physical Volumes 

A possible valid alternative is to deploy Longhorn storage for the PVs.

==== Pre-requisites

==== Installation of Longhorn

----
$ kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.1.0/deploy/longhorn.yaml
----

==== Create a Storage Class on top of Longhorn

----
$ kubectl create -f https://raw.githubusercontent.com/longhorn/longhorn/v1.1.0/examples/storageclass.yaml
----

==== Longhorn Documentation

For more details see the Longhorn documentation:
https://longhorn.io/docs/1.1.0/


=== Download SLCBridge

The SLCBridge can be obtained via the following ways

* download from SAP software center https://support.sap.com/en/tools/software-logistics-tools.html#section_622087154 choose download SLCBridge

* see release note of SLCBridge https://launchpad.support.sap.com/#/notes/2589449

* see https://help.sap.com/viewer/a8d90a56d61a49718ebcb5f65014bbe7/3.1.latest/en-US/8ae38791d71046fab1f25ee0f682dc4c.html

* download the SLCBridge software to the management workstation.


=== Install the SLCBridge

Re-name the SLCBridge binary to slcb and make it executable. Deploy the SLCBridge to the Kubernetes cluster.

----
$ mv SLCB01_XX-70003322.EXE slcb
$ chmod 0700 slcb
$ export KUBECONFIG=<KUBE_CONFIG>
$ ./slcb init
----
During the interactive install the following information is needed:

* URL of secure private registry
* choose expert mode
* choose NodePort for the service

Take a note of the service port of the slcbridge, it is needed for the install of SAP DI 3.1 or re-configuring DI 3.1, e.g. enabling backup.

----
$ kubectl -n sap-slcbridge get svc
----

=== Create and Download Stack XML for SAP DI installation

Follow the steps in SAP DI 3.1 installation guide:
Install SAP Data Intelligence with SLC Bridge in a Cluster with Internet Access:
https://help.sap.com/viewer/a8d90a56d61a49718ebcb5f65014bbe7/3.1.latest/en-US/7e4847e241c340b3a3c50a5db11b46e2.html

==== Create a stack xml


The stack.xml can be created via the SAP Maintenance Planner, this tool can be accessed via https://support.sap.com/en/alm/solution-manager/processes-72/maintenance-planner.html
Go to the Maintenance Planner at https://apps.support.sap.com/sap/support/mp published on SAP site and generate a Stack XML file with the container image definitions of the SAP Data Intelligence release that you want to install. Download the Stack XML file to a local directory. Copy the stack.xml to the management workstation.


=== Run the Installation of SAP DI

The installation of SAP DI 3.1 is invoked by:

----
$ export KUBECONFIG=<path to kubeconfig>
$ ./slcb execute --useStackXML MP_Stack_XXXXXXXXXX_XXXXXXXX_.xml --url https://<node>:<service port>/docs/index.html
----

This starts an interactive process for configuring and deploying SAP DI 3.1.

This table lists some of the parameters possible for SAP DI 3.1 installation:

[cols="3",options="header"]
|===
| Parameter| Condition | Recommendation
| Kubernetes Namespace | Always | set to namespace created beforehand
| Installation Type | installation or update| either
| Container Registry| Always | add the uri for the secure private registry
| Checkpoint Store Configuration| installation | wether to enable Checkpoint Store
| Checkpoint Store Type |if Checkpoint Store is enabled | use S3 object store from SES
| Checkpoint Store Validation |if Checkpoint is enabled | Object store access will be verified
| Container Registry Settings for Pipeline Modeler |optional| used if a second container registry is used
| StorageClass Configuration ||
| Default StorageClass ||
| Enable Kaniko Usage |optional if running on Docker| enable
| Container Image Repository Settings for SAP Data Intelligence Modeler||
| Container Registry for Pipeline Modeler |optional|
| Loading NFS Modules |optional| Make sure that nfsd and nfsv4 kernel modules are loaded on worker nodes
| Additional Installer Parameters |optional|
|===
See SAP documenation here for details on input parameters for SAP DI 3.1 installation.
https://help.sap.com/viewer/a8d90a56d61a49718ebcb5f65014bbe7/3.1.latest/en-US/abfa9c73f7704de2907ea7ff65e7a20a.html


=== Post-Installation Tasks

After the successful finish of the installation workflow there are some tasks to be done:

* create ingress to access the SAP DI installation

----
$ cat <<EOF > ingress.yaml
EOF
$ kubectl apply -f ingress.yaml
----

* download vctl

=== Test of Data Intelligence Installation

Finally the SAP DI installation should be verified with some very basic tests:

* logon to SAP DI's launchpad

* create example pipeline

* create ML Scenario

* test machine learning


== Troubleshooting

Here some are listed some errors and their respective solution.

=== error acessing registry

get sa

----
 kubectl -n $NAMESPACE get  -o jsonpath=$'{.spec.serviceAccountName}\n' pod/default-4wtmgwe-backup-hana-zbdlc
----

create secret

----
kubectl -n $NAMESPACE create secret docker-registry pull-secret --docker-server="<URI of registry>" --docker-username=<username> --docker-password=<password>
----

patch sa

----
kubectl -n $NAMESPACE patch serviceaccount <service account> -p '{"imagePullSecrets": [{"name": "pull-secret"}]}'
----

restart pod or parent

----
kubectl -n $NAMESPACE delete pod 
----

== Day 2 Operation considerations

* Monitoring
** built-in monitoring in SAP DI

* security

* availability


== Maintenance Tasks

This section gives some hints what should and could be done to maintain the Kubernetes cluster, operating system and SAP DI.

=== Backup

It is good practice to keep backups of all relevant data to be able to restore the environment in case of failure.

* Regular backups

** RKE see https://rancher.com/docs/rke/latest/en/etcd-snapshots/



=== Upgrade/Update

Keep the installation up to date.

==== Updating the Operating System

* In order to be eligible and to obtain updates for SLES 15 SP2, the installations must be registered either to SUSE Customer Center or a SMT/RMT-server or SUSE Manager with a valid subscription.

* The OS can be updated using the zypper commandline tool

----
$ zypper ref -s
$ zypper lu
$ zypper patch
----

* other methods for updating SLES 15 SP2 are described in the product documentation

* if an update requires a reboot of the server, make sure that this can be done safely, i.e. shutdown SAP DI, drain and cordon the Kubernetes node before rebooting.



==== Updating RKE

* Download the version of RKE that fits your needs and uses a Kubernetes version that is compatible with SAP DI 3.1

* Create a backup of everything. 

* Shutdown the SAP DI

* Run the update with the new RKE binary with your cluster.yaml file.

* See Rancher RKE documentation https://rancher.com/docs/rke/latest/en/upgrades/

==== Updating SAP Data Intelligence

Follow SAP's update guide and notes.


