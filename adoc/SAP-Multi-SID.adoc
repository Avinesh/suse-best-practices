:cluhost1: freki
:cluhost2: geri
:cluhost3: sleipnir

:sid1: YAS
:sid1-lc: yas
:inst-ascs1: 00
:inst-ers1: 10
:vip-asc1: 172.17.1.07
:vip-ers1: 172.17.1.08
:v-ascs1host: sapascs1
:v-ers1host: sapers1
:ascs1nfsexp: 172.17.0.1:/srv/install/sapascs1
:ers1nfsexp: 172.17.0.1:/srv/install/sapers1

:sid2: VAS
:sid2-lc: vas
:inst-ascs2: 20
:inst-ers2: 30
:vip-asc2: 172.17.1.17
:vip-ers2: 172.17.1.18
:v-ascs2host: sapascs2
:v-ers2host: sapers2
:ascs2nfsexp: 172.17.0.1:/srv/install/sapascs2
:ers2nfsexp: 172.17.0.1:/srv/install/sapers2

:sid3: WAS
:sid3-lc: was
:inst-ascs3: 31
:inst-ers3: 41
:vip-asc3: 172.17.1.27
:vip-ers3: 172.17.1.28
:v-ascs3host: sapascs3
:v-ers3host: sapers3
:ascs3nfsexp: 172.17.0.1:/srv/install/sapascs3
:ers3nfsexp: 172.17.0.1:/srv/install/sapers3

:sid4: XAS
:sid4-lc: xas
:inst-ascs4: 42
:inst-ers4: 52
:vip-asc4: 172.17.1.37
:vip-ers4: 172.17.1.38
:v-ascs4host: sapascs4
:v-ers4host: sapers4
:ascs4nfsexp: 172.17.0.1:/srv/install/sapascs4
:ers4nfsexp: 172.17.0.1:/srv/install/sapers4

:sid5: ZAS
:sid5-lc: zas
:inst-ascs5: 53
:inst-ers5: 63
:vip-asc5: 172.17.1.39
:vip-ers5: 172.17.1.40
:v-ascs5host: sapascs5
:v-ers5host: sapers5
:ascs5nfsexp: 172.17.0.1:/srv/install/sapascs5
:ers5nfsexp: 172.17.0.1:/srv/install/sapers5


:sap: SAP
:sapReg: SAP*
:sapBS: {SAP} Business Suite
:sapBSReg: {SAPReg} Business Suite
:sapNW: {SAP} NetWeaver
:sapS4: {sap} S/4HANA
:sapS41809: {sap} S/4HANA 1809
:sapS4in: {sap} S/4HANA Server 1809
:sapS4pl: {sap} S/4HANA ABAP Platform
:sapCert: {SAP} S/4-HA-CLU 1.0
:sapERS: {sap} Enqueue Replication Server 2
:sapHana: {sap} HANA
:s4Hana: {sap} S/4HANA
:sapStartSrv: sapstartsrv
:sapCtrl: sapcontrol
:sapHostAgent: saphostagent

:linux: Linux


:suse: SUSE
:SUSEReg: SUSE(R)
:sleAbbr: SLE
:sle: SUSE Linux Enterprise
:sleReg: {SUSEReg} Linux Enterprise
:slesAbbr: SLES
:sles: {sle} Server
:slesReg: {sleReg} Server
:sles4sapAbbr: {slesAbbr} for {SAP}
:sles4sap: {sles} for {SAP} Applications
:sles4sapReg: {slesReg} for {SAP} Applications
:sleHA: {sle} High Availability
:s4sClConnector: sap_suse_cluster_connector
:s4sClConnector3: sap-suse-cluster-connector
:sapHanaSR: {sap}HanaSR


:docinfo:

:localdate:


= SAP S/4HANA and NetWeaver Multi - SID Cluster Guide

== About This Guide

=== Introduction

{sles4sapReg} is the optimal platform to
run {sapReg} applications with high availability. Together with a redundant layout
of the technical infrastructure, single points of failure can be eliminated.

{sapBSReg} is a sophisticated application platform for large enterprises
and mid-size companies. Many critical business environments require the highest
possible {sapReg} application availability.

The described cluster solution can be used for {sapReg} {sapS4pl}.

{sapS4pl} is a common stack of middleware functionality used to support SAP
business applications. The {sapERS} constitutes application
level redundancy for one of the most crucial components of the {sapS4pl} stack,
the enqueue service. An optimal effect of the enqueue replication mechanism can
be achieved when combining the application level redundancy with a high
availability cluster solution, as provided for example by {sles4sap}. Over several
years of productive operations, the components mentioned have proven their maturity
for customers of different sizes and industries.


[id="sec.resource"]

=== Additional Documentation and Resources

Several chapters in this document contain links to additional documentation resources that
are either available on the system or on the Internet.

For the latest product documentation updates, see https://documentation.suse.com/ .

More whitepapers, guides and best practices documents referring to SUSE Linux Enterprise Server and SAP can be
found and downloaded at the SUSE Best Practices Web page:

https://documentation.suse.com/sbp/all

Here you can access guides for {SAPHANA} system replication
automation and High Availability (HA) scenarios for {SAPNw} and {s4hana}.

Additional resources, such as customer references, brochures or flyer, can be found at the SUSE Linux
Enterprise Server for SAP Applications resource library:

https://www.suse.com/products/sles-for-sap/#resources .

=== Feedback
include::common_intro_feedback.adoc[]

== Scope of This Document

The document at hand explains how to:


////
TODO:
////


=== General Rules and Requirements

WARNING: Higher complexity with more resources. Higher impact if one nodes goes down.
Higher administrative effort in case of maintenance activities, like OS patching.

.Two node setup
- The hardware configuration of each node must be able to run all resources in case of a fail over
- Each SAP system (SID) must be independent from each other

.Multi node setup
- The hardware configuration of each node must be able to run all resources in case of a fail over
- Each SAP system (SID) must be independent from each other
- ENSA1 prohibited host (TODO)
- ENSA2 loadbalancing (TODO)

.Installation
The installation process is described in our *Best Practice Guides* for {sapS4} and {sapNW}.  They can be found here:
https://documentation.suse.com/sbp/all

Repeat the step for preparing the host and infrastructure.

* Provide the storage and IP addresses as needed.
* Check the name resolution and time settings on each the host.
* Provide the SAP installation sources as you will need.
* Create the directory structure similar to the first SAP instance for all new installations.
* Manually mount the file systems and assign the IP address for the virtual hostname
* Start SAP installation with virtual hostname using swpm

The installation process himself is similar to the first setup. The major different is during the ERS
installation during the profile selection. At this point you have to choose the right folder / directory to
point to the correct ASCS installation.

.Limitations
We recommend  to limit the numbers of SAP systems (SID) running in one cluster. Each new SID will increase the complexity.
The tested setup was done with *five* SID in one cluster.

== Example ENSA1 in a Two Node Cluster
//[[2nensa1]]
As known from the setup based on our *Best Practice* for {sapNW}
(https://documentation.suse.com/sbp/all -> SAP NetWeaver High Availability Cluster 7.40 - Setup Guide)
the cluster integration can be done with a configuration file. Create one configuration file for each SID
and load them once by once into the cluster. The following steps will help to do the steps in the right direction.

The cluster is already up and running and still run one ASCS / ERS configuration based on the group concept.
This task must be done:

* stop the new installed SAP system, which will be added into the cluster
* umount the filesystem and remove the IP address from the new SAP system
* check / modify the ASCS and ERS profile files and add the SUSE HAE functions, if not done so far
* modify the haclient group on each cluster node and add the new <sid>adm to it, if not done so far
* set the cluster in maintenance mode
* load the new configuration with _crm configure load update <filename>_
* release the cluster maintenance mode
* check the cluster and SAP instances with _crm_mon_ and _sapcontrol -nr ..._

.Configuration file and adapting cluster
=========
_As user *root*_ prepare a file for the SAP system {sid2}
//[subs="specialchars,attributes,verbatim,quotes"]
//======
[subs="attributes"]
----
# cat ensa1-2nd-{sid2-lc}.txt

primitive rsc_fs_{sid2}_ASCS{inst-ascs2} Filesystem \
	params device="{ascs2nfsexp}" directory="/usr/sap/{sid2}/ASCS{inst-ascs2}" fstype=nfs options="vers=3" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=300s
primitive rsc_fs_{sid2}_ERS{inst-ers2} Filesystem \
	params device="{ers2nfsexp}" directory="/usr/sap/{sid2}/ERS{inst-ers2}" fstype=nfs options="vers=3" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=300s
primitive rsc_ip_{sid2}_ASCS{inst-ascs2} IPaddr2 \
	params ip={vip-ascs2} \
	op monitor interval=10s timeout=20s
primitive rsc_ip_{sid2}_ERS{inst-ers2} IPaddr2 \
	params ip={vip-ers2} \
	op monitor interval=10s timeout=20s
primitive rsc_sap_{sid2}_ASCS{inst-ascs2} SAPInstance \
	operations $id=rsc_sap_{sid2}_ASCS{inst-ascs2}-operations \
	op monitor interval=11 timeout=60 \
	op_params on_fail=restart \
	params InstanceName={sid2}_ASCS{inst-ascs2}_{v-ascs2host} START_PROFILE="/sapmnt/{sid2}/profile/{sid2}_ASCS{inst-ascs2}_{v-ascs2host}" AUTOMATIC_RECOVER=false \
	meta resource-stickiness=5000 failure-timeout=60 \
        migration-threshold=1 priority=10
primitive rsc_sap_{sid2}_ERS{inst-ers2} SAPInstance \
	operations $id=rsc_sap_{sid2}_ERS{inst-ers2}-operations \
	op monitor interval=11 timeout=60 \
	op_params on_fail=restart \
	params InstanceName={sid2}_ERS{inst-ers2}_{v-ers2host} START_PROFILE="/sapmnt/{sid2}/profile/{sid2}_ERS{inst-ers2}_{v-ers2host}" AUTOMATIC_RECOVER=false IS_ERS=true \
	meta priority=1000
group grp_{sid2}_ASCS{inst-ascs2} rsc_ip_{sid2}_ASCS{inst-ascs2} rsc_fs_{sid2}_ASCS{inst-ascs2} rsc_sap_{sid2}_ASCS{inst-ascs2} \
	meta resource-stickiness=3000
group grp_{sid2}_ERS{inst-ers2} rsc_ip_{sid2}_ERS{inst-ers2} rsc_fs_{sid2}_ERS{inst-ers2} rsc_sap_{sid2}_ERS{inst-ers2}
colocation col_sap_{sid2}_no_both -5000: grp_{sid2}_ERS{inst-ers2} grp_{sid2}_ASCS{inst-ascs2}
location loc_sap_{sid2}_failover_to_ers rsc_sap_{sid2}_ASCS{inst-ascs2} \
         rule 2000: runs_ers_{sid2} eq 1
order ord_sap_{sid2}_first_start_ascs Optional: rsc_sap_{sid2}_ASCS{inst-ascs2}:start rsc_sap_{sid2}_ERS{inst-ers2}:stop symmetrical=false
----
//======

.Configuration Example for SID {sid3}

_As user *root*_ prepare a file for the SAP system {sid3}
//[subs="specialchars,attributes,verbatim,quotes"]
//======
[subs="attributes"]
----
# cat ensa1-3nd-{sid3-lc}.txt

primitive rsc_fs_{sid3}_ASCS{inst-ascs3} Filesystem \
	params device="{ascs3nfsexp}" directory="/usr/sap/{sid3}/ASCS{inst-ascs3}" fstype=nfs options="vers=3" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=300s
primitive rsc_fs_{sid3}_ERS{inst-ers3} Filesystem \
	params device="{ers3nfsexp}" directory="/usr/sap/{sid3}/ERS{inst-ers3}" fstype=nfs options="vers=3" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=300s
primitive rsc_ip_{sid3}_ASCS{inst-ascs3} IPaddr2 \
	params ip={vip-ascs3} \
	op monitor interval=10s timeout=20s
primitive rsc_ip_{sid3}_ERS{inst-ers3} IPaddr2 \
	params ip={vip-ers3} \
	op monitor interval=10s timeout=20s
primitive rsc_sap_{sid3}_ASCS{inst-ascs3} SAPInstance \
	operations $id=rsc_sap_{sid3}_ASCS{inst-ascs3}-operations \
	op monitor interval=11 timeout=60 \
	op_params on_fail=restart \
	params InstanceName={sid3}_ASCS{inst-ascs3}_{v-ascs3host} START_PROFILE="/sapmnt/{sid3}/profile/{sid3}_ASCS{inst-ascs3}_{v-ascs3host}" AUTOMATIC_RECOVER=false \
	meta resource-stickiness=5000 failure-timeout=60 \
        migration-threshold=1 priority=10
primitive rsc_sap_{sid3}_ERS{inst-ers3} SAPInstance \
	operations $id=rsc_sap_{sid3}_ERS{inst-ers3}-operations \
	op monitor interval=11 timeout=60 \
	op_params on_fail=restart \
	params InstanceName={sid3}_ERS{inst-ers3}_{v-ers3host} START_PROFILE="/sapmnt/{sid3}/profile/{sid3}_ERS{inst-ers3}_{v-ers3host}" AUTOMATIC_RECOVER=false IS_ERS=true \
	meta priority=1000
group grp_{sid3}_ASCS{inst-ascs3} rsc_ip_{sid3}_ASCS{inst-ascs3} rsc_fs_{sid3}_ASCS{inst-ascs3} rsc_sap_{sid3}_ASCS{inst-ascs3} \
	meta resource-stickiness=3000
group grp_{sid3}_ERS{inst-ers3} rsc_ip_{sid3}_ERS{inst-ers3} rsc_fs_{sid3}_ERS{inst-ers3} rsc_sap_{sid3}_ERS{inst-ers3}
colocation col_sap_{sid3}_no_both -5000: grp_{sid3}_ERS{inst-ers3} grp_{sid3}_ASCS{inst-ascs3}
location loc_sap_{sid3}_failover_to_ers rsc_sap_{sid3}_ASCS{inst-ascs3} \
         rule 2000: runs_ers_{sid3} eq 1
order ord_sap_{sid3}_first_start_ascs Optional: rsc_sap_{sid3}_ASCS{inst-ascs3}:start rsc_sap_{sid3}_ERS{inst-ers3}:stop symmetrical=false
----
//======

.Configuration Example for SID {sid4} and following

This is similar to the SID examples before. You must adapt:

* filesystem sources which will be mounted
* IP address for virtual hostname of ASCS and ERS
* Instance number for ASCS and ERS
* local mount point for ASCS and ERS
* profile path for ASCS and ERS


.Add the 2nd and further SID into the cluster

The cluster is already up and running and still run one ASCS / ERS configuration based on the group concept.
This task must be done:

* stop the new installed SAP system, which will be added into the cluster
* umount the filesystem and remove the IP address from the new SAP system
* check / modify the ASCS and ERS profile files and add the SUSE HAE functions, if not done so far
* modify the haclient group on each cluster node and add the new <sid>adm to it, if not done so far
* set the cluster in maintenance mode
* load the new configuration with _crm configure load update <filename>_
* pre check the cluster with _crm status_
* release the cluster maintenance mode
* check the cluster and SAP instances with _crm_mon_ and _sapcontrol -nr ..._

Login to one of the cluster nodes and run the commands from there.

_As user *root*_ set cluster maintenance and load the configuration into the cluster
with _crm configure load update <filename>_
//[subs="specialchars,attributes,verbatim,quotes"]
//======
[subs="attributes"]
----
# crm configure property maintenance-mode=true
# crm configure load update ensa1-2nd-{sid2-lc}.txt
# crm configure load update ensa1-3nd-{sid3-lc}.txt
# crm configure load update ensa1-4nd-{sid4-lc}.txt
...
# crm status
----
//======
You should *not* get a message back only the prompt. If there are messages there is maybe something wrong in the configuration file.

NOTE: If there is a wrong path e.g. for the profile this will not be detected during the configuration load. But during the cluster start action it will be shown.

_As user *root*_ verify the new inactive loaded configuration
//[subs="specialchars,attributes,verbatim,quotes"]
[subs="attributes"]
//======
----
# crm status
{cluhost1}:~ # crm status
Stack: corosync
Current DC: {cluhost1} (version 1.1.18+20180430.b12c320f5-3.3.1-b12c320f5) - partition with quorum
Last updated: Tue Oct 15 16:57:07 2019
Last change: Tue Oct 15 16:56:56 2019 by hacluster via crmd on {cluhost1}

2 nodes configured
31 resources configured

*** Resource management is DISABLED ***
The cluster will not attempt to start, stop or recover services

Online: [ {cluhost1} {cluhost2} ]

Full list of resources:

Resource Group: grp_{sid1}_ASCS{inst-ascs1}
	rsc_ip_{sid1}_ASCS{inst-ascs1}	(ocf::heartbeat:IPaddr2):	Started {cluhost1}
	rsc_fs_{sid1}_ASCS{inst-ascs1}	(ocf::heartbeat:Filesystem):	Started {cluhost1}
	rsc_sap_{sid1}_ASCS{inst-ascs1}	(ocf::heartbeat:SAPInstance):	Started {cluhost1}
Resource Group: grp_{sid1}_ERS{inst-ers1}
	rsc_ip_{sid1}_ERS{inst-ers1}	(ocf::heartbeat:IPaddr2):	Started {cluhost2}
	rsc_fs_{sid1}_ERS{inst-ers1}	(ocf::heartbeat:Filesystem):	Started {cluhost2}
	rsc_sap_{sid1}_ERS{inst-ers1}	(ocf::heartbeat:SAPInstance):	Started {cluhost2}
	rsc_ip_hawk	(ocf::heartbeat:IPaddr2):	Started {cluhost1}
Resource Group: grp_{sid4}_ASCS{inst-ascs4}
	rsc_ip_{sid4}_ASCS{inst-ascs4}	(ocf::heartbeat:IPaddr2):	Stopped (unmanaged)
	rsc_fs_{sid4}_ASCS{inst-ascs4}	(ocf::heartbeat:Filesystem):	Stopped (unmanaged)
	rsc_sap_{sid4}_ASCS{inst-ascs4}	(ocf::heartbeat:SAPInstance):	Stopped (unmanaged)
Resource Group: grp_{sid4}_ERS{inst-ers4}
	rsc_ip_{sid4}_ERS{inst-ers4}	(ocf::heartbeat:IPaddr2):	Stopped (unmanaged)
	rsc_fs_{sid4}_ERS{inst-ers4}	(ocf::heartbeat:Filesystem):	Stopped (unmanaged)
	rsc_sap_{sid4}_ERS{inst-ers4}	(ocf::heartbeat:SAPInstance):	Stopped (unmanaged)
Resource Group: grp_{sid3}_ASCS{inst-ascs3}
	rsc_ip_{sid3}_ASCS{inst-ascs3}	(ocf::heartbeat:IPaddr2):	Stopped (unmanaged)
	rsc_fs_{sid3}_ASCS{inst-ascs3}	(ocf::heartbeat:Filesystem):	Stopped (unmanaged)
	rsc_sap_{sid3}_ASCS{inst-ascs3}	(ocf::heartbeat:SAPInstance):	Stopped (unmanaged)
Resource Group: grp_{sid3}_ERS{inst-ers3}
	rsc_ip_{sid3}_ERS{inst-ers3}	(ocf::heartbeat:IPaddr2):	Stopped (unmanaged)
	rsc_fs_{sid3}_ERS{inst-ers3}	(ocf::heartbeat:Filesystem):	Stopped (unmanaged)
	rsc_sap_{sid3}_ERS{inst-ers3}	(ocf::heartbeat:SAPInstance):	Stopped (unmanaged)
Resource Group: grp_{sid5}_ASCS{inst-ascs5}
	rsc_ip_{sid5}_ASCS{inst-ascs5}	(ocf::heartbeat:IPaddr2):	Stopped (unmanaged)
	rsc_fs_{sid5}_ASCS{inst-ascs5}	(ocf::heartbeat:Filesystem):	Stopped (unmanaged)
	rsc_sap_{sid5}_ASCS{inst-ascs5}	(ocf::heartbeat:SAPInstance):	Stopped (unmanaged)
Resource Group: grp_{sid5}_ERS{inst-ers5}
	rsc_ip_{sid5}_ERS{inst-ers5}	(ocf::heartbeat:IPaddr2):	Stopped (unmanaged)
	rsc_fs_{sid5}_ERS{inst-ers5}	(ocf::heartbeat:Filesystem):	Stopped (unmanaged)
	rsc_sap_{sid5}_ERS{inst-ers5}	(ocf::heartbeat:SAPInstance):	Stopped (unmanaged)
Resource Group: grp_{sid2}_ASCS{inst-ascs2}
	rsc_ip_{sid2}_ASCS{inst-ascs2}	(ocf::heartbeat:IPaddr2):	Stopped (unmanaged)
	rsc_fs_{sid2}_ASCS{inst-ascs2}	(ocf::heartbeat:Filesystem):	Stopped (unmanaged)
	rsc_sap_{sid2}_ASCS{inst-ascs2}	(ocf::heartbeat:SAPInstance):	Stopped (unmanaged)
Resource Group: grp_{sid2}_ERS{inst-ers2}
	rsc_ip_{sid2}_ERS{inst-ers2}	(ocf::heartbeat:IPaddr2):	Stopped (unmanaged)
	rsc_fs_{sid2}_ERS{inst-ers2}	(ocf::heartbeat:Filesystem):	Stopped (unmanaged)
	rsc_sap_{sid2}_ERS{inst-ers2}	(ocf::heartbeat:SAPInstance):	Stopped (unmanaged)
----
//======

_As user *root*_ release cluster maintenance and check that the new cluster resources became active
//[subs="specialchars,attributes,verbatim,quotes"]
//======
[subs="attributes"]
----
# crm configure property maintenance-mode=false
# crm_mon -rfn
----
//======

_As user {sid2-lc}adm_ e.g. on ASCS host check the SAP system
//[subs="specialchars,attributes,verbatim,quotes"]
//======
[subs="attributes"]
----
# su - {sid2-lc}adm
# sapcontrol -nr {inst-ascs2} -function GetSystemInstanceList
# sapcontrol -nr {inst-ascs2} -function GetProcessList -host {v-ascs2host}
# sapcontrol -nr {inst-ers2} -function GetProcessList -host {v-ers2host}
----
//======
NOTE: Repeat the steps for each SID you have installed.

.Example output of five running SID in a two node cluster

[subs="attributes"]
----
# crm_mon -1rfn

Stack: corosync
Current DC: {cluhost1} (version 1.1.18+20180430.b12c320f5-3.3.1-b12c320f5) - partition with quorum
Last updated: Tue Oct 15 16:26:57 2019
Last change: Tue Oct 15 16:25:55 2019 by root via cibadmin on {cluhost1}

2 nodes configured
31 resources configured

Node {cluhost1}: online
        rsc_ip_{sid3}_ASCS{inst-ascs3}       (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid3}_ASCS{inst-ascs3}       (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid3}_ASCS{inst-ascs3}      (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_{sid2}_ERS{inst-ers2}        (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid2}_ERS{inst-ers2}        (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid2}_ERS{inst-ers2}       (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_{sid1}_ERS{inst-ers1}        (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid1}_ERS{inst-ers1}        (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid1}_ERS{inst-ers1}       (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_{sid5}_ERS{inst-ers5}        (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid5}_ERS{inst-ers5}        (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid5}_ERS{inst-ers5}       (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_{sid4}_ERS{inst-ers4}        (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid4}_ERS{inst-ers4}        (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid4}_ERS{inst-ers4}       (ocf::heartbeat:SAPInstance):   Started
Node {cluhost2}: online
        rsc_ip_{sid3}_ERS{inst-ers3}        (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid3}_ERS{inst-ers3}        (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid3}_ERS{inst-ers3}       (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_{sid1}_ASCS{inst-ascs1}       (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid1}_ASCS{inst-ascs1}       (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid1}_ASCS{inst-ascs1}      (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_{sid4}_ASCS{inst-ascs4}       (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid4}_ASCS{inst-ascs4}       (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid4}_ASCS{inst-ascs4}      (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_{sid2}_ASCS{inst-ascs2}       (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid2}_ASCS{inst-ascs2}       (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid2}_ASCS{inst-ascs2}      (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_{sid5}_ASCS{inst-ascs5}       (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid5}_ASCS{inst-ascs5}       (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid5}_ASCS{inst-ascs5}      (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_hawk     (ocf::heartbeat:IPaddr2):       Started

No inactive resources


Migration Summary:
* Node {cluhost1}:
* Node {cluhost2}:

----

Each group consists of three resources, rsc_ip / rsc_fs and rsc_sap. The resource rsc_ip_hawk is the
virtual IP for the HAWK web interface.

////
_As user _{sid2-lc}adm_ e.g. on ASCS host check the SAP system
//[subs="specialchars,attributes,verbatim,quotes"]
//======
[subs="attributes"]
----
# su - {sid2-lc}adm
# sapcontrol -nr {inst-ascs2} -function GetSystemInstanceList
# sapcontrol -nr {inst-ascs2} -function GetProcessList -host {v-ascs2host}
# sapcontrol -nr {inst-ers2} -function GetProcessList -host {v-ers2host}
----
//======
////
=========

== Example ENSA2 in a Two Node Cluster
//[[2nensa2]]
As known from the setup based on our *Best Practice* for {s4Hana}
(https://documentation.suse.com/sbp/all -> SAP S/4 HANA - Enqueue Replication 2 High Availability Cluster - Setup Guide)
the cluster integration can be done with a configuration file. Create one configuration file for each SID
and load them once by once into the cluster. The following steps will help to do the steps in the right direction.

The cluster is already up and running and still run one ASCS / ERS configuration based on the group concept.
This task must be done:

* stop the new installed SAP system, which will be added into the cluster
* umount the filesystem and remove the IP address from the new SAP system
* check / modify the ASCS and ERS profile files and add the SUSE HAE functions, if not done so far
* modify the haclient group on each cluster node and add the new <sid>adm to it, if not done so far
* set the cluster in maintenance mode
* load the new configuration with _crm configure load update <filename>_
* pre check the cluster with _crm status_
* release the cluster maintenance mode
* check the cluster and SAP instances with _crm_mon_ and _sapcontrol -nr ..._

.Prepare a configuration file and extend the cluster
=========
_As user *root*_ prepare a file for the SAP system {sid2}
//[subs="specialchars,attributes,verbatim,quotes"]
//======
[subs="attributes"]
----
#  cat ensa2-2nd-{sid2-lc}.txt

primitive rsc_fs_{sid2}_ASCS{inst-ascs2} Filesystem \
	params device="{ascs2nfsexp}" directory="/usr/sap/{sid2}/ASCS{inst-ascs2}" fstype=nfs options="vers=3" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=300s
primitive rsc_fs_{sid2}_ERS{inst-ers2} Filesystem \
	params device="{ers2nfsexp}" directory="/usr/sap/{sid2}/ERS{inst-ers2}" fstype=nfs options="vers=3" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=300s
primitive rsc_ip_{sid2}_ASCS{inst-ascs2} IPaddr2 \
	params ip={vip-ascs2} \
	op monitor interval=10s timeout=20s
primitive rsc_ip_{sid2}_ERS{inst-ers2} IPaddr2 \
	params ip={vip-ers2} \
	op monitor interval=10s timeout=20s
primitive rsc_sap_{sid2}_ASCS{inst-ascs2} SAPInstance \
	operations $id=rsc_sap_{sid2}_ASCS{inst-ascs2}-operations \
	op monitor interval=11 timeout=60 \
	op_params on_fail=restart \
	params InstanceName={sid2}_ASCS{inst-ascs2}_{v-ascs2host} START_PROFILE="/sapmnt/{sid2}/profile/{sid2}_ASCS{inst-ascs2}_{v-ascs2host}" AUTOMATIC_RECOVER=false \
	meta resource-stickiness=5000
primitive rsc_sap_{sid2}_ERS{inst-ers2} SAPInstance \
	operations $id=rsc_sap_{sid2}_ERS{inst-ers2}-operations \
	op monitor interval=11 timeout=60 \
	op_params on_fail=restart \
	params InstanceName={sid2}_ERS{inst-ers2}_{v-ers2host} START_PROFILE="/sapmnt/{sid2}/profile/{sid2}_ERS{inst-ers2}_{v-ers2host}" AUTOMATIC_RECOVER=false IS_ERS=true
group grp_{sid2}_ASCS{inst-ascs2} rsc_ip_{sid2}_ASCS{inst-ascs2} rsc_fs_{sid2}_ASCS{inst-ascs2} rsc_sap_{sid2}_ASCS{inst-ascs2} \
	meta resource-stickiness=3000
group grp_{sid2}_ERS{inst-ers2} rsc_ip_{sid2}_ERS{inst-ers2} rsc_fs_{sid2}_ERS{inst-ers2} rsc_sap_{sid2}_ERS{inst-ers2}
colocation col_sap_{sid2}_no_both -5000: grp_{sid2}_ERS{inst-ers2} grp_{sid2}_ASCS{inst-ascs2}
order ord_sap_{sid2}_first_start_ascs Optional: rsc_sap_{sid2}_ASCS{inst-ascs2}:start rsc_sap_{sid2}_ERS{inst-ers2}:stop symmetrical=false
----
//======

_As user *root*_ prepare a file for the SAP system {sid3}
//[subs="specialchars,attributes,verbatim,quotes"]
//======
[subs="attributes"]
----
#  cat ensa2-3nd-{sid3-lc}.txt

primitive rsc_fs_{sid3}_ASCS{inst-ascs3} Filesystem \
	params device="{ascs3nfsexp}" directory="/usr/sap/{sid3}/ASCS{inst-ascs3}" fstype=nfs options="vers=3" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=300s
primitive rsc_fs_{sid3}_ERS{inst-ers3} Filesystem \
	params device="{ers3nfsexp}" directory="/usr/sap/{sid3}/ERS{inst-ers3}" fstype=nfs options="vers=3" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=300s
primitive rsc_ip_{sid3}_ASCS{inst-ascs3} IPaddr2 \
	params ip={vip-ascs3} \
	op monitor interval=10s timeout=20s
primitive rsc_ip_{sid3}_ERS{inst-ers3} IPaddr2 \
	params ip={vip-ers3} \
	op monitor interval=10s timeout=20s
primitive rsc_sap_{sid3}_ASCS{inst-ascs3} SAPInstance \
	operations $id=rsc_sap_{sid3}_ASCS{inst-ascs3}-operations \
	op monitor interval=11 timeout=60 \
	op_params on_fail=restart \
	params InstanceName={sid3}_ASCS{inst-ascs3}_{v-ascs3host} START_PROFILE="/sapmnt/{sid3}/profile/{sid3}_ASCS{inst-ascs3}_{v-ascs3host}" AUTOMATIC_RECOVER=false \
	meta resource-stickiness=5000
primitive rsc_sap_{sid3}_ERS{inst-ers3} SAPInstance \
	operations $id=rsc_sap_{sid3}_ERS{inst-ers3}-operations \
	op monitor interval=11 timeout=60 \
	op_params on_fail=restart \
	params InstanceName={sid3}_ERS{inst-ers3}_{v-ers3host} START_PROFILE="/sapmnt/{sid3}/profile/{sid3}_ERS{inst-ers3}_{v-ers3host}" AUTOMATIC_RECOVER=false IS_ERS=true
group grp_{sid3}_ASCS{inst-ascs3} rsc_ip_{sid3}_ASCS{inst-ascs3} rsc_fs_{sid3}_ASCS{inst-ascs3} rsc_sap_{sid3}_ASCS{inst-ascs3} \
	meta resource-stickiness=3000
group grp_{sid3}_ERS{inst-ers3} rsc_ip_{sid3}_ERS{inst-ers3} rsc_fs_{sid3}_ERS{inst-ers3} rsc_sap_{sid3}_ERS{inst-ers3}
colocation col_sap_{sid3}_no_both -5000: grp_{sid3}_ERS{inst-ers3} grp_{sid3}_ASCS{inst-ascs3}
order ord_sap_{sid3}_first_start_ascs Optional: rsc_sap_{sid3}_ASCS{inst-ascs3}:start rsc_sap_{sid3}_ERS{inst-ers3}:stop symmetrical=false
----
//======

.Configuration Example for SID {sid4} and following

This is similar to the SID examples before. You must adapt:

* filesystem sources which will be mounted
* IP address for virtual hostname of ASCS and ERS
* Instance number for ASCS and ERS
* local mount point for ASCS and ERS
* profile path for ASCS and ERS


.Add the 2nd and further SID into the cluster

The cluster is already up and running and still run one ASCS / ERS configuration based on the group concept.
This task must be done:

* stop the new installed SAP system, which will be added into the cluster
* umount the filesystem and remove the IP address from the new SAP system
* check / modify the ASCS and ERS profile files and add the SUSE HAE functions, if not done so far
* modify the haclient group on each cluster node and add the new <sid>adm to it, if not done so far
* set the cluster in maintenance mode
* load the new configuration with _crm configure load update <filename>_
* pre check the cluster with _crm status_
* release the cluster maintenance mode
* check the cluster and SAP instances with _crm_mon_ and _sapcontrol -nr ..._

Login to one of the cluster nodes and run the commands from there.

_As user *root*_ set cluster maintenance and load the configuration into the cluster
//[subs="specialchars,attributes,verbatim,quotes"]
//======
[subs="attributes"]
----
# crm configure property maintenance-mode=true
# crm configure load update ensa2-2nd-{sid2-lc}.txt
# crm configure load update ensa2-3nd-{sid3-lc}.txt
# crm configure load update ensa2-4nd-{sid4-lc}.txt
...
# crm status
----
//======
You should *not* get a message back only the prompt. If there are messages there is maybe something wrong in the configuration file.

NOTE: If there is a wrong path e.g. for the profile this will not be detected during the configuration load. But during the cluster start action it will be shown.

_As user *root*_ release cluster maintenance and check the new cluster resources
//[subs="specialchars,attributes,verbatim,quotes"]
//======
[subs="attributes"]
----
# crm configure property maintenance-mode=false
# crm_mon -rfn
----
//======

_As user _{sid2-lc}adm_ e.g. on ASCS host check the SAP system
//[subs="specialchars,attributes,verbatim,quotes"]
//======
[subs="attributes"]
----
# su - {sid2-lc}adm
# sapcontrol -nr {inst-ascs2} -function GetSystemInstanceList
# sapcontrol -nr {inst-ascs2} -function GetProcessList -host {vhn-ascs2}
# sapcontrol -nr {inst-ers2} -function GetProcessList -host {vhn-ers2}
----
//======

NOTE: Repeat the steps for each SID you have installed.

////
_As user *root*_ showing a cluster configuration for two SAP systems
//[subs="specialchars,attributes,verbatim,quotes"]
//======
[subs="attributes"]
----
#  crm configure show


----
//======
////
=========

== Example ENSA2 in a Multi Node Cluster

<<Example ENSA2 in a Two Node Cluster>>

TODO: loadbalancing

As known from the setup based on our *Best Practice* for {s4Hana}
(https://documentation.suse.com/sbp/all -> SAP S/4 HANA - Enqueue Replication 2 High Availability Cluster - Setup Guide)
the cluster integration can be done with a configuration file. Create one configuration file for each SID
and load them once by once into the cluster. The following steps will help to do the steps in the right direction.

The cluster is already up and running and still run one ASCS / ERS configuration based on the group concept.


[subs="attributes"]
//[subs="specialchars,attributes,verbatim,quotes"]
----
//======
# placeholder
//======
----

== Example ENSA1 and ENSA2 in a Two Node Cluster
//[[2nensa1ensa2]]

As known from the setup based on our *Best Practice* for {sapNW} and {s4Hana}
(https://documentation.suse.com/sbp/all -> SAP NetWeaver High Availability Cluster 7.40 - Setup Guide)
(https://documentation.suse.com/sbp/all -> SAP S/4 HANA - Enqueue Replication 2 High Availability Cluster - Setup Guide)
the cluster integration can be done with a configuration file. Create one configuration file for each SID
and load them once by once into the cluster. The following steps will help to do the steps in the right direction.

The cluster is already up and running and still run one ASCS / ERS configuration based on the group concept.

NOTE: Combine the chapter "Example ENSA1 in a Two Node Cluster" and "Example ENSA2 in a Two Node Cluster".
Be careful and keep in mind each SAP system must be independent from each other.

<<Example ENSA1 in a Two Node Cluster>>
<<Example ENSA2 in a Two Node Cluster>>

////
[subs="specialchars,attributes,verbatim,quotes"]
======
======
////

== Example ENSA1 and ENSA2 in a Multi Node Cluster

TODO: prohibited host for ENSA1

<<Example ENSA1 and ENSA2 in a Two Node Cluster>>

////
[subs="specialchars,attributes,verbatim,quotes"]
======
======
////

== Testing the New Configuration of Multiple SID in One Cluster

IMPORTANT: A well and overall testing of the new configuration is extremely recommended. Please
read the requirements for each setup carefully. The test procedure will show if the cluster works as expected.
We already have described multiple test scenarios in our base Best Practice documentation for
{sapNW} and {s4Hana}.


// Standard SUSE Best Practices includes
== Legal Notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
:leveloffset: 1
include::common_gfdl1.2_i.adoc[]
