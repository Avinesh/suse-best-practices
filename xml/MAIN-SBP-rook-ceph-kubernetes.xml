<?xml version="1.0" encoding="UTF-8"?>
<!--<?oxygen RNGSchema="http://www.oasis-open.org/docbook/xml/5.0/rng/docbook.rng" type="xml"?>-->
<!DOCTYPE article [
<!ENTITY % entity SYSTEM "entity-decl.ent">
%entity;
]>
<article role="sbp" xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
	xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="rook-kubernetes-ceph"
	xml:lang="en">
	<info>
		<title>Rook Best Practices for Running Ceph on Kubernetes </title>
		<productname>SUSE Enterprise Storage, Ceph, Rook, Kubernetes, Cloud as a Service Platform</productname>
		<author>
			<personname>
				<firstname>Blaine</firstname>
				<surname>Gardner, Senior Software Developer</surname>
			</personname>
		</author>
		<author>
      <personname>
				<firstname>Alexandra</firstname>
				<surname>Settle, Senior Information Developer</surname>
			</personname>
		</author>
		<date>May 14, 2020</date>
	</info>
  <sect1 xml:id="rook-bp-overview">
    <title>Overview</title>
		<para>
			Ceph and Kubernetes are both complex tools and harmonizing
			the interactions between the two can be daunting. This is
			especially true for users who are new to operating either
			system, prompting questions such as:
		</para>
		<simplelist type="vert">
			<member>“How can I restrict Ceph to a portion of my nodes?”</member>
			<member>“Can I set Kubernetes CPU or RAM limits for my Ceph daemons?”</member>
			<member>“What are some ways to get better performance from my cluster?”</member>
		</simplelist>
		<para>
			This document covers tested patterns and best practices to answer
			these questions and more. Our examples will help you configure and
			manage your Ceph cluster running in Kubernetes to meet your needs.
		</para>
		<para>
			This is a moderately advanced topic, so basic experience with Rook
			is recommended. Before you begin, ensure you have the following
			requisite knowledge:
		</para>
		<itemizedlist>
			<listitem>
				<para>
					Basics of Kubernetes
				</para>
			</listitem>
			<listitem>
				<para>
					How to create Kubernetes applications using manifests
				</para>
			</listitem>
			<listitem>
				<para>
					Kubernetes topics:
				</para>
				<itemizedlist>
					<listitem>
						<para>
							Pods
						</para>
					</listitem>
					<listitem>
						<para>
							Nodes
						</para>
					</listitem>
					<listitem>
						<para>
							Labels
						</para>
					</listitem>
					<listitem>
						<para>
							Topology
						</para>
					</listitem>
					<listitem>
						<para>
							Taints and tolerations
						</para>
					</listitem>
					<listitem>
						<para>
							(Anti-)affinity
						</para>
					</listitem>
					<listitem>
						<para>
							Resource requests
						</para>
					</listitem>
					<listitem>
						<para>
							Limits
						</para>
					</listitem>
				</itemizedlist>
			</listitem>
			<listitem>
				<para>
					Ceph components and daemons, basic Ceph configuration
				</para>
			</listitem>
			<listitem>
				<para>
					Rook basics and how to install Rook-Ceph. For more information
					see <link xlink:href="https://rook.io/docs/rook/master/ceph-storage.html"/>.
				</para>
			</listitem>
		</itemizedlist>
		<para>
			In places, we will give examples that describe an imaginary
			data center. This data center is hypothetical, and it will focus
			on the Ceph- and Rook-centric elements and ignore user applications.
		</para>
		<para>
			Our example data center has two rooms for data storage. Ceph must
			have at least three monitor (MON) nodes, and these should be spread
			across fault-tolerant rooms if possible. The example will have a
			separate failure domain for the third monitor node. As such, our
			hypothetical data center has two rooms and one failure domain, with
			the following configuration:
		</para>
		<itemizedlist>
			<listitem>
				<para>
					The failure domain is small and can only to be used for the
					third Ceph MON; it does not have space for storage nodes.
				</para>
			</listitem>
			<listitem>
				<para>
					Eight OSD nodes provides a good amount of data safety without
					requiring too many nodes.
				</para>
			</listitem>
			<listitem>
				<para>
					These eight nodes should be equally separated — four to each data center room.
				</para>
			</listitem>
			<listitem>
				<para>
					The four nodes are separated in each room into two racks.
				</para>
			</listitem>
			<listitem>
				<para>
					In the event of a MON node failure, ensure that you can run MONs on
					each rack for failure scenarios .
				</para>
			</listitem>
		</itemizedlist>
		<para>
			The data center looks as follows:
		</para>
  </sect1>
	<sect1 xml:id="rook-bp-intro">
    <title>Introduction</title>
  </sect1>
	<sect1 xml:id="rook-bp-general">
    <title>General Best Practices</title>
  </sect1>
	<sect1 xml:id="limit-ceph-specifc-nodes">
    <title>Limiting Ceph to Specific Nodes</title>
  </sect1>
	<sect1 xml:id="segregate-ceph-user-app">
    <title>Segregating Ceph from user applications</title>
  </sect1>
	<sect1 xml:id="planning-nodes-ceph-daemons">
    <title>Planning the nodes where Ceph daemons will run</title>
  </sect1>
	<sect1 xml:id="hardware-resource-req">
    <title>Hardware resource requirements and requests</title>
		<sect2 xml:id="resource-req-mon-mgr">
			<title>Resource requests - Mon/MGR</title>
		</sect2>
		<sect2 xml:id="resource-req-osd-cpu">
			<title>Resource requests - OSD CPU</title>
		</sect2>
		<sect2 xml:id="resource-req-osd-ram">
			<title>Resource requests - OSD productname</title>
		</sect2>
		<sect2 xml:id="resource-req-gateways">
			<title>Resource requests - Gateways</title>
		</sect2>
  </sect1>
	<sect1 xml:id="basic-performance-enhancements">
		<title>Basic Performance Enhancements</title>
	</sect1>
</article>
