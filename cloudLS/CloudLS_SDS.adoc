== Storage

A working implementation for the storage of customer payload data is of
crucial importance to any setup present in modern IT. This chapter
elaborates on the relevance of storage and a typical implementation in
conventional setups. It also explains how storage requirements 
are different in scalable platforms and clouds and why the
conventional approaches are not suitable. Finally, it details what object 
storage is, how Ceph works and how SUSE Enterprise Storage based on Ceph
is an ideal storage solution for highly scalable environments such as 
OpenStack.

=== Storage in Conventional Setups

The need for resilient and robust storage is nothing new. Every 
IT administrator is familiar with storage solutions such as SAN appliances 
provided by vendors such as EMC, Dell or HP. For many years and good reasons,
these solutions were considered state-of-the-art when it came to storing 
data in IT setups. Said solutions combine easy-to-use GUIs and management 
interfaces with a solid technical performance. They come, however, at a high 
cost. Nevertheless, SAN appliances have been around for a long time and 
continue to be successful. 

==== SAN-Based Storage Architectures

SAN storage appliances are a central component of conventional setups. 
Although they are very important for the functionality of the environment, 
SAN storages often are not designed redundantly. This is because redundant 
SANs require the device to be present twice, which would double the
costs for the setup. Most SAN storages use technologies such as RAID to 
ensure internal redundancy. In case of 
the loss of the device or a whole site because of a network outage or a
catastrophy, however, the data stored on the SAN becomes unavailable.

As SAN storages usually act as central data shelf in their setups, the
surrounding IT systems need a way to connect to them. This typically is
done using either the FibreChannel protocol or Ethernet in combination
with protocols such as iSCSI. While a single SAN instance may feature a
lot of diskspace (as in several terabyte), it is usually cut into small
slices using the supplied SAN management software. These slices are then
made available to the consuming systems, which will use them either via
iSCSI and the network connection available or by connecting to them via
special FibreChannel HBAs. The storage volume that becomes available on
the host is then consumed by services such as KVM or Xen or for the use
with a standard, POSIX-compliant file system.

When connected via FibreChannel, SAN storage devices deliver very high
performance and much better performance than network-connected solutions
in general. This comes, however, at the disadvantage of having to
maintain a second, independent network infrastructure, which adds some
remarkable administrative overhead to the setup.

==== Problems of SAN-Based Storage

Typical SAN-based storages come with a number of problems, some of which
have already been mentioned in this chapter. The bigget challenges with
typical SAN installations are these:

- *Very high TCO*: As SAN storages usually come with a *vendor lock-in*,
  replacement or spare parts as well as extension kits can only be bought
  from the original vendor. Often, the supplied hardware is standard
  COTS hardware that comes with a modified firmware at a price twice as
  high as the original part would be on the free market. Redundancy is
  also hard to achieve because it would double the already immense costs
  of typical SAN devices.

- *Lack of scalability*: SAN storages are usually limited in the overall
  amount of hard disks that fit in. Once a device is "full" and no more
  extension chassis can be connected, the only way to add more storage
  is to add larger disks -- or to install a second, independent SAN.
  That approach, however, does away with the huge avantange of having a
  single point of administration and is not very appealing. Coming from
  a cloud perspective, the scalability of SAN devices is not sufficient
  as for clouds, seamless scalability is required.

- *Lack of proper connectivity*: While FibreChannel is a nice goodie in
  small and medium-sized setups, it doesn't work at scale in clouds. As
  the number of target nodes in clouds can easily reach hundreds or even
  thousands, creating a shadow FibreChannel infrastructure becomes next
  to impossible.

=== Proprietary Scale Out Storage Solutions

As cloud environments started to become more widespread, the vendors of
typical SAN storage solutions have come up quickly with new products and
solutions. These were supposed to accomodate the need for resilient and
seamlessly scalable storage. Numerous well-known vendors entered the
market with similar, hardware-centric solutions. Other vendors decided to
face the challenge by providing proprietary and commercial software
solutions that would allow to for seamless scalability on standard
hardware. When looking at history of such products, it becomes quickly
obvious that many of them have already disappeared again while other
solutions have turned out to be long-lasting.

==== Issues and Limitations

When looking at the issues present in many commercial scale out storage
solutions, a few factors regularly appear, namely

- *The need for special hardware*: While some solutions for scale out
  storage allow to use COTS hardware, many commercial variantes still do
  insist on special hardware that needs to be bought from the vendor who
  also delivers the storage solution. This keeps the lock-in effect up
  and increases the TCO of the storage solution massively.

- *A lock-in effect for commercial software*: When customers decide to
  buy a proprietary storage solution from a certain vendor, the almost
  always buy a "black box" that does not, for instance, provide access
  to the source code of the solution. This is a notable risk: Scalable
  platforms are usually not expected to reach an EOL status at all, so
  constant hardware updates will be necessary. If, however, the vendor
  of the storage solution in use decides to discontinue the product at a
  certain point in time, it may be impossible to even install it on new
  hardware. The same concerns are true for scenarios in which the vendor
  discontinues the product altogether: In such cases, the customer
  either must have a contract that grants him access to the source code
  under these circumstances, or he will have the need to migrate to
  another platform. Both approaches come with substantial strugge and
  the need for massive investment.

- *Support available from one company only*: Another effect of the
  aforementioned lock-in effect is the necessity to buy support from the
  same vendor who delivered the original solution. As the vendors know
  that there is no way out for customers anyway, support contracts do
  usually come with largish price tags, further increasing the TCO of
  the storage solution. And in case of emergencies it is almost totally
  impossible for the ISP to debug their platform themselves -- instead,
  they are totally dependent on the solution provider.

=== Storage Requirements in Scale-Out Environments

Based on the explanations that this chapter has given so far, it is very
obvious that conventional storage approaches such as SANs or commercial
scale out solutions do not fit very well into the principle of scale and
cloud setups in general. To figure out the ideal solution for a setup's
storage needs, it is helpful to define the basic features that the setup
expects the storage solution to have. The following paragraphs list
these features in deep detail.

==== Required Storage Types

Clouds usually require two different kinds of storage types -- one is
for data payload resulting from virtual machines, the other is related
to the storage of asset data such as video files and images.

[[Ephemeral_and_Persistent_Storage]]
When talking about storing data in clouds, terms such as "persistent"
and "ephemeral" will also often be used. It's important to understand
what these mean to measure their effect on the functionality of a cloud.
"Ephemeral" storage is usually the volatile storage space that a VM in a
cloud computing environment has for its main system disk. As VMs in the
cloud are generally expected to be reproducible at any time from scratch
by means of automation and orchestration, there is no point in storing
the main system installation on a persistent storage volume.

In stark contrast to that, "persistent" storage in clouds usually is the
kind of storage that is maintained independently from virtual machines
but can be attached to arbitrary virtual machines at any point in time.
When running a database such as MariaDB in a cloud. for instance, the
data belonging to said database would usually reside on a persistent and
permanent volume provided as a block device. Following this architecture,
the virtual machine running MariaDB could easily be replaced while
having exactly the same data in place -- the volume would simply be moved
to the new VM.

Persistent storage is where typical storage solutions come in -- the
need for ephemeral VM storage is usually served using the local space
present on the compute nodes.

In addition to these two storage types, clouds are generally expected to
offer an object-storage service that allows for access via the ReSTful
protocol, which is based on HTTP(S). Amazon S3 is the best-known type of
implementation for such a service: Users upload their asset data using
the ReST protocol and can access them later from anywhere in the world.

This kind of storage is often used as replacement for central asset
stores based on technologies such as NFS. Instead of having all servers
running a certain application access asset data from shared media such
as NFS, media is centrally accessed from a central asset store. This is
usually much more performant than using the full set of POSIX metrics
available in standard file systems and even more so given that object
stores can easily be combined with Content Delivery Networks (CDNs) for
effective caching.

==== Seamless Scalability

Solutions such as SUSE Cloud allow for indefinite growth and scalability
on the compute side of things -- the storage in the platform is hence
expected to provide the same functionality when it comes to storing data.

==== COTS Hardware

If a cloud setup is successful, it will likely be growing even in several
years from now -- no matter whether the own storage is based on Free and
Open Source software or a proprietary product: It will very likely not be
possible to get the same hardware in several years from now that was
bought for the original incarnation of the setup. Hence, the hardware
used for cloud storage must be as generic as possible -- the servers in
use must at least be Intel-based standard machines for which replacements
will be available even several years later. A nice side-effect of using
COTS hardware is, of course, the fact that it is usually much cheaper than
spezialized hardware for proprietary solutions -- and one can also choose
between a variety of many suppliers and even negotiate prices. This also
leads to the possibility to use the same hardware class for compute and
storage servers - in different configurations. 

==== Open Technology

The fate of the data belonging to the own setup should not be only at the
discretion of a commercial provider and a proprietary product. Free and
Open Source software effectively avoids lock-ins and makes it possible to
understand, operate and maintain a platform even if the original inventor
of the solution has lost interest of does not exist anymore. Open
technology in use also helps to keep the costs for support low: Usually,
ISPs will have the choice between a large number of providers offering
support for a certain product. Also, the more widespread a solution is,
the smaller is the probability that it will simply disappear from the
market.

==== Single Point of Administration

Supplying large cloud environments with arbitrary amounts of storage is in
fact not the most complicated task. A very complex task is, however, to
provide a storage solution that has only a single point of administration.
Think of it like this: Having dozens or hundreds of JBOD chassis through
the setup wildly connected to individual servers would actually accomodate
for disk space -- but a setup like that is hard to even imagine in a state
characterized as "maintainable". Hence, a storage solution for a cloud
setup does not only need to provide an arbitrary amount of storage devices
-- it also needs to provide a central and single point of administration.

==== Integration into an Existing Cloud

In clouds, large storage setups for scale out data is usually provided as
one logical instance that is then cut into small pieces which are assigned
to services such as VMs. Based on the consumption-based payment model,
users must have the opportunity to create new storage devices and assign
them to their accounts in the cloud at any time and at their discretion.
For this mechanism to work, the storage is expected to provide a proper
interface for the cloud platform to connect to -- both services must, in
fact, be seamlessly integrated to provide a maximum of comfort for every
customer in the setup.

=== The Perfect Alternative: Object Storage

A new approach to scalable storage for cloud environments was Ceph (or,
as its previous name was, RADOS). Ceph is an object storage and allows
for storage environments to be build spanning across thousands of servers
and millions of individual storage devices, making storages in sizes of
several petabytes a reality. In the following chapter, this document
will explain the basic issue of building seamlessly scalable storages
and how Ceph works around these issues. After a quick Ceph introduction,
this document will focus on how Ceph as part of SUSE Storage and SUSE
Cloud based on OpenStack team-up as the perfect couple for compute needs
and storage needs in large-scale environments.

==== An Introduction to Object Storage

All storage devics found in modern-days electrical devics are referred
to as "block storage" devices because they organize internally based on
"blocks". A block is a chunk of data that must be read from the device
completely and written to the device completely in case something goes
wrong. This holds true for expensive flash-based SSDs for servers just
as well as for the average USB memory stick one can buy from the local
consumer hardware dealer of the own preference.

The issue with standard block devices is that they do not provide any
mechanism to write data onto them or to read data from them in any
structured manner. In other words: It would be possible to write down a
certain piece of information into a block on an SSD or a hard disk --
but in order to find read said information later, it would be necessary
to read the device's complete content and then filter for the data that
is being looked for. Obviously, this will not work in everyday's IT as
the performence of this approach is less than optimal.

To work around the shortcommings of block-based storage devices, little
helpers are required -- these are generally referred to as file systems.
A file system's sole responsibility is to add a structure to a storage
device that the fileystem understands well to write down data controlled
onto certain areas of the device -- and read them in the same controlled
manner at a later point in time. Most users will already have dealt with
filesystems: Typical Windows filesystems include NTFS and FAT32 while in
Linux, Ext4 and XFS are very widespread. File systems have continuously
and massively improved over the course of the last 15 years -- and today
count as a very handy utility to make use of storage devices properly.

They do, however, have one big disadvantage: Most file systems assume a
tight bonding between the physical device and the file system on top of
it. That is why scaling out a storage based on block-storage devices is
a huge technical challenge: It is simply not possible to take an already
existing file system, split it into numerous stripes and distribute
these over multiple physical devices (which could, of course, be in a
number of different servers). This would simply corrupt the file systems
and render them unusable.

This is where object-storage solutions come in. Object stores consider
any and all pieces of information stored in them to be binary data --
and binary data can, at any point in time, be split arbitrarily and put
together again later as long as both processes happen in the correct
order. Based on this principle, object stores add an intermediate layer
between the physical storage devices on the one hand and the actual data
on the other -- the object layer. Following this principle, the amount
of storage devices supported in the background is usually only limited
by physical factors such as the available space in a given datacenter.
The logical object-storage layer, in stark contrast, will scale to
almost any size ("almost" as a few limitations actually exist when setups
grow to sizes of several hundreds of millions of disks).

One of the most prominent solutions in terms of object storages is Ceph.
Originally invented by Sage Weil for the American EPA, it has quickly
evolved to a valuable, Open Source storage solution that is backing many
of the largest cloud setups throughout the world.

=== An Introduction to Ceph

Ceph is a perfect example for the principle of object storages -- just as
described in the previous paragraph, Ceph will consider any kind of data
uploaded into it a binary object. It will split these objects into many
smaller objects (the default size per object is four megabytes) and then
distribute these objects onto numerous hard disks present in its backend.
To better understand this process, a quick glance at how Ceph works is
necessary. Thanks to its modular design, understanding the basic design
of Ceph is easy, though.

As explained earlier, Ceph was originally planned as a massive storage
platform for the American Environmental Protection Agency EPA. The target
of lead-developer Sage Weil was to create a seamlessly scalable platform
to replace shared storage solutions such as NFS. Originally, Ceph was,
in the first place, planned as a POSIX-compatible file system backed by
an object store. Of course, said object store had its own name: RADOS,
which stands for *Reliable Autonomous Distributed Object Store*. During
its first releases, Ceph was originally using the name RADOS while Ceph
was used for the POSIX-compatible filesystem on top of RADOS. Later, the
object store got renamed to Ceph and the filesystem to CephFS. The name
RADOS, however, will still appear here and there in old documentation or
in the development discussions of Ceph developers.

To understand what RADOS actually means, it helps to start reading the
self-describing name from the back. "Object Store" characterises the
kind of storage that RADOS provides -- it is an object store considering
all uploaded data to be binary objects. Distributed stands for the fact
that RADOS can spread individual binary objects over an almost endless
amount of storage devices in its backend, and the storage devics may
well be distributed across different servers, different firezones in a
datacenter or different physical locations. Autonomous means that RADOS
is taking care of its health and the integrity of the data stored in
it itself -- if, for instance, a storage device fails, Ceph will make
sure that no data loss occurs from this event. Last but not least, the
word "Reliable" points to the fact that RADOS has built-in replication
and redundancy and is also capable to re-enforce replication policies in
case of hardware failures without manual intervention.

==== The Ceph Storage Backend

The Ceph object store is built of three different services that together
provide the desired functionality: OSDs, MONs and MDSes.

OSD is the acronym for "Object Storage Device". OSDs are the data silos
in Ceph: Any block device can act as an OSD for the Ceph object storage.
The cool thing about OSDs is that they can appear in almost any scheme
in a platform -- they can be distributed over as many servers as the
admin sees fit in the same room of a datacenter, different rooms or even
onto different datacenters. OSDs are responsible for serving clients who
want to write or read a specific binary object. They also take care of
the internal replication of binary objects -- as soon as an OSD receives
a new binary object, it will automatically copy said object to as many
other OSDs as the replication policy requires. For said replication, it
is a good idea in general to establish a distinct network connection for
all nodes participating in a Ceph cluster. This helps to ensure that the
"normal" management network connection between nodes does not suffer from
congestion due to Ceph traffic.

MON is the acronym for Monitoring Server. In Ceph, MONs act as some kind
of accountant: They maintain lists of all present MON servers, all MDSes
and all OSDs in the cluster and are responsible for distributing these
to all clients (it needs to be said that from the MON perspective, OSDs
and MDSes also are clients). MONs also enforce quota in Ceph clusters:
If a Ceph cluster gets split into two partitions, MONs will ensure that
only the part of the cluster with the majority of MON servers continues
to function. The other partition will cease operations until the cluster
is fully restored. It's hence perferctly valid to consider MONs a very
crucial component of Ceph setups -- they are, however, not involved in
the data exchange between clients and the OSDs (more on how Ceph clients
store data in the cluster will be explained later in this chapter).

MDS stands for "Metadata Server". MDSes are required only for CephFS,
the Ceph-backed filesystem: They supply POSIX-compatible meta data for
clients accessing the filesystem. As CephFS is not typically used when
it comes to large cloud environments, this document will not elaborate
on it further.

Ceph's scalability features result from the fact that at any point in
time, new OSDs, MDSes or MONs may be added to the cluster even during
the normal operations procedures. Thanks to this, Ceph can scale up to
almost no limits.

==== How Data Storage in Ceph Works: CRUSH

Based on the explanations in the previous paragraphs, it's time to look
at how data storage in Ceph works to understand why Ceph is the perfect
solution for scalable storage. Ceph clients (the following paragraph
will elaborate on those) will initially be configured with the adresses
of at least one working MON server in the Ceph setup. Once they have set
up a connection to a working MON successfully, they will receive a
current copy of the MON map and a current copy of the OSD map from said
MON. Starting from now, they will ignore their statical configuration
and receive information on MONs and OSDs from the MON servers in the
list they have just received. This is part of the auto-healing features
of Ceph: Even if the MON server that a client has configured fails, the
client will still know all other valid MON servers as long as it has a
working MON map. For production setups, at least three MONs are required;
it is also better in general to use uneven numbers of MON servers as the
availability of these is mathematically better. The same obviously counts
for OSDs: OSDs in a production setup should be distributed over at least
three distinct hosts. If factors such as fire proctection areas play a
role, they must also be taken into consideration when acquiring hadware
for a new Ceph deployment.

A client that is equipped with a valid MON and a valid OSD map will, as
soon as it is requested to store a certain file in Ceph, split said file
into binary objects first (4 megabytes each if the original file is not
smaller than that). The client then performs a mathematical calculation
based on the so-called CRUSH algorithm. CRUSH is the algorithm at the
heart of Ceph and its main function; it is a so-called "pseudo-random"
hash algorithm that determines which OSDs receive certain binary objects.
The acronym stands for "Controlled Replication Under Scalable Hashing".
"Pseudo-random" is used to describe CRUSH because it will produce random
results as to where individual binary objects need to be put -- but the
result for a certain calculation will always be the same as long as the
overall layout of the cluster does not change (i.e. as long as no OSDs
fail or new OSDs are added).

Once the client has done the CRUSH calculation for a certain binary
object, it will perform the actual upload. The receiving OSDs notices
that a new binary object has arrived and performs the same calculation
using CRUSH to determine where to put the replicas of this object. As
soon as all replicas have been created, the sending client receives a
confirmation for the successful finish of the write operation -- and the
data is safely stored in Ceph.

If a node of a Ceph cluster containing OSDs fails, all other OSDs will
after a very short time notice this as all OSDs are performing regular
health checks for all other OSDs. Once MONs receive enough messages on
a certain OSD (or many OSDs in the case of the outage of a whole server
containing OSDs) having failed, they will mark the OSD as "down" in the
OSD map and force all clients in the cluster to request an update of
their local OSD map copy. After a configurable timeout, the OSD(s) will
be marked "out" and Ceph-internal recovery processes will automatically
start.

Finally, it shall be noted that CRUSH is not a closed mechanism that is
residing at the core of Ceph and cannot be influenced. A configuration
file that is maintained by the MONs exists: the so-called CRUSH map. In
said map, the administrator can influence the CRUSH behaviour, making it
follow certain replication policies with regards to datacenter rooms,
datacenter fire protection areas or even different data centers. Special
tools in SUSE Enterprise Storage make editing the CRUSH map and having
influence on the CRUSH map easy and concise.

==== Ceph Frontends: CephFS, RBD

Most front-ends available to Ceph have already been mentioned in the
previous paragraphs. CephFS is the "original" front-end but not a very
commonly used front-end in large-scale and cloud environments. Rather
widespread, however, is the use of Ceph's RBD front-end. RBD stands for
"RADOS Block Device" and describes a way to access a Ceph object store
through a block-device layer.

The Linux kernel itself contains an *rbd* kernel module that connects to
a running Ceph cluster and sets up a local block-device that writes into
Ceph in its backend. Based on the RADOS programming library (*librados*),
there also is a native storage-driver available for Qemu, the emulator
that is usually used with KVM on Linux systems. KVM can hence direcetly
use RBD volumes as backing devics for virtual machines without having to
use the *rbd* kernel driver, which allows for massive performance gain,
which allows for massive performance gains.

==== Ceph Frontends: S3 + OpenStack Swift

The third commonly used Ceph-frontend refers to the other type of storage
that clouds are generally expected to provide today: object storage via
a ReSTful protocol. As already explained, Amazon's S3 service is by far
the most widespread service of its kind in the world. OpenStack also has
a solution for storing objects and making them accessible via HTTPs named
OpenStack Swift; out of all OpenStack components, however, Swift is the
one likely to be present in the fewest OpenStack setups. The protocol of
OpenStack Swift is nevertheless very handy and superior to Amazon S3 in
a number of cases. In addition, OpenStack Swift and its protocol also are
official Open Source components while S3 is a proprietary protocol made
available by Amazon.

Now obviously, Ceph is an object store and the idea to store arbitrary
files in Ceph as binary objects perfectly fits into this -- the only piece
missing in the puzzle if access is supposed to happen via S3 or Swift is
a protocol bridge between Ceph and HTTP(S) clients. This is where the
Ceph Object Gateway (also known as RADOS Gateway or RGW) comes in: It is
a translation layer that can communicate with Ceph in its backend and to
clients by using a reverse-engineered version of the S3 protocol or of
OpenStack Swift at the other hand.

Using the Ceph Object Gateway, it actually becomes possible to run a
local "clone" of Amazon S3 in the own datacenter. That way, Ceph can
also provide for the second type of cloud-based data storage perfectly
well. And because Ceph also speaks the OpenStack Swift protocol, there
is no need to rollout Swift as a service, which keeps maintaining the
platform comfortable.

==== Ceph and OpenStack: A Perfect Couple

Customers looking into building a large-scale cloud environment will in
most cases inevitably also face the question of building resilient and
scalable storage. SUSE Enterprise Storage, which is based on Ceph and
supports features such as Erasure Coding and many more, allows companies
to leverage Ceph's advantages the best possible way. Also, OpenStack and
Ceph are a perfect couple as the both became widespread roughly at the
same time and several features on both solutions were developed for the
other component respectively.

The core component for running, administering and distributing persistent
storage devices in OpenStack is OpenStack Cinder. The RBD backend for
Cinder was one of the very first Cinder backends at all that could be
used in production officially several years ago. Since then, a lot of
development and care has gone into the Cinder, making the RBD backends
even more stable and resilient. Using Ceph as a backend storage for
Cinder to supply virtual machines in OpenStack with persistent volumes
is easy and concise and works reliably.

OpenStack Glance has a working backend for Ceph as well. Glance takes
the responsibility for for storing image data used by newly created VMs
and can easily put these image data into Ceph.

OpenStack Manila provides shared storage for virtual machines in clouds;
CephFS, the POSIX-compatible file system in Ceph, can act as backend for
Manila easily.

And last but not least, Ceph with the Ceph Object Gateway can act as a
drop-in replacement for OpenStack Swift, the ReSTful object storage for
asset data. The Ceph Object Gateway even supports authentication using
the OpenStack Identity component ("Keystone") so that administering the
users allowed to access Ceph's Swift backend happens using the OpenStack
tools.

Combining SUSE Enterprise Storage, which is SUSE's Ceph-based offering, 
and SUSE OpenStack Cloud allows for the creation of a highly scalable 
computing platform with highly scalable storage attached to its backend. 
This is a very powerful, stable and rock-solid solution for large-scale 
cloud environments.

==== Disaster Recovery and Off-Site Replication

As Ceph clusters can reach sizes of several terabytes or petabytes,
topics such as backup, restore and disaster recovery are more complex
than in conventional environments. Backing up a storage device of several
petabytes would obviously require a second storage just as large. Many
ISPs do not want to go that way for financial reasons and instead
offload the responsibility of taking backups of relevant data to their
customers.

And the same goes for Disaster Recovery and Off-Site replication: While
it may be tempting to simply split a Ceph cluster onto multiple sites,
this approach does not turn out a good idea at a second look. In order
to guarantee disaster recovery qualities, CRUSH would need to be ensure
that at least one copy of all objects is always existing on both sites.
That, however. adds the latency between the datacenters as latency to
every write process that happens in the cloud. To work around this issue,
Ceph does support certain Disaster Recovery strategies using replication
on the level of the Ceph Object Gateway.

// vim:set syntax=asciidoc:
