== Architecture

Getting a large-scale cloud environment right is a complex task. 
This chapter's purpose is to paint the bigger picture of all the
factors you need to take into consideration. After an introduction into the
principle of the economy of scale, this chapter outlines the main components 
of an OpenStack cloud and how these work together. A special focus is laid 
on designing a resilient and stable scale-out setup along with its individual 
layers and the needed considerations. Last but not least, a typical OpenStack 
architecture is shown to serve as a valid example.

In general, cloud platforms have a complex design and yet allow for massive 
scalability. But what does "scalability" in cloud environments mean?

=== Scalability in Clouds

Scalability is a word that most administrators are familiar with. 
However, a lot of different definitions of scalability exist and the 
word is often used in different contexts. Therfore, it is important to
provide a definition of what scalability is for the purpose of this 
document and what kind of scalability this document focusses on. 

When talking about "scale" processes, administrators usually intend to 
extend the load that a specific setup can process by adding new hardware. 
The way new hardware is added strongly depends on local conditions and
may vary when looking at different setups.

Until a few years ago, the term "scalability" typically used
to refer to a process called "scale-up" or "vertical scaling". This 
describes a process in which existing hardware is extended so that it can 
handle more load. Adding more RAM to an existing server, a stronger 
CPU to a node or additional hard disks to an existing SAN storage 
appliance are typical examples for scaling up. The issue with this approach 
is that, after a while, it cannot be pursued any further due to physical 
limitations. As an example, all of your physical server's memory banks 
with the biggest RAM modules may already be in use. This means you cannot 
expand your server's memory anymore. It may also be impossible 
to replace the CPU in a server simply because for the given CPU socket, 
when no more powerful CPUs are available. And extending SANs
typically fails because all device slots of the SAN appliance are already
in use with the largest hard disks available for this model.

Not being able to scale-up a system further used to be a large issue in 
the past. In many cases, the only possibility to work around the problem 
was to buy completely new, more capable hardware that would be able to 
cope with the load present. That was, however, an expensive and not always 
successful strategy.

The opposite of the process to "scale-up" or of "vertical scaling" is to 
"scale-out" or "horizontal scaling". This approach is fundamentally 
different and assumes that there is no point in extending the existing
infrastructure by replacing individual hardware components. Instead, 
in scale-out scenarios, the idea is to add new machines to the setup 
to distribute the load more evenly to more target systems within the 
installation. This obviously is a superior approach to scale-up
approaches because the only limiting factor is the physical space that
is available in the datacenter. Thanks to dark fibre connections and other
modern technologies, it is even possible to create new datacenter sites 
and connect those to existing sites to accomodate for seamless scale-out 
processes.

NOTE: Not all scalability approaches work for all environments. The
ability to scale-out requires the software in use to support this 
operational mode. Cloud solutions such as OpenStack usually
are built for scale-out environments and have the ability to scale in a
horizontal manner at the core of their functionality. Legacy software,
in contrast to that, may only support scale-up scenarios.

As this document is about scalability in massively scalable environments,
the best scalability approach is to scale-out, means horizontal
scalability. Thus, whenever "scalability" is mentioned in this document, 
it references to scale-out processes, unless noted differently in the 
respective section or paragraph.

=== Cloud Computing Primer

Similar to "scalability", also "cloud" is used as a technical term in 
an almost indefinite number of contexts. This document elaborates
on the architecture of large-scale cloud environments based on SUSE 
OpenStack Cloud. Therefore it is appropriate to define "cloud" and 
"cloud software" in the context of this document.

Conventional IT setups are usually a "turn-key solution" delivered to
the customer for a specific purpose. The customer rarely takes care of
running and operating the solution himself; instead, the IT service provider
does that for him. This is statical and often not satisfying for the customer
or the service provider. 

In cloud setups, service providers become platform providers. Their
main responsibility is to run a platform whose services customers can
consume at their own discretion. In addition to running the platform and
providing resources, these providers also have to offer a way for customers to
"service themselves", which means to consume services without having to contact
the service provider each time. The hardware that typically is used in 
datacenters cannot provide what it takes to offer the described functionalities
without additional software. For the purpose of this document, a
"cloud software" therefore is a software that creates a bridge between the
"platform" or "infrastructure" on the one hand and customers on the other hand, 
allowing them to consume the available resources as dynamically as possible. 

In summary, the following attributes can be used to define "cloud":
- Self Service portal / API access
- Network based
- Pooling of existing resources
- Consumtion based metering


=== OpenStack Primer

OpenStack is the best-known Open Source cloud solution currently
available at the market. It is the fundamental technology for the SUSE
Cloud product and plays an important role when building a large
scale-out cloud based on SUSE products as desribed in this document. An
OpenStack Cloud also consists of several components that are
well worth knowing. The following paragraphs provide an overview of the
components of an OpenStack cloud and you a quick introduction
into OpenStack per se and into how OpenStack will help you to build a
perfectly well scalable compute- and storage platform.

==== The OpenStack History

The OpenStack project originally started as a joint venture between
NASA and the American hosting provider Rackspace. NASA controllers had
found out that many of their scientists were conducting experiments for
which they ordered hardware. When their experiments were finished, often the
hardware would not be used for anything else -- while other scientists
in other departments were ordering new hardware for their respective
experiments. The basic idea behind OpenStack was to create a tool
to centrally administer an arbitrary amount of compute resources and to
allow scientists to consume these resources as seen fit. Rackspace at
their end brought in the OpenStack Swift object storage component, which
will be explained in deeper detail in chapter 4 of this document.

In 2012, NASA withdrew from OpenStack as an active contributor. Nevertheless,
since its official launch in 2010, dozens of companies have decided to 
adopt OpenStack, including solutions from large system vendors such as SUSE,
as their primary cloud technology. Today, the project is very stable and 
reliable, and the functionalities are constantly improved. Thus, OpenStack is
the ideal fundament when building a large scale-out environment.

At the time of writing, OpenStack consists of more than 30 services. Not all of
them are required for a basic cloud implementation -- the number of core
components is generally considered to be six these days (and even out
of those only 5 are strictly necessary). For a minimum viable cloud
setup, a few additional supporting components are also required. The
following paragraphs provide a more detailed description of the
OpenStack base services.

==== Supporting Services: RabbitMQ

OpenStack follows a strictly decentralized approach. Most components of
OpenStack (and the ones described in the following paragraphs in
particular) are not made of a single service but consist of many
small services that often run on different hosts. Yet, all these services
require a way to exchange messages between each other. Message protocols
such as the AMQP standard exist for exactly that purpose and OpenStack
is usually deployed along with the RabbitMQ message bus. RabbitMQ is one
of the oldest AMQP implementations and written in the Erlang programming
language. Several tools in the OpenStack universe use RabbitMQ to send
and receive messages. Hence, every OpenStack setup needs RabbitMQ. For
reasons of performance and redundancy, large-scale environments will
usually have more than one RabbitMQ instance running (more details about
the ideal architecture of services for RabbitMQ and other services will 
be explained further down in this chapter).

==== Supporting Services: MariaDB

A second component that is included in most if not all setups based
on OpenStack is MariaDB (or its predecessor MySQL). Almost all OpenStack
components use MariaDB to store their internal metadata in a persistent
manner. As the overall number of requests to the databases is large, just
like RabbitMQ, MariaDB will typically be rolled out in a highly available
scale-out manner in cloud environments (for example together with the
Galera Multi-master replication solution).

==== Authentication & Authorization: Keystone

The first actual component that every OpenStack cloud must have is the
Identity component, also known as Keystone. Keystone serves two purposes
in OpenStack: It takes care of authenticating users by requiring them to
login to the API services and the GUI with a combination of a username
and a password. And it determines what role a specific user has inside
a project (or "tenant"). All OpenStack components associate certain roles 
with certain permissions. If a user has a certain role in a project, that 
automatically entitles him to the permissions of said role for every respective 
service.

Keystone is one of the few components that only comprise one program; this is
the Keystone API itself. It is capable of conneting to existing user directories 
such as LDAP or Active Directory but can also run in a stand-alone manner.

==== Operating System Image Provisioning: Glance

Glance is an OpenStack component that is often forgotten but nevertheless 
important. Not all customers consuming cloud services are IT professionals.
They may not have the knowledge required to install an operating system 
in a newly created virtual machine (VM) in the cloud. And even IT professionals
who are using cloud services cannot go through the entire setup process for
every new VM they have to create. That would take too much time and hurt the 
principle of the economy of scale.

But it also would be unnecessary. A virtual machine inside KVM can, if
spanwed in a cloud environment, be very well controlled and will be the
same inside different clouds if the underlying technology is identical.
It has hence become quite common for cloud provider to supply users with
a set of basic operating system images compatible with a given cloud. In
OpenStack, Glance is the component storing and administering these OS
images.

==== Virtual Networking: Neutron

Networking is a part of modern-day clouds that shows the most obvious
differences to conventional setups. Most paradigms about networking that
are valid for legacy installations are not true in clouds and often not
even applicable. While legacy setups make use of technologies such as
VLAN on the hardware level, clouds use Software Defined Networking (SDN)
and create a virtual "overlay" networking level where virtual customer
networks reside. Customers have the ability to design their own virtual
network topology according to their needs, without any interaction by
the cloud provider.

In OpenStack, Neutron is the component implementing SDN. Through a system 
of loadable plug-ins, Neutron supports a large number of SDN implementations 
such as Open vSwitch. Chapter 3 will elaborate on networking in OpenStack 
and Neutron in deep detail. It will also explain how networks for clouds 
must be designed to accomodate for the requirements of large-scale cloud
implementations.

==== Persistent VM Block-Storage: Cinder

Conventional setups will often have a central storage appliance such as
a SAN to provide storage to virtual machines through the installation.
These devices come with a number of shortcomings and will usually not
scale the way it is required on large-scale environments. And no matter
what storage solution is in place -- there still needs to be a method to
semi-automatically configure the storage from within the cloud to create
new volumes dynamically. After all, giving administrative rights to all
users in the cloud is not recommended at all.

Cinder is the OpenStack component that takes care of splitting storage
into small pieces and making it available to VMs throughout the cloud.
Chapter 4 elaborates on Cinder and explains in deep detail how it can be
used together with the Ceph object store to provide the required storage
in a scalable manner in cloud environments.

==== Compute: Nova

Nova is the primordial soup of OpenStack -- it's the one component that

//// 
OMG priomorial soup ... in a tech document? 
Nova is the original fundament of OpenStack ??
////

was originally developed by the Nebula project at NASA and from which
most other projects have spawned off.

Nova in OpenStack is the centralized administration of compute resources
and virtual machines throughout the whole setup. Whenever a request to
start a new VM, terminate an existing VM or change a VM is issued by a
user, that request hits the Nova API component first. Nova is built of
almost a dozen different pieces taking care of individual tasks inside a
setup. That includes tasks such as the scheduling of new VMs the most
effective way (that is, answering the question "What host can and should
this virtual machine be running on?") and making sure that accessing the
virtual KVM console of a VM is possible.

Nova is a feature-rich component: Besides the standard hypervisor KVM,
it also supports solutions such as Xen, Hyper-V by Microsoft or VMware.
It has many screws and knobs that allow to control Nova's behaviour and
belongs to the most matur OpenStack components.

==== A Concise GUI: Horizon

Last but not least, the OpenStack Dashboard, Horizon, must be mentioned
in this list of OpenStack components. Many OpenStack users may rarely
ever see it: Clouds function on the principle of API interfaces that
commands can be sent to in a specialized format to trigger a certain
action. This means that all components in OpenStack come with an API 
component that will accept commands based on the ReSTful HTTP approach.

There are, however, some tasks where a graphical representation of the
tasks at hand is helpful and maybe even desired. This is done with
Horizon: It is the standard UI interface of OpenStack and allows concise 
graphical access to all aforementioned components. It is written in Django 
(a Python-based HTML version) and must be combined with a WSGI server.

=== A Perfect Design for OpenStack

To put it into a metaphor: OpenStack is like an orchestra where a whole lot
of instruments need to join forces to play a symphony. That is
even more true for large environments with huge numbers of participating
nodes. What is a good way to structure and design such a setup? How can
companies provide a platform suitable for the respective requirements in
the best and most resilient manner? The following paragraphs will
answer these questions.

==== Logical layers in Cloud environments

To understand how to run a resilient and stable cloud environment, it is
of crucial importance to understand that a cloud typically comes with
several layers. These layers are:

- *The hardware layer*: This layer contains all standard rack servers in
  an environment, this means devices that are not specific network
  devices or other devices such as storage appliances.

- *The network layer*: This layer contains all devices responsible for
  providing physical network connectivity inside the setup and also to
  the outside. Switches, network cabling, upstream routers and special
  devices such as VPN bridges are good examples.

- *The storage layer*: This layer represents all devices responsible for
  providing persistent storage inside the setup along with the software
  components required for that. If solutions such as Ceph are in use,
  the storage layer only represents the software required for Software
  Defiend Storage as the hardware is already part of the hardware layer.

- *The control layer*: This layer includes all logical components that
  belong to the cloud solution, which means OpenStack for the purpose of 
  this   document. All tools and programs in this layer are required for 
  proper functionality of the system.

- *The compute layer*: This layer covers all software components on the
  compute nodes of a cloud environment.
  

A cloud can encounter different scenarios of issues that come with 
different severities. The two most notables categories of issues are:

- *Loss of control*: In such a scenario, existing services in the cloud
  continue to work as before, but it is impossible to control them via
  the APIs provided by the cloud. It is also impossible to start new
  services or to delete existing services.

- *Loss of functionality*: Here, not only is it impossible to control
  and steer the resources in a cloud but instead, these resources have
  become unavailable due to an outage.

When designing resilience and redundancy for large-scale environments,
it is very important to understand these different issue categories and
to understand how to avoid them.

==== Brazing for Impact: Failure Domains

An often discussed topic is the question of how to make a
cloud environment resilient and highly available. It's of crucial
importance to understand that "high availability" in the cloud context
is usually not the same as high availability in the classical meaning of
IT. Most administrators used to traditional IT setups will typically
assume that high availability for clouds means to make every host in
the cloud environment redundant. That is, however, usually not the case.
Cloud environments make a few assumptions on the applications
running inside of them. One assumption is that virtual setups are as
automated as possible. That way, it is very easy to restart a virtual
environment in case the old instance went down. Another assumption
that applications running there are "cloud-native" and inherently resilient 
against failures of the hardware that they reside on.

Hence, most major public cloud providers have created SLAs that sound
radical from the point of view of conventional setups. Most large
public clouds are distributed over several physical sites that providers
usually call "regions". The SLAs of such setups usually contain a statement
according to which the cloud formally counts as "being up" as long as
customers can, in any of the so many regions of a setup, start a
virtual machine that is connected to a virtual network.

It must clearly be stated that the provider of a cloud setup has no way
to guarantee the availability of all hosts in a cloud setup at any time.

// not sure how to reword the above sentence ... sounds weird

The focus of availability is on the control services, which are needed
to run or operate the cloud itself. OpenStack services have a stateless
design and can be easily run in a active / active manner, distributed
on several nodes.  A cluster tool like *pacemaker* can be used to
manage the services and a load balancer in front of all will combine the 
services and make them available for the users.
Any workload running inside the cloud will not be taken into account. 
With the feature "compute HA", SUSE OpenStack offers an exception.
However, it should be used only where it is really needed, because it 
adds more complexity to the environment and makes it harder to maintain. 
It is recommended to create a dedicated zone of compute nodes, which 
provide the high availability feature. 


In all scenarios, however, it makes sense to define failure domains
and to ensure redundancy over these. Failure domains are often referred 
to as "availability zones" in the OpenStack context. They are similar 
to the aforementioned regions but usually cover a much smaller
geological area. 

The main idea behind a failure domain is to include every needed service 
into one zone. Redundancy is created by adding multiple failure domains 
to the design. The setup has to make sure that a failure inside of a 
failure domain will not affect any service in any other failure domain. 
In addition, the function of the failed service must be taken over by another 
failure domain.

It is important that every failure domain is isolatet with regard to
infrastructure like power, networking, cooling. All services (control, compute,
network and storage) have to be distributed over all failure domains.
The sizing has to take into account that even if one complete failure domain
will die, enough resources are available to operate the whole cloud.

The application layer is responsible for distributing the workload over
all failure domains, so that the availability of the application will be 
ensured in case of a failure inside of one failure domain. OpenStack offers
anti-affinity rules to schedule instances in different zones.

The minimum recommended amount of failure domains for large scale-out
setups based on OpenStack is three. With three faiure domains in place,
a failure domain's outage can easily be compensated by the remaining
two. When planning for additional failure domains, it is important to
keep in mind how quorum works: To have quorum, the remaining parts of a
setup must have the majority of relevant nodes inside of them. For example,
with three failure domains, two failure domains would still have the
majority of relevant nodes in case one failure domain goes down. The
majority here is defined by "50% + one full instance".

.Highlevel architecture of failure domain setup with three nodes
image::architectur_high_level.png[align="center",width=300]

==== The Control Layer

The control layer covers all components that ensure functionality and
the ability to control the cloud. All components of this layer must be
present and distributed evenly across the available failure domains,
namely:

- *MariaDB*: An instance of MariaDB should be running in every failure
  domain of the setup. As MariaDB clustering does not support a
  multi-master scenario out of the box, the Galera clustering solution
  can be used to ensure that all MariaDB nodes in all failure domains
  are fully functional MariaDB instances, allowing for write and read
  access. All three MariaDB instances form one database cluster in a
  scenario with three availability zones. If one zone fails, the other
  two MariaDB instances will still function.

- *RabbitMQ*: RabbitMQ instances should also be present in all failure
  domains of the installation. The built-in clustering functionality of
  RabbitMQ can be used to achieve this goal and to create a RabbitMQ
  cluster that resembles the MariaDB cluster described before.

- *Load balancing*: All OpenStack components that users and 
  other components themselves will be using are HTTP(S) interfaces based
  on the ReST principle. In large environments, they will be
  subject to a lot of load. Therefore, in large-scale setups, it is required
  to use load balancers in front of the API instances to distribute the
  incoming requests evenly. This holds also true for MySQL (RabbitMQ however
  has a built-in cluster functionality and is an exception from the rule).

- *OpenStack components*: All OpenStack components and the programs
  that belong to them with the exception of `nova-compute` and
  `neutron-l3-agent` must be running on dedicated hosts ("controller
  nodes") in all failure domains. Typically, powerful machines are
  used to run these on the same hosts together with MariaDB and
  RabbitMQ. As OpenStack is made for scale-out scenarios, there is no issue
  resulting from running these components many times simultaneously.

==== The Network Layer

The physical network is expected to be built so that it interconnects
the different failure domains of the setup and all nodes redundantly. The
external uplink is also required to be redundant. A separate node in
every failure domain should act as "network node" for OpenStack Neutron.
A "network node" will ensure the cloud's external connectivity by running
the `neutron-l3-agent` API extension of OpenStack Neutron.

In many setups, the dedicated network nodes also run the DHCP agent for
Open vSwitch; that is possible and a valid configuration but not under
all circumstances necessary.

OpenStack enriches the existing Open vSwitch functionality with a feature
usually referred to as _Distributed Virtual Routing_ (DVR). In setups
making use of DVR, external network connectivity is moved from the dedicated
network nodes to the compute node. Each compute node will run the routing
service, which are needed by the local instances. This helps in two cases:
- Scale-out: Adding new compute nodes also adds new network capabilities.
- Failure: A failure of a compute node only effects the routing of local instances.
The routing service is independent from the central networking nodes.

Further details on the individual components of the networking layer and
the way OpenStack deals with networking are available in chapter 3 of
this document.

==== The Storage Layer

Storage is a complex topic in large-scale environments. Chapter 4 deals
with all relevant aspects of it and explains how a Software Defined
Storage (SDS) solution such as Ceph can easily satisfy a scalable setup's need
for redundant storage.

Generally speaking, when using an SDS solution, its components
must be distributed across all failure domains so that every domain has
a working storage cluster. Three nodes per domain are the bare minimum.
In the special example of Ceph, the CRUSH hashing algorithm must also be
configured so that it stores replicas of all data in all failure domains
for every write process.

Should the Ceph Object Gateway be in use to provide for S3/Swift storage
via a ReSTful interface, that service must be evenly available in all
failure domains as well. And of course it's necessary to include these
servers in the loadbalancer setup that is in place for making the
API services redundant and resilient.

==== The Compute Layer

Last but not least, when designing a scalable OpenStack Setup, the Compute 
layer plays an important role. While for the control services
no massive scaling is expected, the compute layer is mostly effected by the
ongoing request of more resources.

The most important factor is to scale-out the failure domains
equally. When the setup is extended, comparable amounts of nodes should
be added to all failure domains to ensure that the setup remains balanced.

[[CPU_and_RAM_Ratio]]
When acquiring hardware for the compute layer, there is one factor that
many administrators do not take into consideration although they should:
the required ratio of RAM and CPU cores for the expected workload. To
explain the relevance of this, think of a simple example: If a server
has 256 gigabytes of RAM and 16 CPU cores that split into 32 threads
with hyper-threading enabled, a possible RAM-CPU-ratio for the host is
32 VMs with one vCPU and 8 gigabytes of RAM. One could also create 16
VMs with 16 gigabytes and two vCPUs or 8 VMs with 32 gigabytes of RAM
and 4 vCPUs. The latter is a fairly common virtual hardware layout (a so
called "flavor") example for a general purpose VM in cloud environments.

Now some workloads may be CPU-intense without the need for much RAM or
may require lots of RAM but hardly CPU power. In those cases, users
would likely want to use different flavors such as "4 CPU cores and 256
Gigabytes of RAM" or "16 CPU cores and 16 gigabytes of RAM". The issue
with those is that if one VM with 4 CPU cores but 256 gigabytes of RAM
or 16 CPU cores and 16 gigabytes of RAM runs on a server, the remaining
resources on said machine are hardly useful for any other task -- they
are "blend" and may remain unused completely.

//what does "blend" mean, I don't understand it

Cloud providers must hence consider the workload of a future setup in the 
best possible way and plan compute nodes according to these requirements.
If the setup to be created is a public cloud, pre-defined flavors ought
to lead customers along the desired patterns of usage. If customers do
insist on special flavours, the provider must take the hardware that
possibly remains unused because of differing virtual hardware schemes in
their calculation at least. And if the usage pattern is hard to predict
at all, a mixture of different hardware kinds likely makes the most
sense. From the operational point of view, it should be noted, that always the same
hardware class is used - only in different expansion stages). This helps to 
reduce the effort in mainetance and spare parts.

//most of the paragraph above sounds weird to me ...

OpenStack comes with a number of functions such as Host Aggregates
to make maintaining such platforms convenient and easy. For the sake of
this document, the ratio of CPU and RAM is generally considered 1:4 in
the following examples.

[[ReferenceArchitecture]]
=== Reference Architecture

The following paragraphs describe a basic design reference
for a large-scale SUSE OpenStack Cloud based on OpenStack and Ceph.

.Highlevel Reference Architecure of a large-scale deployment with 108 Compute Nodes and 36 Storage Nodes
image::Reference_Arch_108.png[align="center",width=500]

==== Basic requirements

To build a basic setup for a large-scale cloud with SUSE components, 
the following factors must apply and the following criteria must be fulfilled:

- Three failure domains (at least in different fire protection zones in the
  same datacenter, although different datacenters would be better) that
  are connected redundantly and independently from each other to power
  and networking must be available.
- OSI level 2 network hardware, spawning over the three failure domains
  to ensure connectivity, must be in place. For reasons of latency and
  timings, the maximum distance between the three failure domains should
  not exceed ten kilometers.
- SUSE OpenStack Cloud must be deployed across all failure domains.
- SUSE Enterprise Storage must be deployed across all failure domains.
- SUSE Manager or a Subscription Management Tool (SMT) instance must be 
  installed to mirror all the required software repositories (including all 
  software channels and patches). This provides the setup with the latest 
  features, enhancements, and security patches.
- Adequate system management tools (as explained in chapter 5) must be
  in place and working to guarantee efficient maintainability and to
  ensure compliance and consistency


==== SUSE OpenStack Cloud roles

SUSE OpenStack Cloud functions based on roles. By assigning a host a
certain role, it automatically also has certain software and tasks
installed and assigned to it. Four major roles exist:

- *Administration Server*: The administration server contains the 
  deployment nodes for SUSE OpenStack Cloud and SUSE Enterprise Storage. 
  It is the fundament for the deployment and management of all
  nodes and services as it hosts the required tools. The administration
  servers can also be a KVM virtual machine. The administration services 
  do not need to be redundant. A working backup and restore process is 
  sufficient to ensure the operation. The virtualization of the nodes 
  makes it easy to create snapshots and use them as a backup scenario. 

- *Control Node Clusters*: These run the control layers of the cloud.
  SUSE OpenStack Cloud can distribute several OpenStack services onto as
  many servers as the administrator sees fit. There must be one Control Node
  Cluster per failure domain.
  
  // should that be *Controller Nodes* instead of *Control Node Clusters* ?

- *Compute Nodes*: As many compute nodes as necessary must be present --
  how many depends on the expected workload. All compute nodes must be
  distributed over the different failure domains.

- *Storage Nodes*: Every failure domain must have a storage available.
  This example assumes that SUSE Enterprise Storage is used for this
  purpose. Therefore, the minimum required number of storage nodes per
  failure domain is 3.

- *Management Nodes*: To run additional services such as Prometheus (a
  time-series database for monitoring, alerting and trending) and the ELK
  stack (ElasticSearch, LogStash, Kibana -- a log collection and index
  engine), further hardware is required. At least three machines per
  failure domain should be made available for this purpose.

- *Load Balancers*: At the central network uplinks to the setup, a load
  balancer must be placed -- this can either be an appliance or a
  Linux server running Nginx, HAProxy or other load balancing software.
  The load balancer must be configured in a highly available manner as
  loss of functionality on this level of the setup would make the
  complete setup unreachable.

The following picture shows a minimal implementation of this reference
architecture for large-scale cloud environments. It is the ideal start
for a Proof of Concept (PoC) setup or a test environment. It can also
serve as the "germ cell" for a larger setup. But in all cases, for 
the final setup, remember to have dedicated control clusters in all 
failure domains (in contrast to what the drawing actually shows).



=== SUSE OpenStack Cloud and SUSE Enterprise Storage

The basic services of an IaaS Cloud offers Compute, Networking and Storage 
Services. SUSE OpenStack Cloud is the base for the Compute and Networking 
Services. For the storage part, it is recommended to use a software defined 
solution and in most cases, a Ceph-based solution is used. SUSE Enterprise 
Storage is such a Ceph-based dirstribution and fits perfect to SUSE OpenStack 
Cloud.

Both products team up perfectly to build a large-scale OpenStack platform 
as described in this document. Certain basic design tenets such as the
distribution over multiple failure domains are integral design aspects
of these solutions and implicitly included. Both products will also not
only help you to set up OpenStack but also to run it in an effective and
efficient way (more on this subject is to follow in chapter 5 of
this document).


For the reference architecture you need the following "Bill of Material"
(BOM).

.Minimal Bill of Material for a Reference Architecture
[cols=">s,^m,^m",frame="topbot",options="header,footer",width="70%"]
|===
| Function | Minimal Reference Architecture | Large-Scale Environment
| Failure Domains | 3 | 3
|||
| Hardware 2+^s|Number of Servers
| Admin Server SOC | 1 | 1
| Admin Server SES | 1 | 1
| SUSE Manager     | 1 | 1
| SOC Control Cluster | 3 | 3
| SOC Network Cluster (Neutron) | 3 | 6
| Prometheus, ELK  | 3 | 18
| Compute Nodes | 15 | 240
| Storage Nodes (OSD) | 9 | 60
| Storage Monitors (MON) | 9 | 9
|   |   |
| Summary Servers ^s| 47 ^s| 2xx
|===


// vim:set syntax=asciidoc:
