== Architecture

Getting a large-scale cloud environment right is a tricky and ambitious
task. This chapter's purpose is to paint the bigger picture of all the
factors that companies need to take into consideration and of all the
considerations that may be relevant. After a quick introduction into the
principle of the economy of scale, this chapter will introduce the
reader to the main components of an OpenStack cloud and how these work
together. Then, a special focus is laid on designing a resilient and
stable scale-out setup along with its individual layers and the needed
considerations. Last but not least, a typical OpenStack architecture is
shown to serve as a valid example.

All cloud platforms share a basic design tenet: They are massive and yet
allow for massive scalability. What does scalability in clouds actually
mean?

=== Scalability in Clouds

Scalability is a word that most administrators will surely have heard and
likely also used throughout their career. A lot of different definitions
of scalability hence exist and the word is often uses in very different
contexts. Hence, creating a definition of what scalability is and what
kind of scalability this document focusses on is necessary. When talking
about "scale" processes, administrators usually intend to extend the
load that a specific setup can process by adding new hardware. The way
new hardware is added, however, strongly depends on local conditions and
may be very different when looking at different setups.

Generally speaking, when talking about "scalability", most people used
to refer to a process called "scale-up" until a few years ago. "Scale
up" or "scaling vertically" describes a process in which existing
hardware is extended so that it can handle more load. Adding more RAM to
an existing server, adding a stronger CPU to a node or adding additional
hard disks to an existing SAN storage appliance are typical examples for
"scale up". The issue with this approach is that it cannot be pursued
any further due to physical limitations after a while. For instance, in
an existing server, all memory banks may already be in use and there may
already be the biggest RAM modules in there that the server can use. It
may also be impossible to replace the CPU in a server simply because for
the given CPU socket, no stronger CPUs are available. And extending SANs
typically fails because all device slots of the SAN appliance are already
in use with the largest hard disks available for this model.

Dealing with the lack of the ability to scale up a system further used
to be a large issue in the past. In many cases, the only possibility to
work around the problem was to buy completely new, more capable hardware
that would be able to cope with the load present. That was, however, a
very expensive strategy and did not always succeed -- especiall in those
cases where substantially more powerful hardware simply was unavailable
at the market.

The opposite of "scale up" or "vertical scaling" is "scale out" or the
"horizontal scaling". This approach is fundamentally different from its
older competitor and assumes that there is no point in extending the
infrastructure that is already there by replacing individual hardware
components. Intead, in "scale out" scenarios, the idea is to add new
machines to the setup to distribute the load more evenly to more targets
in the installation. This obviously is a superior approach to scale up
approaches because the only limiting factor is the physical space that
is available in the datacenter. Thanks to dark fibre connections and the
modern technology, it is even possible to create new sites and connect
those to existing sites to accomodate for seamless scale out processes.

NOTE: Not all scalability approaches work for all environments. The
ability to scale out, for instance, requires the software in use to
support this operational mode. Cloud solutions such as OpenStack usually
are built for scale-out environments and have the ability to scale in a
horizontal manner at the core of their functionality. Legacy software,
in contrast to that, may not support scale out scenarios and may only be
scalable by the scale up principle.

As this document is about scalability in massively scalable environments,
the best scalability approach obviously is scale out, i.e. horizontal
scalability. Whenever "scalability" is mentioned in this document, that
hence is a reference to scale out processes, unless noted explicitly
differently at the respective location.

=== Cloud Computing Primer

Similar to "scalability", "cloud" is use as a technical term in an almost
indefinite number of contexts as well nowadays. This document elaborates
on the architecture of large-scale cloud environments based on SUSE OpenStack
Cloud; hence it does make sense to at least quickly define "cloud" in the
context of this document.

Conventional IT setups are usually a "turn-key solution" delivered to
the customer for a specific purpose. The customer seldomly takes care of
running and operating the solution himself; instead, the cloud provider
does that for him. This is statical and not satisfying for the customer
or the service provider. Instead of having to wait for a new setup for
weeks and months, customers expect to enter their credit cara data these
days and be handed out login credentials that they can use to immediately
start consume services.

In cloud setups, service providers hence become platform providers whose
main responsibility is to run a platform whose services customers can
consume at their own discretion. In addition to running the platform and
providing resources, providers also have to offer a way for customers to
service themselves, i.e. to consume services without having to contact
the service provider first. Now if one takes a look at the hardware that
is typically present in datancenters, it quicky becomes obvious that
without additional software, said hardware simply cannot provide what it
takes to offer the described features. For the sake of this document, a
"cloud software" hence is a software that creates a bridge between the
hardware on the one hand and customers on the other hand, allowing them
to consume the available resources as dynamically as possible. In summary,
the following attributes can be used to define "cloud":
- Self Service portal / API access
- network based
- pooling of existing resources
- consumtion based metering


=== OpenStack Primer

OpenStack most definitely is the best-known Open Source cloud solution
available at the market these days. It's also the fundament for the SUSE
Cloud product and hence plays a very important role when bulding a large
scale out cloud based on SUSE products as laid out in this document. An
OpenStack Cloud also consists of several different components that are
well wort knowing. The following paragraphs give you a quick introduction
into OpenStack per se and into how OpenStack will help you to build a
perfectly well scalable compute- and storage platform.

==== The OpenStack History

The OpenStack project originally started as a joint venture between the
NASA and the American hosting provider Rackspace. NASA controllers had
found out that many NASA scientists were conducting experiments for
which they ordered hardware; when their experiments were done, often the
hardware would not be used for anything else -- while other scientists
in other departments were ordering new hardware for their respective
experiments. The basic idea behind OpenStack hence was to create a tool
to centrally administer an arbitrary amount of compute resources and to
allow scientists to consume these resources as seen fit. Rackspace at
their end brought in the OpenStack Swift object storage component, which
will be explained in deeper detail in chapter 4 of this document.

NASA has withdrawn its OpenStack engagement by now, but the project is
by no means failing or inactive: Since its original incarnation in 2001,
dozens of companies have decided to adopt OpenStack as their primary
cloud product, including large systems vendors such as SUSE. Today, the
project is stable and reliable and still gets better. OpenStack hence is
the ideal fundament when building a large scale out environment.

At this time, OpenStack consists of more than 30 services. Not all of
these are required for basic cloud functionality -- the number of core
components is generally considered to be six these days (and even out
of those only 5 are strictly necessary). For a minimum viable cloud
setup, a few additional supporting components are also required. The
following paragraphs provide a more detailed description of the
OpenStack base services.

==== Supporting Services: RabbitMQ

OpenStack follows a strictly decentralized approach. Most components of
OpenStack (and the ones described in the following paragraphs in
particular) are also not made of a single service but consist of many
small services that often run on different hosts. Yet, all thes services
require a way to exchange messages between each other. Message protocols
such as the AMQP standard exist for exactly that purpose and OpenStack
is usually deployed along with the RabbitMQ message bus. RabbitMQ is one
of the oldest AMQP implementations and written in the Erlang programming
language. Several tools in the OpenStack universe use RabbitMQ to send
and receive messages. Hence, every OpenStack setup needs RabbitMQ -- and
for reasons of performance and redundancy, large-scale environments will
usually have more than one RabbitMQ instance running (more on the ideal
architecture of services for RabbitMQ and other services will be explained
further down in this chapter).

==== Supporting Services: MariaDB

A second component that will be present in most if not all setups based
on OpenStack is MariaDB (or its predecessor MySQL). Almost all OpenStack
components use MariaDB to store their internal meta-data in a persistent
manner. As the overall number of requests the databases is large, just
like RabbitMQ, MariaDB will typically be rolled out in a highly avialable
scale-out manner in cloud environments (for instance together with the
Galera Multi-master replication solution).

==== Authentication & Authorization: Keystone

The first actual component that every OpenStack cloud must have is the
Identity component, also known as Keystone Keystone serves two purposes
in OpenStack: It takes care of authenticating users by requiring them to
login to the API services and the GUI with a combination of a username
and a password. And it determines what role a specific user has inside
a project (or "tenant", as some older documentation still says). All
OpenStack components associate certain roles with certain permissions --
if a user has a certain role in a project, that automatically entitles
him to the permissions of said role for every respective service.

Keystone is one of the few components that only have one program under
the hood -- the Keystone API itself. It is capable of conneting to
existing user directories such as LDAP or Active Directory but can also
run in a standalone manner.

==== Operating System Image Provisioning: Glance

Glance is an OpenStack component that is often forgotten but important
nevertheless. Not all customers consuming cloud services are people
working professionally in IT -- so they not have the knowledge required
to install an operating system in a newly created VM in the cloud. And
even if professional IT people are consuming cloud services, they cannot
go through the tedious setup process of every new VM they create. That
would cost too much time and hurt the principle of the economy of scale.

But it also would be unnecessary. A virtual machine inside KVM can, if
spanwed in a cloud environment, be very well controlled and will be the
same inside different clouds if the underlying technology is identical.
It has hence become quite common for cloud provider to supply users with
a set of basic operating system images compatible with a given cloud. In
OpenStack, Glance is the component storing and administering these OS
images.

==== Virtual Networking: Neutron

Networking is a part of modern-day clouds that shows the most blatant
differences to conventional setups. Most paradigms about networking that
are valid for legacy installations are not true in clouds and often not
even applicable. While legacy setups make use of technologies such as
VLAN on the hardware level, clouds use Software Defined Networking and
create a virtual "overlay" networking level where virtual customer
networks reside. Customers have the ability to design their own virtual
network topology according to their needs and without any requirement
for the cloud provider to do anything.

In OpenStack, Neutron is the component implementing Software Defined
Networking. Through a system of loadable plug-ins, Neutron supports a
large number of Software Defined Networking implementations such as Open
vSwitch. Chapter 3 will elaborate on networking in OpenStack and Neutron
in deep detail. It will also explain how networks for clouds must be
designed to accomodate for the requirements of large-scale clouds.

==== Persistent VM block-storage: Cinder

Conventional setups will often have a central storage appliance such as
a SAN to provide storage to virtual machines through the installation.
These devices come with a number of shortcomings and will usually not
scale the way it is required on large-scale environments. And no matter
what storage solution is in place -- there still needs to be a method to
semi-automatically configure the storage from within the cloud to create
new volumes dynamically. After all, giving administrative rights to all
users in the cloud most definitely is not good idea.

Cinder is the OpenStack component that takes care of splitting storage
into small pieces and making it available to VMs throughout the cloud.
Chapter 4 elaborates on Cinder and explains in deep detail how it can be
used together with the Ceph object store to provide the required storage
in a scalable manner in cloud environments.

==== Compute: Nova

Nova is the primordial soup of OpenStack -- it's the one component that
was originally developed by the Nebula project at NASA and from which
most other projects have spawned off.

Nova in OpenStack is the centralized administration of compute resources
and virtual machines throughout the whole setup. Whenever a request to
start a new VM, terminate an existing VM or change a VM is issued by a
user, that request hits the Nova API component first. Nova is built of
almost a dozen different pieces taking care of individual tasks inside a
setup. That includes tasks such as the scheduling of new VMs the most
effective way (that is, answering the question "What host can and should
this virtual machine be running on?") and making sure that accessing the
virtual KVM console of a VM is possible.

Nova is a feature-rich component: Besides the standard hypervisor KVM,
it also supports solutions such as Xen, Hyper-V by Microsoft or VMware.
It has many screws and knobs that allow to control Nova's behaviour and
belongs to the most matur OpenStack components.

==== A concise GUI: Horizon

Last but not least, the OpenStack Dashboard, Horizon, must be mentioned
in this list of OpenStack components. Many OpenStack users may rarely
ever see it: Clouds function on the principle of API interfaces that
commands can be sent to in a specialized format to trigger a certain
action. And OpenStack is not an excuse from this rule: All components in
OpenStack come with an API component that will accept commands based on
the ReSTful HTTP approach.

There are, however, some tasks where a graphical representation of the
tasks at hand is helpful and maybe even desires. This is the moment that
Horizon enters the stage: It is the standard UI interface of OpenStack
and allows concise graphical access to all aforementioned components. It
is written in Django (i.e. Python-based HTML) and must be combined with
a WSGI server.

=== A perfect design for OpenStack

As one can quickly see, OpenStack is like an orchestra where a whole lot
of instruments need to join forces to play a beautiful symphony. That is
even more true for large environments with huge numbers of participating
nodes. What is a good way to structure and design such a setup? How can
companies provide a platform suitable for the respective requirements in
the best and most resilient manner? The following paragraphs will
deliver the answer to these questions.

==== Logical layers in Cloud environments

To understand how to run a resilient and stable cloud environment, it is
of crucial importance to understand that a cloud typically comes with
several layers. These layers are:

- *The hardware layer*: This layer contains all standard rack servers in
  an environment, i.e. such devices that are not specific network
  devices or other devices such as storage appliances.

- *The network layer*: This layer contains all devices responsible for
  providing physical network connectivity inside the setup and also to
  the outside -- switches, network cabling, upstream routers, special
  devices such as VPN bridges are good examples.

- *The storage layer*: This layer represents all devices responsible for
  providing persistent storage inside the setup along with the software
  components required for that. If solutions such as Ceph are in use,
  the storage layer only represents the software required for Software
  Defiend Storage as the hardware is already part of the hardware layer.

- *The control layer*: This layer includes all logical components that
  belong to the cloud solution, i.e. OpenStack for the sake of this
  document. All tools and programs in this layer are required for proper
  functionality of the system.

- *The compute layer*: This layer covers all software components on the
  compute nodes of a cloud environment.
  

Please note that a cloud can encounter different scenarios of issues
that come with different severities. The two most notables cateogires of
issues are:

- *Loss of control*: In such a scenario, existing services in the cloud
  continue to work as before, but it is impossible to control them via
  the APIs provided by the cloud. It is also impossible to start new
  services or to delete existing services.

- *Loss of functionality*: Here, not only is it impossible to control
  and steer the resources in a cloud but instead, these resources have
  become unavailable due to an outage.

When designing resilience and redundancy for large-scale environments,
it is very important to understand these different issue categories and
to understand how to avoid them.

==== Brazing for impact: Failure domains

An often and vividly discussed question is the question of how to make a
cloud environment resilient and "highly available". It's of crucial
importance to understand that "high availability" in the cloud context
is usually not the same as high availability in the classical meaning of
the words. Most administrators used to legacy setups will typically
assume that "high availability" for clouds means to make every host in
the cloud environment redundant. That is, however, not usually the case.
Cloud environments instead make a few assumptions on the applications
running inside of them. One assumption is that virtual setups are as
automated as possible -- that way, it is very easy to respawn a virtual
environment in case the old instance of it went down. Another assumption
usually found in clouds is that applications running there are "cloud
native" and inherently resilient against failures of the hardware that
they reside on.

Hence, most major public cloud providers have created SLAs that sound
radical from the point of view from conventional setups. Most large
public clouds are distributed over several physical sites that providers
usually call "regions". In their SLAs, these setups contain a statement
according to which the cloud formally counts as "being up" as long as
customers can, in any of the every so many regions of a setup, start a
virtual machine that is connected to a virutal network.

It must clearly be stated that the provider of a cloud setup has no way
to guarantee the availability of all hosts in a cloud setup at any time.

The focus of availability is on the control services, which are needed
to run / operate the cloud itself. OpenStack services are already stateless
designed and can be easily run in a active / active manner - distributed
on several nodes.  A cluster tool like pacemaker can be used to
manage the services and a loadbalancer in front of all will combine the 
services and make them available for the users.
Any workload running inside the cloud
will not be taken into account. SUSE OpenStack offers with the feature
"compute HA" an exception - but it should be used only where it's really
needed, due to the fact, that it adds more complexity to the environment
and makes it harder to maintain. The recommendation is, to create a 
dedicated zone of compute nodes, which provide the high availability
feature. 



In all scenarios, however, it does make sense to define failure domains
and to ensure redundancy over these. Failure domains are often also
referred to as "availability zones" in the OpenStack context; they are
similar to the aforementioned regions but usually cover a much smaller
geological area. 

The main idea behind a failure domain is to include every needed service 
into one zone. Redundancy is created by adding multile failure domains 
to the design. The setup has to make sure, that a failure inside of a 
failure domain will not effect any service in any other failure domain. 
In addition, the function of the failed service must be taken over by another 
failure domain.
So it's important, that every failure domain is isolatet in meaning of
infrastructure like power, networking, cooling. All services (control, compute,
network and storage) have to be distributed over all failure domains.
The sizing has to take into account, that even if one complete failure domain
will die, enough resources to operate the whole cloud are available.

The application layer is responsible to distribute the workload over
all failure domains, so that the availability of the application will be 
ensured in case of a failure inside of one failure domain. OpenStack offers
anti-affinity rules to schedule instances in different zones.

The minimum recommended amount of failure domains for large scale-out
setups based on OpenStack is three. With three faiure domains in place,
a failure domain's outage can easily be compensated by the remaining
two. When planning for additional failure domains, it's important to
keep in mind how quorum works: To have quorum, the remaining parts of a
setup must have the majority of relevant nodes in them. For instance,
with three failure domains, two failure domains would still have the
majority of relevant nodes in case one failure domain goes down. The
majority here is defined by "50% + one full instance".

.Highlevel architecture of failure domain setup with three nodes
image::architectur_high_level.png[align="center",width=300]

==== The Control Layer

The control layer covers all components that ensure functionality and
the ability to control the cloud. All components of this layer must be
present and distributed evenly across the available failure domains,
namely:

- *MariaDB*: An instance of MariaDB should be running in every failure
  domain of the setup. As MariaDB clustering does not support a
  multi-master scenario out of the box, the Galera clustering solution
  can be used to ensure that all MariaDB nodes in all failure domains
  are fully functional MariaDB instances, allowing for write and read
  access. All three MariaDB instances form one database cluster in a
  scenario with three availability zones. If one zone fails, the other
  two MariaDB instances will still function.

- *RabbitMQ*: RabbitMQ instances should also be present in all failure
  domains of the installation. The built-in clustering functionality of
  RabbitMQ can be used to achieve this goal and to create a RabbitMQ
  cluster that resembles the MariaDB cluster described before.

- *Load balancing*: All OpenStack components that users and also the
  other components themselves will be using are HTTP(S) interfaces based
  on the ReST principle. As such, in large environments, the will be the
  subject to a lot of load. It is hence required in large-scale setups
  to use load balancers in front of the API instances to distribute the
  incoming requests evenly. This is also true for MySQL (RabbitMQ has a
  built-in cluster functionality and hence is an exception frm the rule).

- *OpenStack components*: All OpenStack components and the programs
  that belong to them with the exception of `nova-compute` and
  `neutron-l3-agent` must be running on dedicated hosts ("controller
  nodes") in all failure domains. Typically, powerful machines are
  used to run these together on the same hosts with MariaDB and also
  RabbitMQ. As OpenStack is made for scale-out, there is no issue
  resulting from running these components simultaneously many times.

==== The Network Layer

The physical network is expected to be built so that it interconnects
the different failure domains of the setup and all nodes redundantly. The
external uplink is also required to be redundant. A separate node in
every failure domain should act as "network node" for OpenStack Neutron.
A "network node" will ensure the cloud's external connectivity by running
the `neutron-l3-agent` part of OpenStack Neutron.

In many setups, the dedicated network nodes also run the DHCP agent for
Open vSwitch; that is possible and a valid configuration but not under
all circumstances necessary.

OpenStack enriches existing Open vSwitch functionality with a feature
usually referred to as _Distributed Virtual Routing_ or _DVR_. In setups
making use of DVR, external network connectivity is moved from the dedicated
network nodes to the compute node. Each compute node will run the routing
service, which are needed by the local instances. This helps in two cases:
- Scale out: adding new compute nodes also adds new network capabilities.
- Failure: a failure of a compute node only effects the routing of local instances.
The routing service is undependent from the central networking nodes.

Further details on the individual components of the networking layer and
the way OpenStack deals with networking are available in chapter 3 of
this document.

==== The Storage Layer

Storage is a complex topic in large scale environments. Chapter 4 deals
with all relevant aspects of it and also explains how a Software Defined
Storage solution such as Ceph can easily satisfy a scalable setup's need
for redundant storage.

Generally speaking, when using an SDS solution, said solution's parts
must be distributed across all failure domains so that every domain has
a working storage cluster. Three nodes per domain are the bare minimum.
In the special example of Ceph, the CRUSH hashing algorithm must also be
configured so that it stores replicas of all data in all failure domains
for every write process.

Should the Ceph Object Gateway be in use to provide for S3/Swift storage
via a ReSTful interface, that service must be evenly available in all
failure domains as well. And of course it's necessary to include these
servers in the loadbalancer setup that is in place anyway for making the
API services redundant and resilient.

==== The Compute Layer

Last but not least, when designing a scalable OpenStack Setup, of course
the Compute layer plays an important role. While for the control services
no massive scaling is expected, the compute layer is mostly effected by the
ongoing request of more resources.

The most important factor, of course, is to scale out the failure domains
equally. When the setup is extended, comparable amounts of nodes should
be added to all failure domains to ensure that the setup remains balanced.

[[CPU_and_RAM_Ratio]]
When acquiring hardware for the compute layer, there is one factor that
many administrators do not take into consideration although they should:
the required ratio of RAM and CPU cores for the expected workload. To
explain the relevance of this, think of a simple example: If a server
has 256 gigabytes of RAM and 16 CPU cores that split into 32 threads
with hyper-threading enabled, a possible RAM-CPU-ratio for the host is
32 VMs with one vCPU and 8 gigabytes of RAM. One could also create 16
VMs with 16 gigabytes and two vCPUs or 8 VMs with 32 gigabytes of RAM
and 4 vCPUs. The latter is a fairly common virtual hardware layout (a so
called "flavor") example for a general purpose VM in cloud environments.

Now some workloads may be CPU-intense without the need for much RAM or
may require lots of RAM but hardly CPU power -- in those cases, users
would likel want to use different flavors such as "4 CPU cores and 256
Gigabytes of RAM" or "16 CPU cores and 16 gigabytes of RAM". The issue
with those is that if one VM with 4 CPU cores but 256 gigabytes of RAM
or 16 CPU cores and 16 gigabytes of RAM runs on a server, the remaining
resources on said machine are hardly useful for any other task -- they
are "blend" and may remain unused completely.

Cloud providers must hence consider the workload of a future setup as
good as possible and plan compute nodes according to these requirements.
If the setup to be created is a public cloud pre-defined flavors ought
to lead customers along the desired patterns of usage. If customers do
insist on special flavours, the provider must take the hardware that
possibly remains unused because of differing virtual hardware schemes in
their calculation at least. And if the usage pattern is hard to predict
at all, a mixture of different hardware kinds likely makes the most
sense. From the operational point of view, it should be noted, that always the same
hardware class is used - only in different expansion stages). This helps to 
reduce the efford in maintance and spare parts.
OpenStack comes with a number of functions such as Host Aggregates
to make maintaining such platforms convenient and easy. For the sake of
this document, the ratio of CPU and RAM is generally considered 1:4 in
the following examples.

[[ReferenceArchitecture]]
=== Reference Architecture

In the following paragraphs, a perfectly valid basic design reference
for a large-scale SUSE OpenStack Cloud based on OpenStack and Ceph is described.

.Highlevel Reference Architecure of a large scale deployment with 108 Compute Nodes and 36 Storage Nodes
image::Reference_Arch_108.png[align="center",width=500]

==== Basic requirements

Based on the explanations given in this chapter, to build a basic setup
that is a large scale cloud with SUSE components, the following factors
must be valid and the following criteria must be fulfilled:

- three failure domains (at least different fire protection zones in the
  same datacenter, although different datacenters would be better) that
  are redundantly and independently from each other connected to power
  and networking must be present
- OSI level 2 network hardware, spawning over the three failure domains
  to ensure connectivity must be in place. For reasons of latency and
  timings, the maximum distance between the three failure domains should
  not exceed ten kilometers
- SUSE OpenStack Cloud must be deployed across all failure domains
- SUSE Enterprise Storage must be deployed across all failure domains
- SUSE Manager or an SMT instance to mirror all the required software
  repositories (including all software channels and patches) needs to
  exist. This happens to supply the setup with thelatest features,
  enhancements, and security patches
- Adequate system management tools (as explained in chapter 5) must be
  in place and working to guarantee efficient maintainability and to
  ensure compliance and consistency


==== SUSE OpenStack Cloud roles

SUSE OpenStack Cloud functions based on roles. By assigning a host a
certain role, it automatically also has certain software and tasks
installed and assigned to it. Four major roles exist:

- *Administration Server*: These are the deployment nodes for SUSE OpenStack
  Cloud and SUSE Enterprise Storage. It is the fundament for the deployment and management of all
  nodes and services as it hosts the required tools. The admin servers
  can of course also be a KVM virtual machine. The admin services don't
  need to be redundant - a working backup and restore process is sufficient
  to ensure the operation. The virtualization of the nodes makes it easy
  to create snapshots and use this as a backup scenario 

- *Control Node Clusters*: These run the control layers of the cloud.
  SUSE OpenStack Cloud can distribute several OpenStack servics onto as
  many servers as the admin sees fit. There must be one Control Node
  Cluster per failure domain.

- *Compute Nodes*: As many compute nodes as necessary must be present --
  this depends on the expected workload. As compute nodes must be
  distributed over the different failure domains.

- *Storage Nodes*: Every failure domain must have a storage available.
  This example assumes that SUSE Enterprise Storage is used for this
  purpose -- hence, the minimum required number of storage nodes per
  failure domain is 3.

- *Management nodes*: To run additional services such as Prometheus (a
  time-series database for Monitoring, Alerting and Trending), the ELK
  stack (ElasticSearch, LogStash, Kibana -- a log collection and index
  engine), further hardware is required. At least three machines per
  failure domain should be made available for this purpose.

- *Loadbalancers*: At the central network uplinks to the setup, a load
  balancer must be placed -- this can either be an appliance or a
  linux-server running Nginx, HAProxy or other load balancing software.
  The load balancer must be configured in a highly available manner as
  loss of functionality on this level of the setup would render the
  complete setup unreachable.

The following picture shows a minimal implementation of this reference
architecture for large scale cloud environments. It is the ideal start
for a Proof of Concept (PoC) setup or a test environment. It can also
be the germ cell for a larger setup -- but in all cases, for the final
setup remember to have dedicated control clusters in all failure domains
(in contrast to what the drawing actually shows).



=== SUSE OpenStack Cloud & SUSE Enterprise Storage
The basic services of an IaaS Cloud offers Compute,Networking and Storage Services.
SUSE OpenStack Cloud is the base for the Compute and Networking Services.
For the storage part, it's recommended to use a software defined solution and
in most cases, a ceph based solution is used. SUSE Enterprise Storage is 
such a ceph based dirstribution and fits perfect to SUSE OpenStack Cloud.
Both products team up to build a large-scale OpenStack platform
as described in this document. Certain basic design tenets such as the
distribution over multiple failure domains are integral design aspects
of these solutions and implicitly included. Both products will also not
only help you to set OpenStack up but also to run it in an effective and
well-working way (more on this subject is to follow in chapter five of
this document).


For the reference architecture you need the following "Bill of Material"
(BOM).

.Minimal Bill of Material for a Reference Architecture
[cols=">s,^m,^m",frame="topbot",options="header,footer",width="70%"]
|===
| Function | Minimal Reference Architecture | Large Scale Environment
| Failure Domains | 3 | 3
|||
| Hardware 2+^s|Number of Servers
| Admin Server SOC | 1 | 1
| Admin Server SES | 1 | 1
| SUSE Manager     | 1 | 1
| SOC Control Cluster | 3 | 3
| SOC Network Cluster (Neutron) | 3 | 6
| Prometheus, ELK  | 3 | 18
| Compute Nodes | 15 | 240
| Storage Nodes (OSD) | 9 | 60
| Storage Monitors (MON) | 9 | 9
|   |   |
| Summary Servers ^s| 47 ^s| 2xx
|===


// vim:set syntax=asciidoc:
